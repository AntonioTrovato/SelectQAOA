{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statistics\n",
    "from qiskit_optimization import QuadraticProgram\n",
    "from qiskit_optimization.algorithms import MinimumEigenOptimizer\n",
    "from qiskit_algorithms.optimizers import COBYLA\n",
    "from qiskit.primitives import BackendSampler\n",
    "from qiskit_algorithms import QAOA\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit_aer import Aer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from collections import defaultdict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#this cell contains all variable definitions that will be useful throughout the entire project\n",
    "sir_programs = [\"flex\",\"grep\",\"gzip\",\"sed\"]\n",
    "sir_programs_tests_number = {\"flex\":567,\"grep\":806,\"gzip\":214,\"sed\":360}\n",
    "sir_programs_end_lines = {\"flex\":14192,\"grep\":13281,\"gzip\":6701,\"sed\":7118}\n",
    "alpha = 0.5\n",
    "experiments = 20"
   ],
   "id": "60e80f8fc1f9ffa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def json_keys_to_int(d):\n",
    "    \"\"\"This method correctly converts the data\"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        return {int(k) if k.isdigit() else k: json_keys_to_int(v) for k, v in d.items()}\n",
    "    elif isinstance(d, list):\n",
    "        return [json_keys_to_int(i) for i in d]\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "\n",
    "with open(\"datasets/sir_programs/executed_lines_test_by_test.json\", \"r\") as file:\n",
    "    #dictionary that, for each sir program, associates at each LINE of that program the LIST of TESTS COVERING it\n",
    "    executed_lines_test_by_test = json_keys_to_int(json.load(file)) #{program1:{line:[tci,tcj,...,tck],line2:...}\n",
    "with open(\"datasets/sir_programs/faults_dictionary.json\", \"r\") as file:\n",
    "    #dictionary that associates at each SIR PROGRAM the LIST of PAST FAULT COVERAGE VALUES ORDERED BY TEST \n",
    "    faults_dictionary = json.load(file) #{program1:[fault_cov_tc1,fault_cov_tc2,...,fault_cov_tcn],program2:...}\n",
    "with open(\"datasets/sir_programs/test_coverage_line_by_line.json\", \"r\") as file:\n",
    "    #dictionary that, for each sir program, associates at each TEST of that program the LIST of LINES COVERED by it\n",
    "    test_coverage_line_by_line = json_keys_to_int(json.load(file)) #{program1:{tc1:[linei,linej,...,linek],tc2:...}\n",
    "with open(\"datasets/sir_programs/test_cases_costs.json\", \"r\") as file:\n",
    "    #dictionary that, for each sir program, associates at each TEST its EXECUTION COST\n",
    "    test_cases_costs = json_keys_to_int(json.load(file)) #{program1:{tc1:ex_cost1,tc2:ex_cost2,...,tcn:ex_costn},program2:...}\n",
    "with open(\"datasets/sir_programs/total_program_lines.json\", \"r\") as file:\n",
    "    #dictionary which associates at each SIR PROGRAM its size in terms of the NUMBER OF ITS LINES\n",
    "    total_program_lines = json.load(file) #{program1:tot_lines_program1,program2:tot_lines_program2,program3:...}"
   ],
   "id": "ab392a2fa1d33b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def num_of_covered_lines(sir_program,test_cases):\n",
    "    \"\"\"This method returns the number of covered lines (no redundancy)\"\"\"\n",
    "    covered_lines = set()\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        try:\n",
    "            for covered_line in test_coverage_line_by_line[sir_program][test_case]:\n",
    "                covered_lines.add(covered_line)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return len(covered_lines)\n",
    "\n",
    "clusters_dictionary = dict()\n",
    "\n",
    "for sir_program in sir_programs:\n",
    "    tot_test_cases = sir_programs_tests_number[sir_program]\n",
    "    \n",
    "    # from {..., test_case_i : [cov_stmts], ...} to [..., #_stmt_cov_i, ...]\n",
    "    test_cases_stmt_cov = []\n",
    "    for test_case in test_coverage_line_by_line[sir_program].keys():\n",
    "        test_cases_stmt_cov.append(len(test_coverage_line_by_line[sir_program][test_case]))\n",
    "    suite_stmt_cov = sum(test_cases_stmt_cov)\n",
    "    \n",
    "    # Normalize data\n",
    "    data = np.column_stack((list(test_cases_costs[sir_program].values()),faults_dictionary[sir_program],test_cases_stmt_cov))\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "    num_clusters = 50\n",
    "        \n",
    "    max_cluster_dim = 24\n",
    "    \n",
    "    # Step 2: Perform K-Means Clustering\n",
    "    start = time.time()\n",
    "    linkage_matrix = linkage(normalized_data, method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=num_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Organize test cases by cluster\n",
    "    clustered_data = defaultdict(list)\n",
    "    for idx, cluster_id in enumerate(clusters):\n",
    "        clustered_data[cluster_id].append(idx)\n",
    "    \n",
    "    # Process clusters to ensure none exceed max_cluster_dim\n",
    "    new_cluster_id = max(clustered_data.keys()) + 1  # Start new IDs after existing ones\n",
    "    to_add = []  # Collect new smaller clusters\n",
    "    \n",
    "    for cluster_id, elements in list(clustered_data.items()):  # Avoid modifying dict during iteration\n",
    "        if len(elements) > max_cluster_dim:\n",
    "            num_splits = -(-len(elements) // max_cluster_dim)  # Ceiling division to get the required number of splits\n",
    "            split_size = -(-len(elements) // num_splits)  # Recalculate to distribute elements evenly\n",
    "            \n",
    "            # Split while keeping sizes balanced\n",
    "            parts = [elements[i:i + split_size] for i in range(0, len(elements), split_size)]\n",
    "    \n",
    "            # Ensure all new clusters are within max_cluster_dim\n",
    "            for part in parts:\n",
    "                if len(part) > max_cluster_dim:\n",
    "                    raise ValueError(f\"A split cluster still exceeds max_cluster_dim ({len(part)} > {max_cluster_dim})!\")\n",
    "    \n",
    "            # Add new parts to the new clusters\n",
    "            to_add.extend(parts)\n",
    "    \n",
    "            # Remove original large cluster\n",
    "            del clustered_data[cluster_id]\n",
    "    \n",
    "    # Assign new IDs to split parts\n",
    "    for part in to_add:\n",
    "        if part:  # Only add if the part is non-empty\n",
    "            clustered_data[new_cluster_id] = part\n",
    "            new_cluster_id += 1\n",
    "    end = time.time()\n",
    "    print(\"SelectQAOA Decomposition Time(ms): \" + str((end-start)*1000))\n",
    "    \n",
    "    clusters_dictionary[sir_program] = clustered_data\n",
    "        \n",
    "    # Step 3: Calculate the metrics for each cluster and validate\n",
    "    cluster_metrics = {}\n",
    "    for cluster_id in clustered_data.keys():\n",
    "        tot_cluster_exec_cost = 0\n",
    "        tot_cluster_past_fault_cov = 0\n",
    "        tot_cluster_stmt_cov = 0\n",
    "        for test_case in clustered_data[cluster_id]:\n",
    "            tot_cluster_exec_cost += test_cases_costs[sir_program][test_case]\n",
    "            tot_cluster_past_fault_cov += faults_dictionary[sir_program][test_case]\n",
    "        tot_cluster_past_fault_cov = tot_cluster_past_fault_cov/tot_test_cases\n",
    "        tot_cluster_stmt_cov = num_of_covered_lines(sir_program,clustered_data[cluster_id])/total_program_lines[sir_program]\n",
    "        cluster_metrics[cluster_id] = {\n",
    "            \"tot_exec_cost\": tot_cluster_exec_cost,\n",
    "            \"tot_past_fault_cov\": tot_cluster_past_fault_cov,\n",
    "            \"tot_stmt_cov\": tot_cluster_stmt_cov  # Avg stmt coverage per test case in cluster\n",
    "        }\n",
    "        print(f\"Cluster {cluster_id + 1} metrics:\")\n",
    "        print(f\"Test Cases: {clustered_data[cluster_id]}\")\n",
    "        print(f\" - Num. Test Cases: {len(clustered_data[cluster_id]):.2f}\")\n",
    "        print(f\" - Execution Cost: {tot_cluster_exec_cost:.2f}\")\n",
    "        print(f\" - Past Fault Coverage (%): {tot_cluster_past_fault_cov}\")\n",
    "        print(f\" - Statement Coverage (%): {tot_cluster_stmt_cov:.2f}\\n\")\n",
    "    \n",
    "    for cluster_id in clustered_data.keys():\n",
    "        print(\"Test cases of cluster \" + str(cluster_id) + \": \" + str(len(clustered_data[cluster_id])))\n",
    "    \n",
    "    print(\"======================================================================================\")\n",
    "    \n",
    "    print(\"Program Name: \" + sir_program)\n",
    "    \n",
    "    for cluster_id in clustered_data.keys():\n",
    "        if len(clustered_data[cluster_id]) > max_cluster_dim:\n",
    "            print(\"Test cases of cluster \" + str(cluster_id) + \": \" + str(len(clustered_data[cluster_id])))\n",
    "    \n",
    "    # Plotting the clusters in 3D space\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Extracting data for plotting\n",
    "    exec_costs = np.array(list(test_cases_costs[sir_program].values()))\n",
    "    fault_covs = np.array(faults_dictionary[sir_program])\n",
    "    stmt_covs = np.array(test_cases_stmt_cov)\n",
    "    \n",
    "    # Plot each cluster with a different color\n",
    "    colors = plt.cm.get_cmap(\"tab10\", num_clusters)  # A colormap with as many colors as clusters\n",
    "    for cluster_id in clustered_data.keys():\n",
    "        cluster_indices = clustered_data[cluster_id]\n",
    "        \n",
    "        # Plot each cluster's points\n",
    "        ax.scatter(\n",
    "            exec_costs[cluster_indices], \n",
    "            fault_covs[cluster_indices], \n",
    "            stmt_covs[cluster_indices], \n",
    "            color=colors(cluster_id), \n",
    "            label=f\"Cluster {cluster_id + 1}\"\n",
    "        )\n",
    "    \n",
    "    # Label the axes\n",
    "    ax.set_xlabel(\"Execution Cost\")\n",
    "    ax.set_ylabel(\"Past Fault Coverage\")\n",
    "    ax.set_zlabel(\"Statement Coverage\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Test Case Clustering Visualization\")\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "print(clusters_dictionary)"
   ],
   "id": "53799261ddcb09b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def make_linear_terms(sir_program, cluster_test_cases, alpha):\n",
    "    \"\"\"Making the linear terms of QUBO\"\"\"\n",
    "    max_cost = max(test_cases_costs[sir_program].values())\n",
    "    \n",
    "    estimated_costs = []\n",
    "\n",
    "    #linear coefficients, that are the diagonal of the matrix encoding the QUBO\n",
    "    for test_case in cluster_test_cases:\n",
    "        estimated_costs.append((alpha * (test_cases_costs[sir_program][test_case]/max_cost)) - (1 - alpha) * faults_dictionary[sir_program][test_case])\n",
    "    \n",
    "    return np.array(estimated_costs)\n",
    "\n",
    "def make_quadratic_terms(sir_program, variables, cluster_test_cases, linear_terms, penalty):\n",
    "    \"\"\"Making the quadratic terms of QUBO\"\"\"\n",
    "    quadratic_terms = {}\n",
    "    \n",
    "    #k is a stmt\n",
    "    for k in executed_lines_test_by_test[sir_program].keys():\n",
    "        #k_test_cases is the list of test cases covering k\n",
    "        k_test_cases = executed_lines_test_by_test[sir_program][k]\n",
    "        for i in k_test_cases:\n",
    "            if i not in cluster_test_cases or i not in variables:\n",
    "                continue\n",
    "            for j in k_test_cases:\n",
    "                if j not in cluster_test_cases or j not in variables:\n",
    "                    continue\n",
    "                if i < j:\n",
    "                    linear_terms[variables.index(i)] -= penalty\n",
    "                    try:\n",
    "                        quadratic_terms[variables.index(i),variables.index(j)] += 2 * penalty\n",
    "                    except:\n",
    "                        quadratic_terms[variables.index(i),variables.index(j)] = 2 * penalty\n",
    "    \n",
    "    return quadratic_terms"
   ],
   "id": "c0f30c65fa3752be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_QUBO_problem(linear_terms, quadratic_terms):\n",
    "    \"\"\"This function is the one that has to encode the QUBO problem that QAOA will have to solve. The QUBO problem specifies the optimization to solve and a quadratic binary unconstrained problem\"\"\"\n",
    "    qubo = QuadraticProgram()\n",
    "    \n",
    "    for i in range(0,len(linear_terms)):\n",
    "        qubo.binary_var('x%s' % (i))\n",
    "\n",
    "    qubo.minimize(linear=linear_terms,quadratic=quadratic_terms)\n",
    "\n",
    "    return qubo\n"
   ],
   "id": "f1c2dbe8ae294ef2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "penalties_dictionary = {\"flex\":None,\"grep\":None,\"gzip\":None,\"sed\":None}\n",
    "\n",
    "#to get a QUBO problem from a quadratic problem with constraints, we have to insert those constraints into the Hamiltonian to solve (which is the one encoded by the QUBO problem). When we insert constraint into the Hamiltonian, we have to specify also penalties\n",
    "for sir_program in sir_programs:\n",
    "    max_penalty = 0\n",
    "    max_cost = max(test_cases_costs[sir_program].values())\n",
    "    for i in range(sir_programs_tests_number[sir_program]):\n",
    "        cost = (alpha * (test_cases_costs[sir_program][i]/max_cost)) - ((1 - alpha) * faults_dictionary[sir_program][i])\n",
    "        if cost > max_penalty:\n",
    "            max_penalty = cost\n",
    "    penalties_dictionary[sir_program] = max_penalty + 1"
   ],
   "id": "d2d3544d011bf804",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "qubos_dictionary = {\"flex\":[],\"grep\":[],\"gzip\":[],\"sed\":[]}\n",
    "#make a dictionary that saves, for each program, the correspondent QUBO\n",
    "for sir_program in sir_programs:\n",
    "    print(\"SIR Program:\\n\")\n",
    "    for cluster_id in clusters_dictionary[sir_program]:\n",
    "        print(\"Cluster ID: \" + str(cluster_id))\n",
    "        variables = []\n",
    "        for idx in range(0, len(clusters_dictionary[sir_program][cluster_id])):\n",
    "            variables.append(idx)\n",
    "        linear_terms = make_linear_terms(sir_program, clusters_dictionary[sir_program][cluster_id], alpha)\n",
    "        quadratic_terms = make_quadratic_terms(sir_program, variables, clusters_dictionary[sir_program][cluster_id], linear_terms, penalties_dictionary[sir_program])\n",
    "        qubo = create_QUBO_problem(linear_terms, quadratic_terms)\n",
    "        qubos_dictionary[sir_program].append(qubo)\n",
    "        print(qubo)\n",
    "        print(\"/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/\")\n",
    "    print(\"======================================================================================\")"
   ],
   "id": "292a2f7aab08196d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def covered_lines(sir_program,test_cases_list):\n",
    "    \"\"\"Number of covered lines (no redundancy)\"\"\"\n",
    "    covered_lines = set()\n",
    "    \n",
    "    for test_case in test_cases_list:\n",
    "        try:\n",
    "            for covered_line in test_coverage_line_by_line[sir_program][test_case]:\n",
    "                covered_lines.add(covered_line)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return len(covered_lines)\n",
    "\n",
    "def build_pareto_front(sir_program,selected_tests):\n",
    "    \"\"\"This method builds the pareto front additionally from a sub test suite solution\"\"\"\n",
    "    pareto_front = []\n",
    "    max_fault_coverage = 0\n",
    "    max_stmt_coverage = 0\n",
    "    \n",
    "    for index in range(1,len(selected_tests)+1):\n",
    "        #exract the first index selected tests\n",
    "        candidate_solution = selected_tests[:index]\n",
    "        candidate_solution_fault_coverage = 0\n",
    "        candidate_solution_stmt_coverage = 0\n",
    "        for selected_test in candidate_solution:\n",
    "            candidate_solution_fault_coverage += faults_dictionary[sir_program][selected_test]\n",
    "            candidate_solution_stmt_coverage += covered_lines(sir_program,candidate_solution)\n",
    "        #if the actual pareto front dominates the candidate solution, get to the next candidate\n",
    "        if max_fault_coverage >= candidate_solution_fault_coverage and max_stmt_coverage >= candidate_solution_stmt_coverage:\n",
    "            continue\n",
    "        #eventually update the pareto front information\n",
    "        if candidate_solution_stmt_coverage > max_stmt_coverage:\n",
    "            max_stmt_coverage = candidate_solution_stmt_coverage\n",
    "        if candidate_solution_fault_coverage > max_fault_coverage:\n",
    "            max_fault_coverage = candidate_solution_fault_coverage\n",
    "        #add the candidate solution to the pareto front\n",
    "        pareto_front.append(candidate_solution)\n",
    "    \n",
    "    return pareto_front"
   ],
   "id": "f79699d331f5ccbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "algorithm_globals.random_seed = 10598\n",
    "backend = Aer.get_backend(\"statevector_simulator\")\n",
    "\n",
    "#I want to run the sampler 30 times to get different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    for rep in [1,2,4,8,16]:\n",
    "        qaoa_mes = QAOA(sampler=BackendSampler(backend=backend), optimizer=COBYLA(100), reps=rep)\n",
    "        qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "        #the fronts will be saved into files\n",
    "        print(\"SIR Program: \" + sir_program)\n",
    "        file_path = \"results/selectqaoa/statevector_sim/\" + sir_program + \"-data-rep-\" + str(rep) + \".json\"\n",
    "        json_data = {}\n",
    "        response = None\n",
    "        qpu_run_times = []\n",
    "        pareto_fronts_building_times = []\n",
    "        for i in range(experiments):\n",
    "            final_selected_tests = []\n",
    "            cluster_dict_index = 0\n",
    "            for qubo in qubos_dictionary[sir_program]:\n",
    "                print(\"QUBO Problem: \" + str(qubo) + \"\\n Cluster Number: \" + str(cluster_dict_index))\n",
    "                print(\"Cluster's Test Cases: \" +str(list(clusters_dictionary[sir_program].values())[cluster_dict_index]))\n",
    "                #for each iteration get the result\n",
    "                s = time.time()\n",
    "                qaoa_result = qaoa.solve(qubo)\n",
    "                print(\"RESULTS: \" + str(qaoa_result))\n",
    "                e = time.time()\n",
    "                qpu_run_times.append((e - s) * 1000)\n",
    "                #let's extract the selected tests\n",
    "                variable_values = qaoa_result.x\n",
    "                indexes_selected_tests = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "                print(\"Indexes of selected tests to convert. \" + str(indexes_selected_tests))\n",
    "                selected_tests = []\n",
    "                for index in indexes_selected_tests:\n",
    "                    selected_tests.append(list(clusters_dictionary[sir_program].values())[cluster_dict_index][index])\n",
    "                print(\"Selected tests: \" + str(selected_tests))\n",
    "                print(\"Experiment Number: \" + str(i))\n",
    "                cluster_dict_index += 1\n",
    "                for selected_test in selected_tests:\n",
    "                    if selected_test not in final_selected_tests:\n",
    "                        final_selected_tests.append(selected_test)\n",
    "            i+=1\n",
    "            #now we have to build the pareto front\n",
    "            print(\"Final Selected Test Cases: \" + str(final_selected_tests))\n",
    "            print(\"Length of the final list of selected test cases: \" + str(len(final_selected_tests)))\n",
    "            start = time.time()\n",
    "            pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "            end = time.time()\n",
    "            json_data[\"pareto_front_\" + str(i)] = pareto_front\n",
    "            pareto_front_building_time = (end - start) * 1000\n",
    "            pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "    \n",
    "        #compute the average time needed for the construction of a pareto frontier and run time\n",
    "        mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "        mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "        json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "        json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "        json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "        json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "    \n",
    "        with open(file_path, \"w\") as file:\n",
    "            json.dump(json_data, file)\n"
   ],
   "id": "f634f33db3133ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7cb66746225007ff",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
