{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SelectQAOA: Regression Test Case Selection Using QAOA\n",
    "Regression testing is an important part of the software development process in software engineering. It is a practice aimed at identifying any regression, which are the emergence of new defects or issues in a software application following changes, enhancements, or updates made to the source code. In other words, regression testing focuses on how changes made to the software can affect the correct behavior of existing features. Regression testing is particularly important in agile software development environments, where changes are made frequently and rapidly. This practice helps ensure that the software remains stable and reliable as it evolves over time. Ideal regression testing would re-run all the available test cases of a given software system. However, in addition to being potentially very costly, this could even be impractical in some case. In this scenario, test case selection is one of the most widely investigated test suite optimization approaches.\n",
    "Test case selection focuses on selecting a subset from an initial test suite to test software changes, i.e., to test whether unmodified parts of a program continue to work correctly after changes involving other parts. Various techniques, such as Integer Programming, symbolic execution, data flow analysis, dependence graph-based methods, and flow graph-based approaches, can be employed to identify the modified portions of the software. Once test cases covering the unchanged program segments are pinpointed using a specific technique, an optimization algorithm (e.g., additional greedy, DIV-GA,\n",
    "SelectQA, BootQA or SelectQAOA) can select a minimal set of these test cases based on certain testing criteria (e.g., branch coverage). The ultimate aim is to reduce the expenses associated with regression testing."
   ],
   "id": "8600e345c9a018bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T09:58:52.685485Z",
     "start_time": "2025-02-19T09:58:46.100589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#this cell contains all the imports needed by the pipeline\n",
    "#to run it on the browser: jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statistics\n",
    "from qiskit_optimization import QuadraticProgram\n",
    "from qiskit_optimization.algorithms import MinimumEigenOptimizer\n",
    "from qiskit_algorithms.optimizers import COBYLA\n",
    "from qiskit.primitives import BackendSampler\n",
    "from qiskit_algorithms import QAOA\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit_aer import AerSimulator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cdist"
   ],
   "id": "5d9fad51f54e7418",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#this cell contains all variable definitions that will be useful throughout the entire project\n",
    "sir_programs = [\"flex\",\"grep\",\"gzip\",\"sed\"]\n",
    "sir_programs_tests_number = {\"flex\":567,\"grep\":806,\"gzip\":214,\"sed\":360}\n",
    "sir_programs_end_lines = {\"flex\":14192,\"grep\":13281,\"gzip\":6701,\"sed\":7118}\n",
    "alpha = 0.5\n",
    "experiments = 30"
   ],
   "id": "185e0d59f02ed724"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def json_keys_to_int(d):\n",
    "    if isinstance(d, dict):\n",
    "        return {int(k) if k.isdigit() else k: json_keys_to_int(v) for k, v in d.items()}\n",
    "    elif isinstance(d, list):\n",
    "        return [json_keys_to_int(i) for i in d]\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "\n",
    "with open(\"datasets/sir_programs/executed_lines_test_by_test.json\", \"r\") as file:\n",
    "    #dictionary that, for each sir program, associates at each LINE of that program the LIST of TESTS COVERING it\n",
    "    executed_lines_test_by_test = json_keys_to_int(json.load(file)) #{program1:{line:[tci,tcj,...,tck],line2:...}\n",
    "with open(\"datasets/sir_programs/faults_dictionary.json\", \"r\") as file:\n",
    "    #dictionary that associates at each SIR PROGRAM the LIST of PAST FAULT COVERAGE VALUES ORDERED BY TEST \n",
    "    faults_dictionary = json.load(file) #{program1:[fault_cov_tc1,fault_cov_tc2,...,fault_cov_tcn],program2:...}\n",
    "with open(\"datasets/sir_programs/test_coverage_line_by_line.json\", \"r\") as file:\n",
    "    #dictionary that, for each sir program, associates at each TEST of that program the LIST of LINES COVERED by it\n",
    "    test_coverage_line_by_line = json_keys_to_int(json.load(file)) #{program1:{tc1:[linei,linej,...,linek],tc2:...}\n",
    "with open(\"datasets/sir_programs/test_cases_costs.json\", \"r\") as file:\n",
    "    #dictionary that, for each sir program, associates at each TEST its EXECUTION COST\n",
    "    test_cases_costs = json_keys_to_int(json.load(file)) #{program1:{tc1:ex_cost1,tc2:ex_cost2,...,tcn:ex_costn},program2:...}\n",
    "with open(\"datasets/sir_programs/total_program_lines.json\", \"r\") as file:\n",
    "    #dictionary wich associates at each SIR PROGRAM its size in terms of NUMBER OF ITS LINES\n",
    "    total_program_lines = json.load(file) #{program1:tot_lines_program1,program2:tot_lines_program2,program3:...}"
   ],
   "id": "b83ff28f4ebbee7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def num_of_covered_lines(sir_program,test_cases):\n",
    "    covered_lines = set()\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        try:\n",
    "            for covered_line in test_coverage_line_by_line[sir_program][test_case]:\n",
    "                covered_lines.add(covered_line)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return len(covered_lines)\n",
    "\n",
    "clusters_dictionary = dict()\n",
    "\n",
    "for sir_program in sir_programs:\n",
    "    tot_test_cases = sir_programs_tests_number[sir_program]\n",
    "    \n",
    "    # from {..., test_case_i : [cov_stmts], ...} to [..., #_stmt_cov_i, ...]\n",
    "    test_cases_stmt_cov = []\n",
    "    for test_case in test_coverage_line_by_line[sir_program].keys():\n",
    "        test_cases_stmt_cov.append(len(test_coverage_line_by_line[sir_program][test_case]))\n",
    "    suite_stmt_cov = sum(test_cases_stmt_cov)\n",
    "    \n",
    "    # Normalize data\n",
    "    data = np.column_stack((list(test_cases_costs[sir_program].values()),faults_dictionary[sir_program],test_cases_stmt_cov))\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "    num_clusters = 0\n",
    "    \n",
    "    #ATTENTION: this number also depends on the QAOA simulator/machine you will use\n",
    "    if sir_program == \"flex\":\n",
    "        num_clusters = 55\n",
    "    elif sir_program == \"grep\":\n",
    "        num_clusters = 105\n",
    "    elif sir_program == \"gzip\":\n",
    "        num_clusters = 195\n",
    "    elif sir_program == \"sed\":\n",
    "        num_clusters = 195\n",
    "    \n",
    "    # Step 2: Perform K-Means Clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=43)\n",
    "    start = time.time()\n",
    "    clusters = kmeans.fit_predict(normalized_data)\n",
    "    \n",
    "    # Organize test cases by cluster\n",
    "    clustered_data = defaultdict(list)\n",
    "    for idx, cluster_id in enumerate(clusters):\n",
    "        clustered_data[cluster_id].append(idx)\n",
    "        \n",
    "    # Step 4: Reassign points to ensure each cluster has at most 30 elements\n",
    "    max_elements_per_cluster = 30\n",
    "    for cluster_id, points in list(clustered_data.items()):\n",
    "        if len(points) > max_elements_per_cluster:\n",
    "            excess_points = points[max_elements_per_cluster:]\n",
    "            clustered_data[cluster_id] = points[:max_elements_per_cluster]\n",
    "            \n",
    "            # Reassign excess points\n",
    "            excess_data = normalized_data[excess_points]\n",
    "            remaining_clusters = [k for k in clustered_data if len(clustered_data[k]) < max_elements_per_cluster]\n",
    "            \n",
    "            # Find nearest cluster with space for each excess point\n",
    "            distances = cdist(excess_data, kmeans.cluster_centers_[remaining_clusters])\n",
    "            nearest_clusters = np.argmin(distances, axis=1)\n",
    "            \n",
    "            for i, excess_idx in enumerate(excess_points):\n",
    "                new_cluster = remaining_clusters[nearest_clusters[i]]\n",
    "                clustered_data[new_cluster].append(excess_idx)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"SelectQAOA Decomposition Time (ms): \" + str((end-start)*1000))\n",
    "    \n",
    "    clusters_dictionary[sir_program] = clustered_data\n",
    "        \n",
    "    # Step 3: Calculate the metrics for each cluster and validate\n",
    "    cluster_metrics = {}\n",
    "    for cluster_id in clustered_data.keys():\n",
    "        tot_cluster_exec_cost = 0\n",
    "        tot_cluster_past_fault_cov = 0\n",
    "        tot_cluster_stmt_cov = 0\n",
    "        for test_case in clustered_data[cluster_id]:\n",
    "            tot_cluster_exec_cost += test_cases_costs[sir_program][test_case]\n",
    "            tot_cluster_past_fault_cov += faults_dictionary[sir_program][test_case]\n",
    "        tot_cluster_past_fault_cov = tot_cluster_past_fault_cov/tot_test_cases\n",
    "        tot_cluster_stmt_cov = num_of_covered_lines(sir_program,clustered_data[cluster_id])/total_program_lines[sir_program]\n",
    "        cluster_metrics[cluster_id] = {\n",
    "            \"tot_exec_cost\": tot_cluster_exec_cost,\n",
    "            \"tot_past_fault_cov\": tot_cluster_past_fault_cov,\n",
    "            \"tot_stmt_cov\": tot_cluster_stmt_cov  # Avg stmt coverage per test case in cluster\n",
    "        }\n",
    "        print(f\"Cluster {cluster_id + 1} metrics:\")\n",
    "        print(f\"Test Cases: {clustered_data[cluster_id]}\")\n",
    "        print(f\" - Num. Test Cases: {len(clustered_data[cluster_id]):.2f}\")\n",
    "        print(f\" - Execution Cost: {tot_cluster_exec_cost:.2f}\")\n",
    "        print(f\" - Past Fault Coverage (%): {tot_cluster_past_fault_cov}\")\n",
    "        print(f\" - Statement Coverage (%): {tot_cluster_stmt_cov:.2f}\\n\")\n",
    "    \n",
    "    for cluster_id in clustered_data.keys():\n",
    "        print(\"Test cases of cluster \" + str(cluster_id) + \": \" + str(len(clustered_data[cluster_id])))\n",
    "    \n",
    "    print(\"======================================================================================\")\n",
    "    \n",
    "    print(\"Program Name: \" + sir_program)\n",
    "    \n",
    "    for cluster_id in clustered_data.keys():\n",
    "        if len(clustered_data[cluster_id]) > max_elements_per_cluster:\n",
    "            print(\"Test cases of cluster \" + str(cluster_id) + \": \" + str(len(clustered_data[cluster_id])))\n",
    "    \n",
    "    # Plotting the clusters in 3D space\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Extracting data for plotting\n",
    "    exec_costs = np.array(list(test_cases_costs[sir_program].values()))\n",
    "    fault_covs = np.array(faults_dictionary[sir_program])\n",
    "    stmt_covs = np.array(test_cases_stmt_cov)\n",
    "    \n",
    "    # Plot each cluster with a different color\n",
    "    colors = plt.cm.get_cmap(\"tab10\", num_clusters)  # A colormap with as many colors as clusters\n",
    "    for cluster_id in clustered_data.keys():\n",
    "        cluster_indices = clustered_data[cluster_id]\n",
    "        \n",
    "        # Plot each cluster's points\n",
    "        ax.scatter(\n",
    "            exec_costs[cluster_indices], \n",
    "            fault_covs[cluster_indices], \n",
    "            stmt_covs[cluster_indices], \n",
    "            color=colors(cluster_id), \n",
    "            label=f\"Cluster {cluster_id + 1}\"\n",
    "        )\n",
    "    \n",
    "    # Label the axes\n",
    "    ax.set_xlabel(\"Execution Cost\")\n",
    "    ax.set_ylabel(\"Past Fault Coverage\")\n",
    "    ax.set_zlabel(\"Statement Coverage\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Test Case Clustering Visualization\")\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "print(clusters_dictionary)"
   ],
   "id": "b4af9e0f1faef6e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def make_linear_terms(sir_program, cluster_test_cases, alpha):\n",
    "    max_cost = max(test_cases_costs[sir_program].values())\n",
    "    \n",
    "    estimated_costs = []\n",
    "\n",
    "    #linear coefficients, that are the diagonal of the matrix encoding the QUBO\n",
    "    for test_case in cluster_test_cases:\n",
    "        estimated_costs.append((alpha * (test_cases_costs[sir_program][test_case]/max_cost)) - (1 - alpha) * faults_dictionary[sir_program][test_case])\n",
    "    \n",
    "    return np.array(estimated_costs)\n",
    "\n",
    "def make_quadratic_terms(sir_program, variables, cluster_test_cases, linear_terms, penalty):\n",
    "    quadratic_terms = {}\n",
    "    \n",
    "    #k is a stmt\n",
    "    for k in executed_lines_test_by_test[sir_program].keys():\n",
    "        #k_test_cases is the list of test cases covering k\n",
    "        k_test_cases = executed_lines_test_by_test[sir_program][k]\n",
    "        for i in k_test_cases:\n",
    "            if i not in cluster_test_cases or i not in variables:\n",
    "                continue\n",
    "            for j in k_test_cases:\n",
    "                if j not in cluster_test_cases or j not in variables:\n",
    "                    continue\n",
    "                if i < j:\n",
    "                    linear_terms[variables.index(i)] -= penalty\n",
    "                    try:\n",
    "                        quadratic_terms[variables.index(i),variables.index(j)] += 2 * penalty\n",
    "                    except:\n",
    "                        quadratic_terms[variables.index(i),variables.index(j)] = 2 * penalty\n",
    "    \n",
    "    return quadratic_terms"
   ],
   "id": "3d366a8ac17bab8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_QUBO_problem(linear_terms, quadratic_terms):\n",
    "    \"\"\"This function is the one that has to encode the QUBO problem that QAOA will have to solve. The QUBO problem specifies the optimization to solve and a quadratic binary unconstrained problem\"\"\"\n",
    "    qubo = QuadraticProgram()\n",
    "    \n",
    "    for i in range(0,len(linear_terms)):\n",
    "        qubo.binary_var('x%s' % (i))\n",
    "\n",
    "    qubo.minimize(linear=linear_terms,quadratic=quadratic_terms)\n",
    "\n",
    "    return qubo\n"
   ],
   "id": "f5ab57d8db7cd5f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "penalties_dictionary = {\"flex\":None,\"grep\":None,\"gzip\":None,\"sed\":None}\n",
    "\n",
    "#to obtain a QUBO problem from a quadratic problem with constraints, we have to insert those constraints into the Hamiltonian to solve (which is the one encoded by the QUBO problem). When we insert constraint into the Hamiltonian, we have to specify also penalties\n",
    "for sir_program in sir_programs:\n",
    "    max_penalty = 0\n",
    "    max_cost = max(test_cases_costs[sir_program].values())\n",
    "    for i in range(sir_programs_tests_number[sir_program]):\n",
    "        cost = (alpha * (test_cases_costs[sir_program][i]/max_cost)) - ((1 - alpha) * faults_dictionary[sir_program][i])\n",
    "        if cost > max_penalty:\n",
    "            max_penalty = cost\n",
    "    penalties_dictionary[sir_program] = max_penalty + 1"
   ],
   "id": "385149246db97f3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "qubos_dictionary = {\"flex\":[],\"grep\":[],\"gzip\":[],\"sed\":[]}\n",
    "#make a dictionary that saves, for each program, the correspondent QUBO\n",
    "for sir_program in sir_programs:\n",
    "    print(\"SIR Program:\\n\")\n",
    "    for cluster_id in clusters_dictionary[sir_program]:\n",
    "        print(\"Cluster ID: \" + str(cluster_id))\n",
    "        variables = []\n",
    "        for idx in range(0, len(clusters_dictionary[sir_program][cluster_id])):\n",
    "            variables.append(idx)\n",
    "        linear_terms = make_linear_terms(sir_program, clusters_dictionary[sir_program][cluster_id], alpha)\n",
    "        quadratic_terms = make_quadratic_terms(sir_program, variables, clusters_dictionary[sir_program][cluster_id], linear_terms, penalties_dictionary[sir_program])\n",
    "        qubo = create_QUBO_problem(linear_terms, quadratic_terms)\n",
    "        qubos_dictionary[sir_program].append(qubo)\n",
    "        print(qubo)\n",
    "        print(\"/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/\")\n",
    "    print(\"======================================================================================\")"
   ],
   "id": "52f80d819dc480e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def covered_lines(sir_program,test_cases_list):\n",
    "    covered_lines = set()\n",
    "    \n",
    "    for test_case in test_cases_list:\n",
    "        try:\n",
    "            for covered_line in test_coverage_line_by_line[sir_program][test_case]:\n",
    "                covered_lines.add(covered_line)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return len(covered_lines)\n",
    "\n",
    "def build_pareto_front(sir_program,selected_tests):\n",
    "    pareto_front = []\n",
    "    max_fault_coverage = 0\n",
    "    max_stmt_coverage = 0\n",
    "    \n",
    "    for index in range(1,len(selected_tests)+1):\n",
    "        #exract the first index selected tests\n",
    "        candidate_solution = selected_tests[:index]\n",
    "        candidate_solution_fault_coverage = 0\n",
    "        candidate_solution_stmt_coverage = 0\n",
    "        for selected_test in candidate_solution:\n",
    "            candidate_solution_fault_coverage += faults_dictionary[sir_program][selected_test]\n",
    "            candidate_solution_stmt_coverage += covered_lines(sir_program,candidate_solution)\n",
    "        #if the actual pareto front dominates the candidate solution, get to the next candidate\n",
    "        if max_fault_coverage >= candidate_solution_fault_coverage and max_stmt_coverage >= candidate_solution_stmt_coverage:\n",
    "            continue\n",
    "        #eventually update the pareto front information\n",
    "        if candidate_solution_stmt_coverage > max_stmt_coverage:\n",
    "            max_stmt_coverage = candidate_solution_stmt_coverage\n",
    "        if candidate_solution_fault_coverage > max_fault_coverage:\n",
    "            max_fault_coverage = candidate_solution_fault_coverage\n",
    "        #add the candidate solution to the pareto front\n",
    "        pareto_front.append(candidate_solution)\n",
    "    \n",
    "    return pareto_front"
   ],
   "id": "145df30b2cfd521b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#I want to run the sampler 30 times to get different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    reps = [1,2,4,8,16]\n",
    "    for rep in reps:\n",
    "        sim_ideal = AerSimulator()\n",
    "        algorithm_globals.random_seed = 10598\n",
    "        qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_ideal), optimizer=COBYLA(100), reps=rep)\n",
    "        qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "        #the fronts will be saved into files\n",
    "        print(\"SIR Program: \" + sir_program)\n",
    "        file_path = \"results/selectqaoa/ideal/\" + sir_program + \"-data-rep-\" + str(rep) + \".json\"\n",
    "        json_data = {}\n",
    "        response = None\n",
    "        qpu_run_times = []\n",
    "        pareto_fronts_building_times = []\n",
    "        for i in range(experiments):\n",
    "            final_selected_tests = []\n",
    "            cluster_dict_index = 0\n",
    "            for qubo in qubos_dictionary[sir_program]:\n",
    "                print(\"QUBO Problem: \" + str(qubo) + \"\\n Cluster Number: \" + str(cluster_dict_index))\n",
    "                print(\"Cluster's Test Cases: \" +str(list(clusters_dictionary[sir_program].values())[cluster_dict_index]))\n",
    "                #for each iteration get the result\n",
    "                s = time.time()\n",
    "                qaoa_result = qaoa.solve(qubo)\n",
    "                print(\"RESULTS: \" + str(qaoa_result))\n",
    "                e = time.time()\n",
    "                qpu_run_times.append((e - s) * 1000)\n",
    "                #let's extract the selected tests\n",
    "                variable_values = qaoa_result.x\n",
    "                indexes_selected_tests = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "                print(\"Indexes of selected tests to convert. \" + str(indexes_selected_tests))\n",
    "                selected_tests = []\n",
    "                for index in indexes_selected_tests:\n",
    "                    selected_tests.append(list(clusters_dictionary[sir_program].values())[cluster_dict_index][index])\n",
    "                print(\"Selected tests: \" + str(selected_tests))\n",
    "                print(\"Experiment Number: \" + str(i))\n",
    "                cluster_dict_index += 1\n",
    "                for selected_test in selected_tests:\n",
    "                    if selected_test not in final_selected_tests:\n",
    "                        final_selected_tests.append(selected_test)\n",
    "            i+=1\n",
    "            #now we have to build the pareto front\n",
    "            print(\"Final Selected Test Cases: \" + str(final_selected_tests))\n",
    "            print(\"Length of the final list of selected test cases: \" + str(len(final_selected_tests)))\n",
    "            start = time.time()\n",
    "            pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "            end = time.time()\n",
    "            json_data[\"pareto_front_\" + str(i)] = pareto_front\n",
    "            pareto_front_building_time = (end - start) * 1000\n",
    "            pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "\n",
    "    #compute the average time needed for the construction of a pareto frontier and run time\n",
    "    mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "    mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "    json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "    json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file)\n"
   ],
   "id": "41279499a02a02b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
