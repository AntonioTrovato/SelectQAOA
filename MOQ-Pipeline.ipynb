{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df92c7f16f61fcc",
   "metadata": {},
   "source": [
    "# SelectQAOA: Regression Test Case Selection Using QAOAs\n",
    "Regression testing is an important part of the software development process in software engineering. It is a practice aimed at identifying any regression, which is the emergence of new defects or issues in a software application following changes, enhancements, or updates made to the source code. In other words, regression testing focuses on how changes made to the software can affect the correct behavior of existing features. Regression testing is particularly important in agile software development environments, where changes are made frequently and rapidly. This practice helps ensure that the software remains stable and reliable as it evolves over time. Ideal regression testing would re-run all the available test cases of a given software system. However, in addition to being potentially very costly, this could even be impractical in some case. In this scenario, test case selection is one of the most widely investigated test suite optimization approaches.\n",
    "Test case selection focuses on selecting a subset from an initial test suite to test software changes, i.e., to test whether unmodified parts of a program continue to work correctly after changes involving other parts. Furthermore, while in Test Case Minimization, not selecting a test case means permanently removing it; in Test Case Selection, the identified subset of the starting test suite is bounded to a specific execution of regression testing. Various techniques, such as Integer Programming, symbolic execution, data flow analysis, dependence graph-based methods, and flow graph-based approaches, can be employed to identify the modified portions of the software. Once test cases covering the unchanged program segments are pinpointed using a specific technique, an optimization algorithm (e.g., additional greedy, DIV-GA,\n",
    "SelectQA, BootQA or SelectQAOA) can select a minimal set of these test cases based on certain testing criteria (e.g., branch coverage). The ultimate aim is to reduce the expenses associated with regression testing."
   ]
  },
  {
   "cell_type": "code",
   "id": "26cfee6323f1164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:25:48.598750Z",
     "start_time": "2025-03-11T16:25:45.372641Z"
    }
   },
   "source": [
    "#this cell contains all the imports needed by the pipeline\n",
    "#to run it on the browser: jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import statistics\n",
    "import ast\n",
    "import csv\n",
    "\n",
    "from itertools import combinations\n",
    "from scipy.stats import mannwhitneyu, shapiro, kruskal\n",
    "from qiskit_optimization import QuadraticProgram\n",
    "from qiskit_optimization.algorithms import MinimumEigenOptimizer\n",
    "from qiskit_algorithms.optimizers import COBYLA\n",
    "from qiskit.primitives import BackendSampler\n",
    "from qiskit_algorithms import QAOA\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit_aer import Aer, AerSimulator\n",
    "from qiskit_aer.noise import NoiseModel, depolarizing_error\n",
    "from qiskit_ibm_runtime.fake_provider import FakeVigoV2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cdist"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "f36c35765771b1a0",
   "metadata": {},
   "source": [
    "## Multi-Objective SelectQAOA\n",
    "The first part of this work aims to compare SelectQAOA to the Multi-Objective DIV-GA, Additional Greedy, and SelectQA in terms of effectiveness and efficiency.\n",
    "So, we will first focus on the implementation of a Multi-Objective version of SelectQAOA.\n",
    "\n",
    "### The pipeline dataset\n",
    "To experiment the performance of the proposed solution by this work and to compare its results to those of the classical state-of-the-art solutions, four public programs have been downloaded from the SIR website. SIR is a repository of software-related artifacts meant to support rigorous controlled experimentation with program analysis and software testing techniques, and education in controlled experimentation. \n",
    "\n",
    "### Chosen SIR Programs\n",
    "The programs that will be used for experimentation have all been written in C and are:\n",
    "- flex (a program that generates a lexical analysis program, based on regular expressions and C statement contained in one or more input files);\n",
    "- grep (a useful program to search for matching patterns in a file);\n",
    "- gzip (a program that substitutes a file, generally text files or web pages, with their compressed version)\n",
    "- sed (a powerful program for stream text editing).\n",
    "\n",
    "### Necessary information\n",
    "The information needed by the quantum algorithm to work on every one of the four programs is:\n",
    "- a fault matrix: it indicates whether a precise test case already found, during previous execution, a bug in the source code or not;\n",
    "- execution cost: it indicates the execution cost of any test case of the suite;\n",
    "- statement coverage: it indicates statement coverage information for every test case. \n",
    "\n",
    "All this information has been gathered through previous experimentation on the four programs mentioned above and written in files organized in the SIR_Programs folder. So, the first goal of the project will be gathering data from these files for computational purposes."
   ]
  },
  {
   "cell_type": "code",
   "id": "7c35ee998a723ebe",
   "metadata": {},
   "source": [
    "#this cell contains all variable definitions that will be useful throughout the entire project\n",
    "sir_programs = [\"flex\",\"grep\",\"gzip\",\"sed\"]\n",
    "sir_programs_tests_number = {\"flex\":567,\"grep\":806,\"gzip\":214,\"sed\":360}\n",
    "sir_programs_end_lines = {\"flex\":14192,\"grep\":13281,\"gzip\":6701,\"sed\":7118}\n",
    "sir_programs_rep_values = {\"flex\":1,\"grep\":16,\"gzip\":1,\"sed\":1}\n",
    "alpha = 0.5\n",
    "experiments = 10"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def json_keys_to_int(d):\n",
    "    \"\"\"This method correctly converts the data\"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        return {int(k) if k.isdigit() else k: json_keys_to_int(v) for k, v in d.items()}\n",
    "    elif isinstance(d, list):\n",
    "        return [json_keys_to_int(i) for i in d]\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "\n",
    "with open(\"datasets/sir_programs/executed_lines_test_by_test.json\", \"r\") as file:\n",
    "    #dictionary that, for each sir program, associates at each LINE of that program the LIST of TESTS COVERING it\n",
    "    executed_lines_test_by_test = json_keys_to_int(json.load(file)) #{program1:{line:[tci,tcj,...,tck],line2:...}\n",
    "with open(\"datasets/sir_programs/faults_dictionary.json\", \"r\") as file:\n",
    "    #dictionary that associates at each SIR PROGRAM the LIST of PAST FAULT COVERAGE VALUES ORDERED BY TEST \n",
    "    faults_dictionary = json.load(file) #{program1:[fault_cov_tc1,fault_cov_tc2,...,fault_cov_tcn],program2:...}\n",
    "with open(\"datasets/sir_programs/test_coverage_line_by_line.json\", \"r\") as file:\n",
    "    #dictionary that, for each sir program, associates at each TEST of that program the LIST of LINES COVERED by it\n",
    "    test_coverage_line_by_line = json_keys_to_int(json.load(file)) #{program1:{tc1:[linei,linej,...,linek],tc2:...}\n",
    "with open(\"datasets/sir_programs/test_cases_costs.json\", \"r\") as file:\n",
    "    #dictionary that, for each sir program, associates at each TEST its EXECUTION COST\n",
    "    test_cases_costs = json_keys_to_int(json.load(file)) #{program1:{tc1:ex_cost1,tc2:ex_cost2,...,tcn:ex_costn},program2:...}\n",
    "with open(\"datasets/sir_programs/total_program_lines.json\", \"r\") as file:\n",
    "    #dictionary which associates at each SIR PROGRAM its size in terms of the NUMBER OF ITS LINES\n",
    "    total_program_lines = json.load(file) #{program1:tot_lines_program1,program2:tot_lines_program2,program3:...}"
   ],
   "id": "7ef8cfdb01630b6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec65b50547d46586",
   "metadata": {},
   "source": [
    "## Quantum Approximate Optimization Algorithm (QAOA)\n",
    "The Quantum Approximate Optimization Algorithm (QAOA) is a quantum algorithm designed to tackle combinatorial optimization problems by combining classical and quantum computing techniques. It encodes the optimization problem into a Quantum Unconstrained Binary Optimization (QUBO) format and constructs a parameterized quantum circuit that alternates between applying the problem Hamiltonian, which represents the optimization goal, and a mixing Hamiltonian, which promotes exploration of the solution space. The parameters of this circuit are optimized using classical optimization methods to maximize the likelihood of measuring the optimal solution. Once the parameters are tuned, the circuit is executed on a quantum device or simulator to get candidate solutions, making QAOA particularly suitable for NP-hard problems like graph partitioning and maximum cut. This hybrid approach highlights the potential of quantum computing to provide advantages in solving complex optimization challenges.\n",
    "\n",
    "## QUBO Problems\n",
    "A Quadratic Unconstrained Binary Optimization (QUBO) problem is a type of mathematical problem where we seek to find the best combination of binary values (0 or 1) for a set of variables to minimize or maximize an objective function. In other words, we are looking for the optimal solution among all possible variable combinations that satisfies certain constraints and makes the objective function as small as possible.\n",
    "\n",
    "Weights for constraints (or penalty coefficients) are used in QUBO problems to assign a numerical value to the constraints and influence the optimization process. These weights are important because they allow for managing the priority and relative importance of constraints within the optimization problem.\n",
    "\n",
    "## Problem Decomposition with Clustering\n",
    "The qubit availability is the most significant limitation of the currently available quantum computers.\n",
    "Therefore, the test suites to optimize are decomposed into smaller sub-suites solvable by QAOA simulators using clustering techniques. Applying clustering allows for preserving the similarities between test cases in sub-suites, and since similar test cases tend to have redundant coverage, the optimization process will facilitate useless duplications. Also, the similarities between test cases of the same clusters imply a diversified representation of the initial suite, facilitating targeted balancing of the goal while building a diversified final sub-suite."
   ]
  },
  {
   "cell_type": "code",
   "id": "9224a4794535ffef",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "def num_of_covered_lines(sir_program,test_cases):\n",
    "    \"\"\"This method returns the number of covered lines (no redundancy)\"\"\"\n",
    "    covered_lines = set()\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        try:\n",
    "            for covered_line in test_coverage_line_by_line[sir_program][test_case]:\n",
    "                covered_lines.add(covered_line)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return len(covered_lines)\n",
    "\n",
    "clusters_dictionary = dict()\n",
    "\n",
    "for sir_program in sir_programs:\n",
    "    tot_test_cases = sir_programs_tests_number[sir_program]\n",
    "    \n",
    "    # from {..., test_case_i : [cov_stmts], ...} to [..., #_stmt_cov_i, ...]\n",
    "    test_cases_stmt_cov = []\n",
    "    for test_case in test_coverage_line_by_line[sir_program].keys():\n",
    "        test_cases_stmt_cov.append(len(test_coverage_line_by_line[sir_program][test_case]))\n",
    "    suite_stmt_cov = sum(test_cases_stmt_cov)\n",
    "    \n",
    "    # Normalize data\n",
    "    data = np.column_stack((list(test_cases_costs[sir_program].values()),faults_dictionary[sir_program],test_cases_stmt_cov))\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "    num_clusters = 0\n",
    "    \n",
    "    #ATTENTION: this number also depends on the QAOA simulator/machine you will use\n",
    "    if sir_program == \"flex\":\n",
    "        num_clusters = 55\n",
    "    elif sir_program == \"grep\":\n",
    "        num_clusters = 105\n",
    "    elif sir_program == \"gzip\":\n",
    "        num_clusters = 195\n",
    "    elif sir_program == \"sed\":\n",
    "        num_clusters = 195\n",
    "    \n",
    "    # Step 2: Perform K-Means Clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=43)\n",
    "    start = time.time()\n",
    "    clusters = kmeans.fit_predict(normalized_data)\n",
    "    \n",
    "    # Organize test cases by cluster\n",
    "    clustered_data = defaultdict(list)\n",
    "    for idx, cluster_id in enumerate(clusters):\n",
    "        clustered_data[cluster_id].append(idx)\n",
    "        \n",
    "    # Step 4: Reassign points to ensure each cluster has at most 30 elements\n",
    "    max_elements_per_cluster = 30\n",
    "    for cluster_id, points in list(clustered_data.items()):\n",
    "        if len(points) > max_elements_per_cluster:\n",
    "            excess_points = points[max_elements_per_cluster:]\n",
    "            clustered_data[cluster_id] = points[:max_elements_per_cluster]\n",
    "            \n",
    "            # Reassign excess points\n",
    "            excess_data = normalized_data[excess_points]\n",
    "            remaining_clusters = [k for k in clustered_data if len(clustered_data[k]) < max_elements_per_cluster]\n",
    "            \n",
    "            # Find nearest cluster with space for each excess point\n",
    "            distances = cdist(excess_data, kmeans.cluster_centers_[remaining_clusters])\n",
    "            nearest_clusters = np.argmin(distances, axis=1)\n",
    "            \n",
    "            for i, excess_idx in enumerate(excess_points):\n",
    "                new_cluster = remaining_clusters[nearest_clusters[i]]\n",
    "                clustered_data[new_cluster].append(excess_idx)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"SelectQAOA Decomposition Time (ms): \" + str((end-start)*1000))\n",
    "    \n",
    "    clusters_dictionary[sir_program] = clustered_data\n",
    "        \n",
    "    # Step 3: Calculate the metrics for each cluster and validate\n",
    "    cluster_metrics = {}\n",
    "    for cluster_id in clustered_data.keys():\n",
    "        tot_cluster_exec_cost = 0\n",
    "        tot_cluster_past_fault_cov = 0\n",
    "        tot_cluster_stmt_cov = 0\n",
    "        for test_case in clustered_data[cluster_id]:\n",
    "            tot_cluster_exec_cost += test_cases_costs[sir_program][test_case]\n",
    "            tot_cluster_past_fault_cov += faults_dictionary[sir_program][test_case]\n",
    "        tot_cluster_past_fault_cov = tot_cluster_past_fault_cov/tot_test_cases\n",
    "        tot_cluster_stmt_cov = num_of_covered_lines(sir_program,clustered_data[cluster_id])/total_program_lines[sir_program]\n",
    "        cluster_metrics[cluster_id] = {\n",
    "            \"tot_exec_cost\": tot_cluster_exec_cost,\n",
    "            \"tot_past_fault_cov\": tot_cluster_past_fault_cov,\n",
    "            \"tot_stmt_cov\": tot_cluster_stmt_cov  # Avg stmt coverage per test case in cluster\n",
    "        }\n",
    "        print(f\"Cluster {cluster_id + 1} metrics:\")\n",
    "        print(f\"Test Cases: {clustered_data[cluster_id]}\")\n",
    "        print(f\" - Num. Test Cases: {len(clustered_data[cluster_id]):.2f}\")\n",
    "        print(f\" - Execution Cost: {tot_cluster_exec_cost:.2f}\")\n",
    "        print(f\" - Past Fault Coverage (%): {tot_cluster_past_fault_cov}\")\n",
    "        print(f\" - Statement Coverage (%): {tot_cluster_stmt_cov:.2f}\\n\")\n",
    "    \n",
    "    for cluster_id in clustered_data.keys():\n",
    "        print(\"Test cases of cluster \" + str(cluster_id) + \": \" + str(len(clustered_data[cluster_id])))\n",
    "    \n",
    "    print(\"======================================================================================\")\n",
    "    \n",
    "    print(\"Program Name: \" + sir_program)\n",
    "    \n",
    "    for cluster_id in clustered_data.keys():\n",
    "        if len(clustered_data[cluster_id]) > max_elements_per_cluster:\n",
    "            print(\"Test cases of cluster \" + str(cluster_id) + \": \" + str(len(clustered_data[cluster_id])))\n",
    "    \n",
    "    # Plotting the clusters in 3D space\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Extracting data for plotting\n",
    "    exec_costs = np.array(list(test_cases_costs[sir_program].values()))\n",
    "    fault_covs = np.array(faults_dictionary[sir_program])\n",
    "    stmt_covs = np.array(test_cases_stmt_cov)\n",
    "    \n",
    "    # Plot each cluster with a different color\n",
    "    colors = plt.cm.get_cmap(\"tab10\", num_clusters)  # A colormap with as many colors as clusters\n",
    "    for cluster_id in clustered_data.keys():\n",
    "        cluster_indices = clustered_data[cluster_id]\n",
    "        \n",
    "        # Plot each cluster's points\n",
    "        ax.scatter(\n",
    "            exec_costs[cluster_indices], \n",
    "            fault_covs[cluster_indices], \n",
    "            stmt_covs[cluster_indices], \n",
    "            color=colors(cluster_id), \n",
    "            label=f\"Cluster {cluster_id + 1}\"\n",
    "        )\n",
    "    \n",
    "    # Label the axes\n",
    "    ax.set_xlabel(\"Execution Cost\")\n",
    "    ax.set_ylabel(\"Past Fault Coverage\")\n",
    "    ax.set_zlabel(\"Statement Coverage\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Test Case Clustering Visualization\")\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "print(clusters_dictionary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "97b42e5ea26ae8a4",
   "metadata": {},
   "source": [
    "def make_linear_terms(sir_program, cluster_test_cases, alpha):\n",
    "    \"\"\"Making the linear terms of QUBO\"\"\"\n",
    "    max_cost = max(test_cases_costs[sir_program].values())\n",
    "    \n",
    "    estimated_costs = []\n",
    "\n",
    "    #linear coefficients, that are the diagonal of the matrix encoding the QUBO\n",
    "    for test_case in cluster_test_cases:\n",
    "        estimated_costs.append((alpha * (test_cases_costs[sir_program][test_case]/max_cost)) - (1 - alpha) * faults_dictionary[sir_program][test_case])\n",
    "    \n",
    "    return np.array(estimated_costs)\n",
    "\n",
    "def make_quadratic_terms(sir_program, variables, cluster_test_cases, linear_terms, penalty):\n",
    "    \"\"\"Making the quadratic terms of QUBO\"\"\"\n",
    "    quadratic_terms = {}\n",
    "    \n",
    "    #k is a stmt\n",
    "    for k in executed_lines_test_by_test[sir_program].keys():\n",
    "        #k_test_cases is the list of test cases covering k\n",
    "        k_test_cases = executed_lines_test_by_test[sir_program][k]\n",
    "        for i in k_test_cases:\n",
    "            if i not in cluster_test_cases or i not in variables:\n",
    "                continue\n",
    "            for j in k_test_cases:\n",
    "                if j not in cluster_test_cases or j not in variables:\n",
    "                    continue\n",
    "                if i < j:\n",
    "                    linear_terms[variables.index(i)] -= penalty\n",
    "                    try:\n",
    "                        quadratic_terms[variables.index(i),variables.index(j)] += 2 * penalty\n",
    "                    except:\n",
    "                        quadratic_terms[variables.index(i),variables.index(j)] = 2 * penalty\n",
    "    \n",
    "    return quadratic_terms"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "abead0754d567232",
   "metadata": {},
   "source": [
    "def create_QUBO_problem(linear_terms, quadratic_terms):\n",
    "    \"\"\"This function is the one that has to encode the QUBO problem that QAOA will have to solve. The QUBO problem specifies the optimization to solve and a quadratic binary unconstrained problem\"\"\"\n",
    "    qubo = QuadraticProgram()\n",
    "    \n",
    "    for i in range(0,len(linear_terms)):\n",
    "        qubo.binary_var('x%s' % (i))\n",
    "\n",
    "    qubo.minimize(linear=linear_terms,quadratic=quadratic_terms)\n",
    "\n",
    "    return qubo\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ab8c0bce740db9c",
   "metadata": {},
   "source": [
    "penalties_dictionary = {\"flex\":None,\"grep\":None,\"gzip\":None,\"sed\":None}\n",
    "\n",
    "#to get a QUBO problem from a quadratic problem with constraints, we have to insert those constraints into the Hamiltonian to solve (which is the one encoded by the QUBO problem). When we insert constraint into the Hamiltonian, we have to specify also penalties\n",
    "for sir_program in sir_programs:\n",
    "    max_penalty = 0\n",
    "    max_cost = max(test_cases_costs[sir_program].values())\n",
    "    for i in range(sir_programs_tests_number[sir_program]):\n",
    "        cost = (alpha * (test_cases_costs[sir_program][i]/max_cost)) - ((1 - alpha) * faults_dictionary[sir_program][i])\n",
    "        if cost > max_penalty:\n",
    "            max_penalty = cost\n",
    "    penalties_dictionary[sir_program] = max_penalty + 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c7f468b9deadab3",
   "metadata": {},
   "source": [
    "qubos_dictionary = {\"flex\":[],\"grep\":[],\"gzip\":[],\"sed\":[]}\n",
    "#make a dictionary that saves, for each program, the correspondent QUBO\n",
    "for sir_program in sir_programs:\n",
    "    print(\"SIR Program:\\n\")\n",
    "    for cluster_id in clusters_dictionary[sir_program]:\n",
    "        print(\"Cluster ID: \" + str(cluster_id))\n",
    "        variables = []\n",
    "        for idx in range(0, len(clusters_dictionary[sir_program][cluster_id])):\n",
    "            variables.append(idx)\n",
    "        linear_terms = make_linear_terms(sir_program, clusters_dictionary[sir_program][cluster_id], alpha)\n",
    "        quadratic_terms = make_quadratic_terms(sir_program, variables, clusters_dictionary[sir_program][cluster_id], linear_terms, penalties_dictionary[sir_program])\n",
    "        qubo = create_QUBO_problem(linear_terms, quadratic_terms)\n",
    "        qubos_dictionary[sir_program].append(qubo)\n",
    "        print(qubo)\n",
    "        print(\"/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/--/\")\n",
    "    print(\"======================================================================================\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "abaf9693911723af",
   "metadata": {},
   "source": [
    "def covered_lines(sir_program,test_cases_list):\n",
    "    \"\"\"Number of covered lines (no redundancy)\"\"\"\n",
    "    covered_lines = set()\n",
    "    \n",
    "    for test_case in test_cases_list:\n",
    "        try:\n",
    "            for covered_line in test_coverage_line_by_line[sir_program][test_case]:\n",
    "                covered_lines.add(covered_line)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return len(covered_lines)\n",
    "\n",
    "def build_pareto_front(sir_program,selected_tests):\n",
    "    \"\"\"This method builds the pareto front additionally from a sub test suite solution\"\"\"\n",
    "    pareto_front = []\n",
    "    max_fault_coverage = 0\n",
    "    max_stmt_coverage = 0\n",
    "    \n",
    "    for index in range(1,len(selected_tests)+1):\n",
    "        #exract the first index selected tests\n",
    "        candidate_solution = selected_tests[:index]\n",
    "        candidate_solution_fault_coverage = 0\n",
    "        candidate_solution_stmt_coverage = 0\n",
    "        for selected_test in candidate_solution:\n",
    "            candidate_solution_fault_coverage += faults_dictionary[sir_program][selected_test]\n",
    "            candidate_solution_stmt_coverage += covered_lines(sir_program,candidate_solution)\n",
    "        #if the actual pareto front dominates the candidate solution, get to the next candidate\n",
    "        if max_fault_coverage >= candidate_solution_fault_coverage and max_stmt_coverage >= candidate_solution_stmt_coverage:\n",
    "            continue\n",
    "        #eventually update the pareto front information\n",
    "        if candidate_solution_stmt_coverage > max_stmt_coverage:\n",
    "            max_stmt_coverage = candidate_solution_stmt_coverage\n",
    "        if candidate_solution_fault_coverage > max_fault_coverage:\n",
    "            max_fault_coverage = candidate_solution_fault_coverage\n",
    "        #add the candidate solution to the pareto front\n",
    "        pareto_front.append(candidate_solution)\n",
    "    \n",
    "    return pareto_front"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Optimal Multi-Objective SelectQAOA Depth\n",
    "\n",
    "First, we want to analyze whether the depth of the circuit of QAOA affects SelectQAOA. To do this, we have executed 10 independent times SelectQAOA on an IDEAL QAOA simulator for different depth values (1, 2, 4, 8, and 16) for each of the examined SIR programs. Then, we applied the Kruskal-Wallis H-test, to see if SelectQAOA behaves differently with different depth values. Finally, we used the Vargha-Delaney's $\\hat{A}_{12}$ effect size to estimate the differences between the observations and identify the best configuration for the depth parameter.\n",
    "\n",
    "### Kruskal-Wallis H Test\n",
    "\n",
    "The **Kruskal-Wallis H test** is a **non-parametric** statistical test used to determine whether **three or more independent groups** come from the same population. It is an extension of the **Mann-Whitney U test** for multiple groups and serves as a non-parametric alternative to **one-way ANOVA**.\n",
    "\n",
    "- **Null Hypothesis (\\(H_0\\))**: The samples come from the same distribution (no significant difference between groups).\n",
    "- **Alternative Hypothesis (\\(H_A\\))**: At least one group differs significantly from the others.\n",
    "\n",
    "- A **p-value** is computed to assess statistical significance:\n",
    "  - **If \\( p > 0.05 \\)** → Fail to reject \\( H_0 \\) → Groups may come from the same distribution.\n",
    "  - **If \\( p < 0.05 \\)** → Reject \\( H_0 \\) → At least one group differs significantly.\n",
    "\n",
    "### Vargha-Delaney's $\\hat{A}_{12}$ Effect Size\n",
    "\n",
    "The **Vargha-Delaney $\\hat{A}_{12}$ effect size** is a non-parametric measure used to compare two distributions. It quantifies the probability that a randomly chosen value from one group ( X ) is **greater** than a randomly chosen value from another group ( Y ). It is particularly useful for analyzing differences in performance between two methods, conditions, or datasets.\n",
    "- **$\\hat{A}_{12}$ = 0.5**: Both groups have similar distributions.\n",
    "- **$\\hat{A}_{12}$ > 0.5**: Values in \\( X \\) tend to be larger than those in \\( Y \\).\n",
    "- **$\\hat{A}_{12}$ < 0.5**: Values in \\( Y \\) tend to be larger than those in \\( X \\).\n"
   ],
   "id": "ca96f2f7ff88fdec"
  },
  {
   "metadata": {
    "scrolled": true
   },
   "cell_type": "code",
   "source": [
    "#I want to run the sampler 30 times to get different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    reps = [1,2,4,8,16]\n",
    "    for rep in reps:\n",
    "        sim_ideal = AerSimulator()\n",
    "        algorithm_globals.random_seed = 10598\n",
    "        qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_ideal), optimizer=COBYLA(100), reps=rep)\n",
    "        qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "        #the fronts will be saved into files\n",
    "        print(\"SIR Program: \" + sir_program)\n",
    "        file_path = \"results/selectqaoa/ideal/\" + sir_program + \"-data-rep-\" + str(rep) + \".json\"\n",
    "        json_data = {}\n",
    "        response = None\n",
    "        qpu_run_times = []\n",
    "        pareto_fronts_building_times = []\n",
    "        for i in range(experiments):\n",
    "            final_selected_tests = []\n",
    "            cluster_dict_index = 0\n",
    "            for qubo in qubos_dictionary[sir_program]:\n",
    "                print(\"QUBO Problem: \" + str(qubo) + \"\\n Cluster Number: \" + str(cluster_dict_index))\n",
    "                print(\"Cluster's Test Cases: \" +str(list(clusters_dictionary[sir_program].values())[cluster_dict_index]))\n",
    "                #for each iteration get the result\n",
    "                s = time.time()\n",
    "                qaoa_result = qaoa.solve(qubo)\n",
    "                print(\"RESULTS: \" + str(qaoa_result))\n",
    "                e = time.time()\n",
    "                qpu_run_times.append((e - s) * 1000)\n",
    "                #let's extract the selected tests\n",
    "                variable_values = qaoa_result.x\n",
    "                indexes_selected_tests = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "                print(\"Indexes of selected tests to convert. \" + str(indexes_selected_tests))\n",
    "                selected_tests = []\n",
    "                for index in indexes_selected_tests:\n",
    "                    selected_tests.append(list(clusters_dictionary[sir_program].values())[cluster_dict_index][index])\n",
    "                print(\"Selected tests: \" + str(selected_tests))\n",
    "                print(\"Experiment Number: \" + str(i))\n",
    "                cluster_dict_index += 1\n",
    "                for selected_test in selected_tests:\n",
    "                    if selected_test not in final_selected_tests:\n",
    "                        final_selected_tests.append(selected_test)\n",
    "            i+=1\n",
    "            #now we have to build the pareto front\n",
    "            print(\"Final Selected Test Cases: \" + str(final_selected_tests))\n",
    "            print(\"Length of the final list of selected test cases: \" + str(len(final_selected_tests)))\n",
    "            start = time.time()\n",
    "            pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "            end = time.time()\n",
    "            json_data[\"pareto_front_\" + str(i)] = pareto_front\n",
    "            pareto_front_building_time = (end - start) * 1000\n",
    "            pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "\n",
    "    #compute the average time needed for the construction of a pareto frontier and run time\n",
    "    mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "    mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "    json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "    json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file)\n"
   ],
   "id": "8b4789c23259bf42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def total_cost(sir_program,solution):\n",
    "    \"\"\"Total execution cost of the sub suite\"\"\"\n",
    "    solution_cost = 0\n",
    "    \n",
    "    for test_case in solution:\n",
    "        solution_cost += test_cases_costs[sir_program][test_case]\n",
    "\n",
    "    return solution_cost\n",
    "        \n",
    "def total_coverage(sir_program,solution):\n",
    "    \"\"\"Number of covered lines (no redundancy)\"\"\"\n",
    "    covered_lines = set()\n",
    "    \n",
    "    for test_case in solution:\n",
    "        try:\n",
    "            for covered_line in test_coverage_line_by_line[sir_program][test_case]:\n",
    "                covered_lines.add(covered_line)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return len(covered_lines)\n",
    "    \n",
    "def total_faults(sir_program,solution):\n",
    "    \"\"\"Number of covered faults\"\"\"\n",
    "    covered_faults = 0\n",
    "    \n",
    "    for test_case in solution:\n",
    "        covered_faults += faults_dictionary[sir_program][test_case]\n",
    "    \n",
    "    return covered_faults\n",
    "\n",
    "def pareto_dominance(tuple1, tuple2):\n",
    "    \"\"\"This method returns if tuple1 dominates tuple2\"\"\"\n",
    "    # Check if all conditions are satisfied\n",
    "    dominates = (\n",
    "        (tuple2[0] <= tuple1[0]) and \n",
    "        (tuple2[1] >= tuple1[1]) and \n",
    "        (tuple2[2] >= tuple1[2])\n",
    "    )\n",
    "    \n",
    "    # Check if at least one condition is strict\n",
    "    strict = (\n",
    "        (tuple2[0] < tuple1[0]) or \n",
    "        (tuple2[1] > tuple1[1]) or \n",
    "        (tuple2[2] > tuple1[2])\n",
    "    )\n",
    "    \n",
    "    # Return 1 if the second tuple dominates the first, otherwise 0\n",
    "    return 1 if dominates and strict else 0"
   ],
   "id": "c9ada78e44c97be4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#here we make the same but for each of the fronts built by SelectQA and DIV-GA\n",
    "\n",
    "#the following 2 lists will contain much tuples as solutions in each of the 10 pareto fronts\n",
    "#each tuple represent the fitness value for each solution\n",
    "qaoa_ideal_pareto_vectors_1 = []\n",
    "qaoa_ideal_pareto_vectors_2 = []\n",
    "qaoa_ideal_pareto_vectors_4 = []\n",
    "qaoa_ideal_pareto_vectors_8 = []\n",
    "qaoa_ideal_pareto_vectors_16 = []\n",
    "\n",
    "for index in range(0,10):\n",
    "    # Load the JSON file\n",
    "    with open('results/selectqaoa/ideal/sed-data-rep-1.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    #a single solution is a subset of the initial test suite\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        qaoa_ideal_pareto_vectors_1.append((total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)))\n",
    "    #print(len(qaoa_ideal_pareto_vectors_1))\n",
    "    \n",
    "    with open('results/selectqaoa/ideal/sed-data-rep-2.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    #a single solution is a subset of the initial test suite\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        qaoa_ideal_pareto_vectors_2.append((total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)))\n",
    "    #print(len(qaoa_ideal_pareto_vectors_2))\n",
    "    \n",
    "    with open('results/selectqaoa/ideal/sed-data-rep-4.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    #a single solution is a subset of the initial test suite\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        qaoa_ideal_pareto_vectors_4.append((total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)))\n",
    "    #print(len(qaoa_ideal_pareto_vectors_4))\n",
    "    \n",
    "    with open('results/selectqaoa/ideal/sed-data-rep-8.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    #a single solution is a subset of the initial test suite\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        qaoa_ideal_pareto_vectors_8.append((total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)))\n",
    "    #print(len(qaoa_ideal_pareto_vectors_8))\n",
    "    \n",
    "    with open('results/selectqaoa/ideal/sed-data-rep-16.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    #a single solution is a subset of the initial test suite\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        qaoa_ideal_pareto_vectors_16.append((total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)))\n",
    "    #print(len(qaoa_noise2_pareto_vectors))\n",
    "\n",
    "#once we have the pareto vectors from each pareto front obtained by each run, we extract \n",
    "#just the solution not dominated by anyone else\n",
    "#qtcs_pareto_vectors,divga_pareto_vectors,greedy_pareto_vectors,qaoa_ideal_pareto_vectors,qaoa_fakevigo_pareto_vectors,qaoa_noise1_pareto_vectors,qaoa_noise2_pareto_vectors,qaoa_noise5_pareto_vectors\n",
    "total_fronts = [qaoa_ideal_pareto_vectors_1,qaoa_ideal_pareto_vectors_2,qaoa_ideal_pareto_vectors_4,qaoa_ideal_pareto_vectors_8,qaoa_ideal_pareto_vectors_16]\n",
    "reference_pareto = []\n",
    "portions = [0,0,0,0,0]\n",
    "\n",
    "# get the reference frontier\n",
    "for index, front1 in enumerate(total_fronts):\n",
    "    for front_solution1 in front1:\n",
    "        is_dominated = 0\n",
    "        other_fronts = total_fronts[:index] + total_fronts[index+1:]\n",
    "        for front2 in other_fronts:\n",
    "            for front_solution2 in front2:\n",
    "                if pareto_dominance(front_solution1,front_solution2):\n",
    "                    is_dominated = 1\n",
    "                    break\n",
    "            if is_dominated:\n",
    "                break\n",
    "        if not is_dominated:\n",
    "            reference_pareto.append(front_solution1)\n",
    "            portions[index] = portions[index] + 1\n",
    "\n",
    "print(portions)"
   ],
   "id": "d3b1f0a2b0795805",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For SelectQA and DIV-GA we want to compute, for each of the 10 iterations, how many of the solutions of the i-th pareto front were selected by the reference front\n",
    "\n",
    "qaoa_ideal_non_dominated_values_1 = []\n",
    "qaoa_ideal_non_dominated_values_2 = []\n",
    "qaoa_ideal_non_dominated_values_4 = []\n",
    "qaoa_ideal_non_dominated_values_8 = []\n",
    "qaoa_ideal_non_dominated_values_16 = []\n",
    "\n",
    "for index in range(0,10):\n",
    "    #print(\"Iteratrion: \" + str(index))\n",
    "    \n",
    "    qaoa_ideal_non_dominated_1 = 0\n",
    "    qaoa_ideal_non_dominated_2 = 0\n",
    "    qaoa_ideal_non_dominated_4 = 0\n",
    "    qaoa_ideal_non_dominated_8 = 0\n",
    "    qaoa_ideal_non_dominated_16 = 0\n",
    "    \n",
    "    # Load the JSON file\n",
    "    with open('results/selectqaoa/ideal/sed-data-rep-1.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        if (total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)) in reference_pareto:\n",
    "            qaoa_ideal_non_dominated_1 += 1\n",
    "    \n",
    "    qaoa_ideal_non_dominated_values_1.append(qaoa_ideal_non_dominated_1)\n",
    "    #print(qaoa_ideal_non_dominated_1)\n",
    "    \n",
    "    with open('results/selectqaoa/ideal/sed-data-rep-2.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        if (total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)) in reference_pareto:\n",
    "            qaoa_ideal_non_dominated_2 += 1\n",
    "    \n",
    "    qaoa_ideal_non_dominated_values_2.append(qaoa_ideal_non_dominated_1)\n",
    "    #print(qaoa_ideal_non_dominated_1)\n",
    "    \n",
    "    with open('results/selectqaoa/ideal/sed-data-rep-4.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        if (total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)) in reference_pareto:\n",
    "            qaoa_ideal_non_dominated_4 += 1\n",
    "    \n",
    "    qaoa_ideal_non_dominated_values_4.append(qaoa_ideal_non_dominated_4)\n",
    "    #print(qaoa_ideal_non_dominated_4)\n",
    "    \n",
    "    with open('results/selectqaoa/ideal/sed-data-rep-8.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        if (total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)) in reference_pareto:\n",
    "            qaoa_ideal_non_dominated_8 += 1\n",
    "    \n",
    "    qaoa_ideal_non_dominated_values_8.append(qaoa_ideal_non_dominated_4)\n",
    "    #print(qaoa_ideal_non_dominated_4)\n",
    "    \n",
    "    with open('results/selectqaoa/ideal/sed-data-rep-16.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        if (total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)) in reference_pareto:\n",
    "            qaoa_ideal_non_dominated_16 += 1\n",
    "    \n",
    "    qaoa_ideal_non_dominated_values_16.append(qaoa_ideal_non_dominated_8)\n",
    "    #print(qaoa_ideal_non_dominated_8)\n",
    "\n",
    "print(\"SelectQAOA Non Dominated Values (Rep 1)\")\n",
    "print(qaoa_ideal_non_dominated_values_1)\n",
    "print(\"SelectQAOA Non Dominated Mean (Rep 1)\")\n",
    "print(statistics.mean(qaoa_ideal_non_dominated_values_1))\n",
    "print(\"SelectQAOA Non Dominated StDev (Rep 1)\")\n",
    "print(statistics.stdev(qaoa_ideal_non_dominated_values_1))\n",
    "\n",
    "print(\"SelectQAOA Non Dominated Values (Rep 2)\")\n",
    "print(qaoa_ideal_non_dominated_values_2)\n",
    "print(\"SelectQAOA Non Dominated Mean (Rep 2)\")\n",
    "print(statistics.mean(qaoa_ideal_non_dominated_values_2))\n",
    "print(\"SelectQAOA Non Dominated StDev (Rep 2)\")\n",
    "print(statistics.stdev(qaoa_ideal_non_dominated_values_2))\n",
    "\n",
    "print(\"SelectQAOA Non Dominated Values (Rep 4)\")\n",
    "print(qaoa_ideal_non_dominated_values_4)\n",
    "print(\"SelectQAOA Non Dominated Mean (Rep 4)\")\n",
    "print(statistics.mean(qaoa_ideal_non_dominated_values_4))\n",
    "print(\"SelectQAOA Non Dominated StDev (Rep 4)\")\n",
    "print(statistics.stdev(qaoa_ideal_non_dominated_values_4))\n",
    "\n",
    "print(\"SelectQAOA Non Dominated Values (Rep 8)\")\n",
    "print(qaoa_ideal_non_dominated_values_8)\n",
    "print(\"SelectQAOA Non Dominated Mean (Rep 8)\")\n",
    "print(statistics.mean(qaoa_ideal_non_dominated_values_8))\n",
    "print(\"SelectQAOA Non Dominated StDev (Rep 8)\")\n",
    "print(statistics.stdev(qaoa_ideal_non_dominated_values_8))\n",
    "\n",
    "print(\"SelectQAOA Non Dominated Values (Rep 16)\")\n",
    "print(qaoa_ideal_non_dominated_values_16)\n",
    "print(\"SelectQAOA Non Dominated Mean (Rep 16)\")\n",
    "print(statistics.mean(qaoa_ideal_non_dominated_values_16))\n",
    "print(\"SelectQAOA Non Dominated StDev (Rep 16)\")\n",
    "print(statistics.stdev(qaoa_ideal_non_dominated_values_16))"
   ],
   "id": "519ade37bbb42ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Non dominated solutions comparing ideal executions\n",
    "\n",
    "Flex (no differences)\n",
    "list_r1 = [13, 9, 42, 13, 28, 12, 17, 12, 19, 127]\n",
    "list_r2 = [13, 9, 42, 13, 28, 12, 17, 12, 19, 127]\n",
    "list_r4 = [11, 13, 20, 13, 13, 14, 14, 121, 129, 15]\n",
    "list_r8 = [11, 13, 20, 13, 13, 14, 14, 121, 129, 15]\n",
    "list_r16 = [148, 31, 12, 75, 24, 120, 121, 15, 16, 121]\n",
    "\n",
    "Grep (rep = 16 BEST)\n",
    "1. [94, 73, 43, 98, 97, 44, 77, 43, 74, 48]\n",
    "2. [94, 73, 43, 98, 97, 44, 77, 43, 74, 48]\n",
    "4. [38, 316, 73, 139, 104, 139, 38, 316, 43, 239]\n",
    "8. [38, 316, 73, 139, 104, 139, 38, 316, 43, 239]\n",
    "16. [337, 319, 338, 318, 317, 140, 323, 326, 339, 57]\n",
    "\n",
    "Gzip (no differences)\n",
    "1. [169, 166, 167, 169, 169, 167, 168, 167, 167, 168]\n",
    "2. [169, 166, 167, 169, 169, 167, 168, 167, 167, 168]\n",
    "4. [167, 168, 169, 167, 166, 168, 166, 167, 166, 167]\n",
    "8. [167, 168, 169, 167, 166, 168, 166, 167, 166, 167]\n",
    "16. [166, 166, 168, 168, 167, 167, 167, 167, 168, 168]\n",
    "\n",
    "Sed (BEST rep=1 and rep=2 => rep=1)\n",
    "1. [234, 234, 15, 234, 234, 15, 15, 234, 234, 29]\n",
    "2. [234, 234, 15, 234, 234, 15, 15, 234, 234, 29]\n",
    "4. [15, 15, 15, 15, 15, 15, 234, 15, 15, 15]\n",
    "8. [15, 15, 15, 15, 15, 15, 234, 15, 15, 15]\n",
    "16. [15, 15, 15, 234, 15, 234, 15, 234, 234, 15]\n",
    "\"\"\""
   ],
   "id": "964e7f9965b110d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def vargha_delaney_a12(list1, list2):\n",
    "    \"\"\"\n",
    "    Computes Vargha-Delaney's A12 effect size for two lists.\n",
    "    The output is the probability that an element from list1 is greater than an element from list2.\n",
    "    \"\"\"\n",
    "    n1, n2 = len(list1), len(list2)\n",
    "    count = sum(1 for x in list1 for y in list2 if x > y)\n",
    "    count += 0.5 * sum(1 for x in list1 for y in list2 if x == y)  # Consider ties equally\n",
    "    return count / (n1*n2)\n",
    "\n",
    "list_r1 = [234, 234, 15, 234, 234, 15, 15, 234, 234, 29]\n",
    "list_r2 = [234, 234, 15, 234, 234, 15, 15, 234, 234, 29]\n",
    "list_r4 = [15, 15, 15, 15, 15, 15, 234, 15, 15, 15]\n",
    "list_r8 = [15, 15, 15, 15, 15, 15, 234, 15, 15, 15]\n",
    "list_r16 = [15, 15, 15, 234, 15, 234, 15, 234, 234, 15]\n",
    "\n",
    "# Execute H-test\n",
    "stat, p_value = kruskal(list_r1, list_r2, list_r4, list_r8, list_r16)\n",
    "\n",
    "# Show results\n",
    "print(\"Kruskal-Wallis Statistics: \", stat)\n",
    "print(\"p-value: \", p_value)\n",
    "\n",
    "# If p-value indicates significant differences, compute A12 effect size for each pair\n",
    "if p_value < 0.05:\n",
    "    print(\"\\nSignificant result! Null hypothesis refused.\"\n",
    "          \"\\nAt least one of the observations came from another population.\"\n",
    "          \"\\nComputing A12 effect size for each pair of lists:\")\n",
    "    lists = {\"List 1\": list_r1, \"List 2\": list_r2, \"List 3\": list_r4, \"List 4\": list_r8, \"List 5:\": list_r16}\n",
    "    \n",
    "    for (name1, l1), (name2, l2) in combinations(lists.items(), 2):\n",
    "        a12 = vargha_delaney_a12(l1, l2)\n",
    "        print(f\"A12({name1} > {name2}): {a12:.3f}\")\n",
    "        print(f\"A12({name2} > {name1}): {1-a12:.3f}\\n\")  # Complementary comparison\n",
    "else:\n",
    "    print(\"Cannot reject the null hypothesis: the lists may come from the same population.\")"
   ],
   "id": "905758478ed86b18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi-Objective SelectQAOA Execution\n",
    "Once we know the optimal configuration of SelectQAOA for each depth value, we can make the executions of all the simulators with their optimal configurations.\n",
    "\n",
    "## Ideal Simulator\n",
    "Since we already computed all the results for each configuration of the ideal simulator, we can avoid rerunning it.\n",
    "\n",
    "## Statevector Simulator\n",
    "The **Statevector Simulator** in Qiskit Aer is a high-precision quantum circuit simulator that represents the full quantum state of a system. It is useful for **simulating ideal quantum circuits** without noise, decoherence, or measurement effects."
   ],
   "id": "8c86dc11a019880b"
  },
  {
   "metadata": {
    "scrolled": true
   },
   "cell_type": "code",
   "source": [
    "algorithm_globals.random_seed = 10598\n",
    "backend = Aer.get_backend(\"statevector_simulator\")\n",
    "\n",
    "#I want to run the sampler 30 times to get different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=backend), optimizer=COBYLA(100), reps=sir_programs_rep_values[sir_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    #the fronts will be saved into files\n",
    "    print(\"SIR Program: \" + sir_program)\n",
    "    file_path = \"results/selectqaoa/statevector_sim/\" + sir_program + \"-data.json\"\n",
    "    json_data = {}\n",
    "    response = None\n",
    "    qpu_run_times = []\n",
    "    pareto_fronts_building_times = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_tests = []\n",
    "        cluster_dict_index = 0\n",
    "        for qubo in qubos_dictionary[sir_program]:\n",
    "            print(\"QUBO Problem: \" + str(qubo) + \"\\n Cluster Number: \" + str(cluster_dict_index))\n",
    "            print(\"Cluster's Test Cases: \" +str(list(clusters_dictionary[sir_program].values())[cluster_dict_index]))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(qubo)\n",
    "            print(\"RESULTS: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            qpu_run_times.append((e - s) * 1000)\n",
    "            #let's extract the selected tests\n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_tests = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_tests))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_tests:\n",
    "                selected_tests.append(list(clusters_dictionary[sir_program].values())[cluster_dict_index][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            print(\"Experiment Number: \" + str(i))\n",
    "            cluster_dict_index += 1\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_selected_tests:\n",
    "                    final_selected_tests.append(selected_test)\n",
    "        i+=1\n",
    "        #now we have to build the pareto front\n",
    "        print(\"Final Selected Test Cases: \" + str(final_selected_tests))\n",
    "        print(\"Length of the final list of selected test cases: \" + str(len(final_selected_tests)))\n",
    "        start = time.time()\n",
    "        pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "        end = time.time()\n",
    "        json_data[\"pareto_front_\" + str(i)] = pareto_front\n",
    "        pareto_front_building_time = (end - start) * 1000\n",
    "        pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "\n",
    "    #compute the average time needed for the construction of a pareto frontier and run time\n",
    "    mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "    mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "    json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "    json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file)\n"
   ],
   "id": "306aa159e2a51733",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fake Vigo Noise Simulator\n",
    "The **Fake Vigo backend** in Qiskit Aer is a simulated version of IBM Quantum's **real 5-qubit Vigo quantum processor**. It allows users to test quantum circuits in a **realistic noisy environment**"
   ],
   "id": "e63c7b626dc9cd90"
  },
  {
   "metadata": {
    "scrolled": true
   },
   "cell_type": "code",
   "source": [
    "device_backend = FakeVigoV2()\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "#I want to run the sampler 30 times to obtain different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=device_backend), optimizer=COBYLA(100), reps=sir_programs_rep_values[sir_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    #the fronts will be saved into files\n",
    "    print(sir_program)\n",
    "    file_path = \"results/selectqaoa/fake_vigo/\" + sir_program + \"-data.json\"\n",
    "    json_data = {}\n",
    "    response = None\n",
    "    qpu_run_times = []\n",
    "    pareto_fronts_building_times = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_tests = []\n",
    "        cluster_dict_index = 0\n",
    "        for qubo in qubos_dictionary[sir_program]:\n",
    "            print(\"Experiment Number: \" + str(i))\n",
    "            print(\"QUBO Problem: \" + str(qubo) + \"\\nNumber: \" + str(cluster_dict_index))\n",
    "            print(\"Cluster's Test Cases: \" +str(list(clusters_dictionary[sir_program].values())[cluster_dict_index]))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(qubo)\n",
    "            print(\"RESULTS: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            qpu_run_times.append((e - s) * 1000)\n",
    "            #let's extract the selected tests\n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_tests = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_tests))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_tests:\n",
    "                selected_tests.append(list(clusters_dictionary[sir_program].values())[cluster_dict_index][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            cluster_dict_index += 1\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_selected_tests:\n",
    "                    final_selected_tests.append(selected_test)\n",
    "        i+=1\n",
    "        #now we have to build the pareto front\n",
    "        print(\"Final Selected Test Cases: \" + str(final_selected_tests))\n",
    "        print(len(final_selected_tests))\n",
    "        start = time.time()\n",
    "        pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "        end = time.time()\n",
    "        json_data[\"pareto_front_\" + str(i)] = pareto_front\n",
    "        pareto_front_building_time = (end - start) * 1000\n",
    "        pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "\n",
    "    #compute the average time needed for the construction of a pareto frontier and run time\n",
    "    mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "    mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "    json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "    json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file)\n"
   ],
   "id": "8cb54e2582c97bf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Depolarizing Error Simulator\n",
    "The **depolarizing noise model** is a common quantum noise model that simulates the effect of random errors occurring in quantum computations. In a **depolarizing channel**, a qubit undergoes a transformation where it has a probability \\( p \\) of being replaced by a completely mixed state, losing all its quantum coherence.\n",
    "\n",
    "- **1% noise** simulates near-term quantum hardware with optimized error rates.\n",
    "- **2% noise** represents moderate errors in typical superconducting quantum processors.\n",
    "- **5% noise** helps study worst-case scenarios and assess robustness of quantum algorithms.\n",
    "- Comparing different noise levels allows us to evaluate how quantum circuits **degrade** under varying degrees of decoherence and to **develop error mitigation techniques**.\n"
   ],
   "id": "cee2816526058aef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "noise_dep = NoiseModel()\n",
    "\n",
    "# Define a 1% depolarizing error for single-qubit and two-qubit gates\n",
    "depolarizing_error_1q = depolarizing_error(0.01, 1)  # 1% depolarizing error for single-qubit gates\n",
    "depolarizing_error_2q = depolarizing_error(0.01, 2)  # 1% depolarizing error for two-qubit gates\n",
    "\n",
    "# Apply depolarizing noise to single-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_1q, ['rx', 'rz', 'sx', 'x', 'h'])\n",
    "\n",
    "# Apply depolarizing noise to two-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_2q, ['cx', 'cz', 'swap'])\n",
    "\n",
    "sim_dep = AerSimulator(noise_model=noise_dep)\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "#I want to run the sampler 30 times to obtain different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_dep), optimizer=COBYLA(100), reps=sir_programs_rep_values[sir_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    #the fronts will be saved into files\n",
    "    print(sir_program)\n",
    "    file_path = \"results/selectqaoa/depolarizing_sim/01/\" + sir_program + \"-data.json\"\n",
    "    json_data = {}\n",
    "    response = None\n",
    "    qpu_run_times = []\n",
    "    pareto_fronts_building_times = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_tests = []\n",
    "        cluster_dict_index = 0\n",
    "        for qubo in qubos_dictionary[sir_program]:\n",
    "            print(\"Experiment Number: \" + str(i))\n",
    "            print(\"QUBO Problem: \" + str(qubo) + \"\\nNumber: \" + str(cluster_dict_index))\n",
    "            print(\"Cluster's Test Cases: \" +str(list(clusters_dictionary[sir_program].values())[cluster_dict_index]))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(qubo)\n",
    "            print(\"RESULTS: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            qpu_run_times.append((e - s) * 1000)\n",
    "            #let's extract the selected tests\n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_tests = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_tests))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_tests:\n",
    "                selected_tests.append(list(clusters_dictionary[sir_program].values())[cluster_dict_index][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            cluster_dict_index += 1\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_selected_tests:\n",
    "                    final_selected_tests.append(selected_test)\n",
    "        i+=1\n",
    "        #now we have to build the pareto front\n",
    "        print(\"Final Selected Test Cases: \" + str(final_selected_tests))\n",
    "        print(len(final_selected_tests))\n",
    "        start = time.time()\n",
    "        pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "        end = time.time()\n",
    "        json_data[\"pareto_front_\" + str(i)] = pareto_front\n",
    "        pareto_front_building_time = (end - start) * 1000\n",
    "        pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "\n",
    "    #compute the average time needed for the construction of a pareto frontier and run time\n",
    "    mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "    mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "    json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "    json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file)\n"
   ],
   "id": "188a16dd83242111",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "noise_dep = NoiseModel()\n",
    "\n",
    "# Define a 2% depolarizing error for single-qubit and two-qubit gates\n",
    "depolarizing_error_1q = depolarizing_error(0.02, 1)  # 2% depolarizing error for single-qubit gates\n",
    "depolarizing_error_2q = depolarizing_error(0.02, 2)  # 2% depolarizing error for two-qubit gates\n",
    "\n",
    "# Apply depolarizing noise to single-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_1q, ['rx', 'rz', 'sx', 'x', 'h'])\n",
    "\n",
    "# Apply depolarizing noise to two-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_2q, ['cx', 'cz', 'swap'])\n",
    "\n",
    "sim_dep = AerSimulator(noise_model=noise_dep)\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "#I want to run the sampler 30 times to obtain different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_dep), optimizer=COBYLA(100), reps=sir_programs_rep_values[sir_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    #the fronts will be saved into files\n",
    "    print(sir_program)\n",
    "    file_path = \"results/selectqaoa/depolarizing_sim/02/\" + sir_program + \"-data.json\"\n",
    "    json_data = {}\n",
    "    response = None\n",
    "    qpu_run_times = []\n",
    "    pareto_fronts_building_times = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_tests = []\n",
    "        cluster_dict_index = 0\n",
    "        for qubo in qubos_dictionary[sir_program]:\n",
    "            print(\"Experiment Number: \" + str(i))\n",
    "            print(\"QUBO Problem: \" + str(qubo) + \"\\nNumber: \" + str(cluster_dict_index))\n",
    "            print(\"Cluster's Test Cases: \" +str(list(clusters_dictionary[sir_program].values())[cluster_dict_index]))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(qubo)\n",
    "            print(\"RESULTS: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            qpu_run_times.append((e - s) * 1000)\n",
    "            #let's extract the selected tests\n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_tests = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_tests))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_tests:\n",
    "                selected_tests.append(list(clusters_dictionary[sir_program].values())[cluster_dict_index][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            cluster_dict_index += 1\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_selected_tests:\n",
    "                    final_selected_tests.append(selected_test)\n",
    "        i+=1\n",
    "        #now we have to build the pareto front\n",
    "        print(\"Final Selected Test Cases: \" + str(final_selected_tests))\n",
    "        print(len(final_selected_tests))\n",
    "        start = time.time()\n",
    "        pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "        end = time.time()\n",
    "        json_data[\"pareto_front_\" + str(i)] = pareto_front\n",
    "        pareto_front_building_time = (end - start) * 1000\n",
    "        pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "\n",
    "    #compute the average time needed for the construction of a pareto frontier and run time\n",
    "    mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "    mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "    json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "    json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file)\n"
   ],
   "id": "5b50efcb97700f8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "noise_dep = NoiseModel()\n",
    "\n",
    "# Define a 5% depolarizing error for single-qubit and two-qubit gates\n",
    "depolarizing_error_1q = depolarizing_error(0.05, 1)  # 5% depolarizing error for single-qubit gates\n",
    "depolarizing_error_2q = depolarizing_error(0.05, 2)  # 5% depolarizing error for two-qubit gates\n",
    "\n",
    "# Apply depolarizing noise to single-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_1q, ['rx', 'rz', 'sx', 'x', 'h'])\n",
    "\n",
    "# Apply depolarizing noise to two-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_2q, ['cx', 'cz', 'swap'])\n",
    "\n",
    "sim_dep = AerSimulator(noise_model=noise_dep)\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "#I want to run the sampler 30 times to obtain different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_dep), optimizer=COBYLA(100), reps=sir_programs_rep_values[sir_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    #the fronts will be saved into files\n",
    "    print(sir_program)\n",
    "    file_path = \"results/selectqaoa/depolarizing_sim/05/\" + sir_program + \"-data.json\"\n",
    "    json_data = {}\n",
    "    response = None\n",
    "    qpu_run_times = []\n",
    "    pareto_fronts_building_times = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_tests = []\n",
    "        cluster_dict_index = 0\n",
    "        for qubo in qubos_dictionary[sir_program]:\n",
    "            print(\"Experiment Number: \" + str(i))\n",
    "            print(\"QUBO Problem: \" + str(qubo) + \"\\nNumber: \" + str(cluster_dict_index))\n",
    "            print(\"Cluster's Test Cases: \" +str(list(clusters_dictionary[sir_program].values())[cluster_dict_index]))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(qubo)\n",
    "            print(\"RESULTS: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            qpu_run_times.append((e - s) * 1000)\n",
    "            #let's extract the selected tests\n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_tests = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_tests))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_tests:\n",
    "                selected_tests.append(list(clusters_dictionary[sir_program].values())[cluster_dict_index][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            cluster_dict_index += 1\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_selected_tests:\n",
    "                    final_selected_tests.append(selected_test)\n",
    "        i+=1\n",
    "        #now we have to build the pareto front\n",
    "        print(\"Final Selected Test Cases: \" + str(final_selected_tests))\n",
    "        print(len(final_selected_tests))\n",
    "        start = time.time()\n",
    "        pareto_front = build_pareto_front(sir_program, final_selected_tests)\n",
    "        end = time.time()\n",
    "        json_data[\"pareto_front_\" + str(i)] = pareto_front\n",
    "        pareto_front_building_time = (end - start) * 1000\n",
    "        pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "\n",
    "    #compute the average time needed for the construction of a pareto frontier and run time\n",
    "    mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "    mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "    json_data[\"stdev_qpu_run_time(ms)\"] = statistics.stdev(qpu_run_times)\n",
    "    json_data[\"all_qpu_run_times(ms)\"] = qpu_run_times\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        json.dump(json_data, file)\n"
   ],
   "id": "41e0dc50b3f44546",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Effectiveness Empirical Evaluations\n",
    "The four algorithms were evaluated in terms of their **effectiveness**. In particular, we built a **reference Pareto front** made by all the **non-dominated solutions** found by all the algorithms during all ten runs. \n",
    "\n",
    "Let $P = \\{ P_1, ..., P_l \\}$ be the set of \\( $l$ \\) different Pareto fronts obtained after all the experiment runs by all the evaluated algorithms. The **reference Pareto front** $P_{\\text{ref}}$ is defined as follows:\n",
    "\n",
    "\n",
    "$P_{\\text{ref}} \\subseteq \\bigcup_{i=1}^{l} P_i$\n",
    "\n",
    "\n",
    "where $\\forall p \\in P_{\\text{ref}}, \\nexists q \\in P_{\\text{ref}}: q > p$. \n",
    "\n",
    "Given the reference front, we computed as an **effectiveness metric** the **number of non-dominated solutions**, i.e., the number of non-dominated solutions found by an algorithm selected for the final reference front.\n",
    "\n",
    "To ensure the empirical reliability of the results, we used the **Mann-Whitney U test** and the **Vargha-Delaney effect size** $\\hat{A}_{12}$. The null hypothesis, i.e., there is **no statistically significant difference** between the effectiveness of the algorithms, is rejected if p-values < 0.05.\n",
    "\n",
    "\n",
    "### Mann-Whitney's U test\n",
    "The **Mann-Whitney U test** (also called the **Wilcoxon rank-sum test**) is a **non-parametric** statistical test used to compare two independent groups. It assesses whether one group tends to have higher values than the other without assuming a normal distribution."
   ],
   "id": "927ce932-68b7-4a3b-a8ed-edc0e7ded093"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#the other sizes are uniform\n",
    "#this static information can be easily obtained from the pareto .json files\n",
    "\n",
    "qtcs_flex_pareto_sizes = [187,187,187,187,187,187,187,187,187,187]\n",
    "divga_flex_pareto_sizes = [140,140,140,140,140,140,140,140,140,140]\n",
    "qaoa_statevector_flex_pareto_sizes = [447,434,413,409,434,455,446,410,416,427]\n",
    "qaoa_ideal_flex_pareto_sizes = [517,515,518,523,516,518,505,514,518,526]\n",
    "qaoa_fakevigo_flex_pareto_sizes = [540,540,540,540,540,540,540,539,539,539]\n",
    "qaoa_noise1_flex_pareto_sizes = [539,540,540,539,540,540,539,539,539,539]\n",
    "qaoa_noise2_flex_pareto_sizes = [540,539,540,539,540,540,539,540,539,540]\n",
    "qaoa_noise5_flex_pareto_sizes = [539,539,540,540,540,539,540,540,540,539]\n",
    "greedy_flex_size = 567\n",
    "\n",
    "qtcs_grep_pareto_sizes = [229,229,230,229,230,229,230.229,230,230]\n",
    "divga_grep_pareto_sizes = [70,70,70,70,70,70,70,70,70,70]\n",
    "qaoa_statevector_grep_pareto_sizes = [485,507,504,505,489,492,489,493,489,470]\n",
    "qaoa_ideal_grep_pareto_sizes = [343,345,343,341,346,345,345,346,340,348]\n",
    "qaoa_fakevigo_grep_pareto_sizes = [321,321,321,321,321,321,321,321,321,321]\n",
    "qaoa_noise1_grep_pareto_sizes = [321,321,321,321,321,321,321,321,321,321]\n",
    "qaoa_noise2_grep_pareto_sizes = [321,321,321,321,321,321,321,321,321,321]\n",
    "qaoa_noise5_grep_pareto_sizes = [321,321,321,321,321,321,321,321,321,321]\n",
    "greedy_grep_size = 802\n",
    "\n",
    "qtcs_gzip_pareto_sizes = [87,86,85,86,87,87,86,87,85,87]\n",
    "divga_gzip_pareto_sizes = [105,105,105,105,105,105,105,105,105,105]\n",
    "qaoa_statevector_gzip_pareto_sizes = [137,143,146,144,164,143,157,144,141,169]\n",
    "qaoa_ideal_gzip_pareto_sizes = [147,146,145,146,151,155,155,151,154,156]\n",
    "qaoa_fakevigo_gzip_pareto_sizes = [168,168,166,169,168,167,167,168,168,167]\n",
    "qaoa_noise1_gzip_pareto_sizes = [167,167,167,166,168,167,166,166,168,166]\n",
    "qaoa_noise2_gzip_pareto_sizes = [166,166,168,167,166,166,166,167,167,166]\n",
    "qaoa_noise5_gzip_pareto_sizes = [169,167,167,169,168,168,168,165,167,167]\n",
    "greedy_gzip_size = 199\n",
    "\n",
    "qtcs_sed_pareto_sizes = [131,131,131,131,131,131,131,131,131,131]\n",
    "divga_sed_pareto_sizes = [105,62,105,105,102,105,105,97,105,105]\n",
    "qaoa_statevector_sed_pareto_sizes = [249,241,252,226,245,231,248,241,245,237]\n",
    "qaoa_ideal_sed_pareto_sizes = [260,251,253,263,252,263,260,256,250,253]\n",
    "qaoa_fakevigo_sed_pareto_sizes = [235,235,236,235,236,236,235,236,236,236]\n",
    "qaoa_noise1_sed_pareto_sizes = [236,235,235,235,236,236,236,235,236,235]\n",
    "qaoa_noise2_sed_pareto_sizes = [235,235,236,235,235,236,236,236,236,236]\n",
    "qaoa_noise5_sed_pareto_sizes = [236,236,235,236,236,235,235,236,236,235]\n",
    "greedy_sed_size = 356"
   ],
   "id": "fd4f851336f7e0fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37611d4d066cd05b",
   "metadata": {},
   "source": [
    "with open('./results/add-greedy/flex_data.json', 'r') as file:\n",
    "    pareto_fronts_json = json.load(file)\n",
    "    \n",
    "greedy_pareto_front = pareto_fronts_json['pareto_front']\n",
    "\n",
    "#each solution of the pareto front is a subset of the test suite\n",
    "greedy_pareto_vectors = []\n",
    "for front_solution in greedy_pareto_front:\n",
    "    #compute the fitness scores of each single solution\n",
    "    greedy_pareto_vectors.append((total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62693154-2df4-44c5-98d4-54b1b2b2a34d",
   "metadata": {},
   "source": [
    "#here we make the same but for each of the fronts built by SelectQA and DIV-GA\n",
    "\n",
    "#the following 2 lists will contain much tuples as solutions in each of the 10 pareto fronts\n",
    "#each tuple represent the fitness value for each solution\n",
    "qtcs_pareto_vectors = []\n",
    "divga_pareto_vectors = []\n",
    "qaoa_statevector_pareto_vectors = []\n",
    "qaoa_ideal_pareto_vectors = []\n",
    "qaoa_fakevigo_pareto_vectors = []\n",
    "qaoa_noise1_pareto_vectors = []\n",
    "qaoa_noise2_pareto_vectors = []\n",
    "qaoa_noise5_pareto_vectors = []\n",
    "\n",
    "for index in range(0,10):\n",
    "    # Load the JSON file\n",
    "    with open('results/selectqa/flex-data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qtcs_pareto_front = pareto_fronts_json['pareto_front_'+str(index)]\n",
    "    #a single solution is a subset of the initial test suite\n",
    "    for front_solution in qtcs_pareto_front:\n",
    "        qtcs_pareto_vectors.append((total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)))\n",
    "    #print(len(qtcs_pareto_vectors))\n",
    "    \n",
    "    with open('./results/divga/flex_pareto_fronts_divga.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    divga_pareto_front = pareto_fronts_json['flex_pareto_front_'+str(index)]\n",
    "    for front_solution in divga_pareto_front:\n",
    "        divga_pareto_vectors.append((total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)))\n",
    "    #print(len(divga_pareto_vectors))\n",
    "    \n",
    "    with open('results/selectqaoa/statevector_sim/flex-data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    #a single solution is a subset of the initial test suite\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        qaoa_statevector_pareto_vectors.append((total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)))\n",
    "    #print(len(qaoa_statevector_pareto_vectors))\n",
    "    \n",
    "    with open('results/selectqaoa/ideal/flex-data-rep-16.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    #a single solution is a subset of the initial test suite\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        qaoa_ideal_pareto_vectors.append((total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)))\n",
    "    #print(len(qaoa_ideal_pareto_vectors))\n",
    "    \n",
    "    with open('results/selectqaoa/fake_vigo/flex-data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    #a single solution is a subset of the initial test suite\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        qaoa_fakevigo_pareto_vectors.append((total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)))\n",
    "    #print(len(qaoa_fakevigo_pareto_vectors))\n",
    "    \n",
    "    with open('results/selectqaoa/depolarizing_sim/01/flex-data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    #a single solution is a subset of the initial test suite\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        qaoa_noise1_pareto_vectors.append((total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)))\n",
    "    #print(len(qaoa_noise1_pareto_vectors))\n",
    "    \n",
    "    with open('results/selectqaoa/depolarizing_sim/02/flex-data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    #a single solution is a subset of the initial test suite\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        qaoa_noise2_pareto_vectors.append((total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)))\n",
    "    #print(len(qaoa_noise2_pareto_vectors))\n",
    "    \n",
    "    with open('results/selectqaoa/depolarizing_sim/05/flex-data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    #a single solution is a subset of the initial test suite\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        qaoa_noise5_pareto_vectors.append((total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)))\n",
    "    #print(len(qaoa_noise5_pareto_vectors))\n",
    "\n",
    "#once we have the pareto vectors from each pareto front obtained by each run, we extract \n",
    "#just the solution not dominated by anyone else\n",
    "#qtcs_pareto_vectors,divga_pareto_vectors,greedy_pareto_vectors,qaoa_ideal_pareto_vectors,qaoa_fakevigo_pareto_vectors,qaoa_noise1_pareto_vectors,qaoa_noise2_pareto_vectors,qaoa_noise5_pareto_vectors\n",
    "total_fronts = [qtcs_pareto_vectors,divga_pareto_vectors,greedy_pareto_vectors,qaoa_ideal_pareto_vectors]\n",
    "reference_pareto = []\n",
    "portions = [0,0,0,0]\n",
    "\n",
    "# get the reference frontier\n",
    "for index, front1 in enumerate(total_fronts):\n",
    "    for front_solution1 in front1:\n",
    "        is_dominated = 0\n",
    "        other_fronts = total_fronts[:index] + total_fronts[index+1:]\n",
    "        for front2 in other_fronts:\n",
    "            for front_solution2 in front2:\n",
    "                if pareto_dominance(front_solution1,front_solution2):\n",
    "                    is_dominated = 1\n",
    "                    break\n",
    "            if is_dominated:\n",
    "                break\n",
    "        if not is_dominated:\n",
    "            reference_pareto.append(front_solution1)\n",
    "            portions[index] = portions[index] + 1\n",
    "\n",
    "print(portions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77d824ace461716a",
   "metadata": {},
   "source": [
    "# For SelectQA and DIV-GA we want to compute, for each of the 10 iterations, how many of the solutions of the i-th pareto front were selected by the reference front\n",
    "\n",
    "qtcs_non_dominated_values = []\n",
    "divga_non_dominated_values = []\n",
    "qaoa_statevector_non_dominated_values = []\n",
    "qaoa_ideal_non_dominated_values = []\n",
    "qaoa_fakevigo_non_dominated_values = []\n",
    "qaoa_noise1_non_dominated_values = []\n",
    "qaoa_noise2_non_dominated_values = []\n",
    "qaoa_noise5_non_dominated_values = []\n",
    "greedy_non_dominated = 0\n",
    "\n",
    "for index in range(0,10):\n",
    "    #print(\"Iteratrion: \" + str(index))\n",
    "    \n",
    "    qtcs_non_dominated = 0\n",
    "    divga_non_dominated = 0\n",
    "    qaoa_statevector_non_dominated = 0\n",
    "    qaoa_ideal_non_dominated = 0\n",
    "    qaoa_fakevigo_non_dominated = 0\n",
    "    qaoa_noise1_non_dominated = 0\n",
    "    qaoa_noise2_non_dominated = 0\n",
    "    qaoa_noise5_non_dominated = 0\n",
    "    \n",
    "    # Load the JSON file\n",
    "    with open('results/selectqa/flex-data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qtcs_pareto_front = pareto_fronts_json['pareto_front_'+str(index)]\n",
    "    for front_solution in qtcs_pareto_front:\n",
    "        if (total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)) in reference_pareto:\n",
    "            qtcs_non_dominated += 1\n",
    "    \n",
    "    qtcs_non_dominated_values.append(qtcs_non_dominated)\n",
    "    #print(qtcs_non_dominated)\n",
    "    \n",
    "    with open('./results/divga/flex_pareto_fronts_divga.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    divga_pareto_front = pareto_fronts_json['flex_pareto_front_'+str(index)]\n",
    "    for front_solution in divga_pareto_front:\n",
    "        if (total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)) in reference_pareto:\n",
    "            divga_non_dominated += 1\n",
    "            \n",
    "    divga_non_dominated_values.append(divga_non_dominated)\n",
    "    #print(divga_non_dominated)\n",
    "    \n",
    "    with open('results/selectqaoa/statevector_sim/flex-data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        if (total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)) in reference_pareto:\n",
    "            qaoa_statevector_non_dominated += 1\n",
    "    \n",
    "    qaoa_statevector_non_dominated_values.append(qaoa_statevector_non_dominated)\n",
    "    #print(qaoa_statevector_non_dominated)\n",
    "    \n",
    "    with open('results/selectqaoa/ideal/flex-data-rep-16.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        if (total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)) in reference_pareto:\n",
    "            qaoa_ideal_non_dominated += 1\n",
    "    \n",
    "    qaoa_ideal_non_dominated_values.append(qaoa_ideal_non_dominated)\n",
    "    #print(qaoa_statevector_non_dominated)\n",
    "    \n",
    "    with open('results/selectqaoa/fake_vigo/flex-data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        if (total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)) in reference_pareto:\n",
    "            qaoa_fakevigo_non_dominated += 1\n",
    "    \n",
    "    qaoa_fakevigo_non_dominated_values.append(qaoa_fakevigo_non_dominated)\n",
    "    #print(qaoa_statevector_non_dominated)\n",
    "    \n",
    "    with open('results/selectqaoa/depolarizing_sim/01/flex-data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        if (total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)) in reference_pareto:\n",
    "            qaoa_noise1_non_dominated += 1\n",
    "    \n",
    "    qaoa_noise1_non_dominated_values.append(qaoa_noise1_non_dominated)\n",
    "    #print(qaoa_statevector_non_dominated)\n",
    "    \n",
    "    with open('results/selectqaoa/depolarizing_sim/02/flex-data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        if (total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)) in reference_pareto:\n",
    "            qaoa_noise2_non_dominated += 1\n",
    "    \n",
    "    qaoa_noise2_non_dominated_values.append(qaoa_noise2_non_dominated)\n",
    "    #print(qaoa_statevector_non_dominated)\n",
    "    \n",
    "    with open('results/selectqaoa/depolarizing_sim/05/flex-data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qaoa_pareto_front = pareto_fronts_json['pareto_front_'+str(index+1)]\n",
    "    for front_solution in qaoa_pareto_front:\n",
    "        if (total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)) in reference_pareto:\n",
    "            qaoa_noise5_non_dominated += 1\n",
    "    \n",
    "    qaoa_noise5_non_dominated_values.append(qaoa_noise5_non_dominated)\n",
    "    #print(qaoa_statevector_non_dominated)\n",
    "    \n",
    "for front_solution in greedy_pareto_front:\n",
    "    if (total_cost(\"flex\",front_solution),total_coverage(\"flex\",front_solution),total_faults(\"flex\",front_solution)) in reference_pareto:\n",
    "        greedy_non_dominated += 1\n",
    "#print(greedy_non_dominated)\n",
    "\n",
    "print(\"QTCS/DIVGA/ADD GREEDY\")\n",
    "print(\"SelectQA Non Dominated Values\")\n",
    "print(qtcs_non_dominated_values)\n",
    "print(\"SelectQA Non Dominated Mean\")\n",
    "print(statistics.mean(qtcs_non_dominated_values))\n",
    "print(\"SelectQA Non Dominated StDev\")\n",
    "print(statistics.stdev(qtcs_non_dominated_values))\n",
    "\n",
    "print(\"DIV-GA Non Dominated Values\")\n",
    "print(divga_non_dominated_values)\n",
    "print(\"DIV-GA Non Dominated Mean\")\n",
    "print(statistics.mean(divga_non_dominated_values))\n",
    "print(\"DIV-GA Non Dominated StDev\")\n",
    "print(statistics.stdev(divga_non_dominated_values))\n",
    "\n",
    "print(\"SelectQAOA Non Dominated Values (Statevector)\")\n",
    "print(qtcs_non_dominated_values)\n",
    "print(\"SelectQAOA Non Dominated Mean (Statevector)\")\n",
    "print(statistics.mean(qtcs_non_dominated_values))\n",
    "print(\"SelectQAOA Non Dominated StDev (Statevector)\")\n",
    "print(statistics.stdev(qtcs_non_dominated_values))\n",
    "\n",
    "print(\"SelectQAOA Non Dominated Values (Ideal)\")\n",
    "print(qaoa_ideal_non_dominated_values)\n",
    "print(\"SelectQAOA Non Dominated Mean (Ideal)\")\n",
    "print(statistics.mean(qaoa_ideal_non_dominated_values))\n",
    "print(\"SelectQAOA Non Dominated StDev (Ideal)\")\n",
    "print(statistics.stdev(qaoa_ideal_non_dominated_values))\n",
    "\n",
    "print(\"SelectQAOA Non Dominated Values (Fake Vigo)\")\n",
    "print(qaoa_fakevigo_non_dominated_values)\n",
    "print(\"SelectQAOA Non Dominated Mean (Fake Vigo)\")\n",
    "print(statistics.mean(qaoa_fakevigo_non_dominated_values))\n",
    "print(\"SelectQAOA Non Dominated StDev (Fake Vigo)\")\n",
    "print(statistics.stdev(qaoa_fakevigo_non_dominated_values))\n",
    "\n",
    "print(\"SelectQAOA Non Dominated Values (Depolarizing Error 1%)\")\n",
    "print(qaoa_noise1_non_dominated_values)\n",
    "print(\"SelectQAOA Non Dominated Mean (Depolarizing Error 1%)\")\n",
    "print(statistics.mean(qaoa_noise1_non_dominated_values))\n",
    "print(\"SelectQAOA Non Dominated StDev (Depolarizing Error 1%)\")\n",
    "print(statistics.stdev(qaoa_noise1_non_dominated_values))\n",
    "\n",
    "print(\"SelectQAOA Non Dominated Values (Depolarizing Error 2%)\")\n",
    "print(qaoa_noise2_non_dominated_values)\n",
    "print(\"SelectQAOA Non Dominated Mean (Depolarizing Error 2%)\")\n",
    "print(statistics.mean(qaoa_noise2_non_dominated_values))\n",
    "print(\"SelectQAOA Non Dominated StDev (Depolarizing Error 2%)\")\n",
    "print(statistics.stdev(qaoa_noise2_non_dominated_values))\n",
    "\n",
    "print(\"SelectQAOA Non Dominated Values (Depolarizing Error 5%)\")\n",
    "print(qaoa_noise5_non_dominated_values)\n",
    "print(\"SelectQAOA Non Dominated Mean (Depolarizing Error 5%)\")\n",
    "print(statistics.mean(qaoa_noise5_non_dominated_values))\n",
    "print(\"SelectQAOA Non Dominated StDev (Depolarizing Error 5%)\")\n",
    "print(statistics.stdev(qaoa_noise5_non_dominated_values))\n",
    "\n",
    "print(\"Greedy Non Dominated Values\")\n",
    "print(greedy_non_dominated)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "01879508-5e39-4f21-9b83-d67f52da655b",
   "metadata": {},
   "source": [
    "# Unpack the tuples for each list\n",
    "def unpack_tuples(data):\n",
    "    \"\"\"Unpack the data\"\"\"\n",
    "    return zip(*data)\n",
    "\n",
    "#qtcs_pareto_vectors,divga_pareto_vectors,greedy_pareto_vectors\n",
    "\n",
    "#x1, y1, z1 = unpack_tuples(qtcs_pareto_vectors)\n",
    "#x2, y2, z2 = unpack_tuples(divga_pareto_vectors)\n",
    "#x3, y3, z3 = unpack_tuples(greedy_pareto_vectors)\n",
    "x4, y4, z4 = unpack_tuples(qaoa_noise5_pareto_vectors)\n",
    "x5, y5, z5 = unpack_tuples(reference_pareto)\n",
    "\n",
    "# Create 3D scatter plots\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "\n",
    "\"\"\"# First plot: list1 vs list5\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(x1, y1, z1, c='r', marker='s', s=100, label='QA')\n",
    "ax1.scatter(x5, y5, z5, c='b', marker='o', s=50, label='Reference Pareto')\n",
    "ax1.set_xlabel('Cost')\n",
    "ax1.set_ylabel('Statement Coverage')\n",
    "ax1.set_zlabel('Past Faults',labelpad=0.2)\n",
    "ax1.set_title('QA vs Reference')\n",
    "ax1.legend()\n",
    "\n",
    "# Second plot: list2 vs list5\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax2.scatter(x2, y2, z2, c='g', marker='s', s=100, label='DIV-GA')\n",
    "ax2.scatter(x5, y5, z5, c='b', marker='o', s=50, label='Reference Pareto')\n",
    "ax2.set_xlabel('Cost')\n",
    "ax2.set_ylabel('Statement Coverage')\n",
    "ax2.set_zlabel('Past Faults',labelpad=0.2)\n",
    "ax2.set_title('DIV-GA vs Reference Pareto')\n",
    "ax2.legend()\n",
    "\n",
    "# Third plot: list3 vs list5\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "ax3.scatter(x3, y3, z3, c='m', marker='s', s=100, label='Add. Greedy')\n",
    "ax3.scatter(x5, y5, z5, c='b', marker='o', s=50, label='Reference Pareto')\n",
    "ax3.set_xlabel('Cost')\n",
    "ax3.set_ylabel('Statement Coverage')\n",
    "ax3.set_zlabel('Past Faults',labelpad=0.2)\n",
    "ax3.set_title('Add. Greedy vs Reference Pareto')\n",
    "ax3.legend()\"\"\"\n",
    "\n",
    "# Third plot: list4 vs list5\n",
    "ax4 = fig.add_subplot(133, projection='3d')\n",
    "ax4.scatter(x4, y4, z4, c='m', marker='s', s=100, label='QAOA')\n",
    "ax4.scatter(x5, y5, z5, c='b', marker='o', s=50, label='Reference Pareto')\n",
    "ax4.set_xlabel('Cost')\n",
    "ax4.set_ylabel('Statement Coverage')\n",
    "ax4.set_zlabel('Past Faults',labelpad=0.2)\n",
    "ax4.set_title('Ideal QAOA vs Reference Pareto')\n",
    "ax4.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c077798f217ea21b",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "NON DOM SEQUENCES \n",
    "IDEAL\n",
    "qtcs_nondom_flex = [187, 187, 187, 187, 187, 187, 187, 187, 187, 187]\n",
    "div_ga_nondom_flex = [140, 140, 140, 140, 140, 140, 140, 140, 140, 139]\n",
    "qaoa_nondom_flex = [471, 479, 479, 487, 478, 478, 468, 478, 473, 482]\n",
    "add_greedy_nondom_flex = [205,205,205,205,205,205,205,205,205,205]\n",
    "\n",
    "qtcs_nondom_grep = [10, 10, 11, 10, 11, 10, 11, 10, 11, 11]\n",
    "div_ga_nondom_grep = [70, 70, 70, 70, 70, 70, 70, 70, 70, 70]\n",
    "qaoa_nondom_grep = [376, 316, 370, 348, 327, 338, 350, 316, 358, 405]\n",
    "add_greedy_nondom_grep = [171,171,171,171,171,171,171,171,171,171]\n",
    "\n",
    "qtcs_nondom_gzip = [38, 37, 36, 37, 38, 38, 37, 38, 36, 38]\n",
    "div_ga_nondom_gzip = [105, 105, 105, 105, 105, 105, 105, 105, 105, 105]\n",
    "qaoa_nondom_gzip = [343, 345, 343, 341, 346, 345, 345, 346, 340, 348]\n",
    "add_greedy_nondom_gzip = [33,33,33,33,33,33,33,33,33,33]\n",
    "\n",
    "qtcs_nondom_sed = [110, 110, 110, 110, 110, 110, 110, 110, 110, 110]\n",
    "div_ga_nondom_sed = [95, 62, 99, 105, 90, 100, 98, 93, 103, 104]\n",
    "qaoa_nondom_sed = [192, 196, 198, 188, 194, 188, 196, 189, 195, 199]\n",
    "add_greedy_nondom_sed = [80,80,80,80,80,80,80,80,80,80]\n",
    "\n",
    "FAKE VIGO\n",
    "qtcs_nondom_flex = [187, 187, 187, 187, 187, 187, 187, 187, 187, 187]\n",
    "div_ga_nondom_flex = [140, 140, 140, 140, 140, 140, 140, 140, 140, 140]\n",
    "qaoa_nondom_flex = [492, 492, 492, 492, 492, 492, 492, 491, 491, 491]\n",
    "add_greedy_nondom_flex = [205,205,205,205,205,205,205,205,205,205]\n",
    "\n",
    "qtcs_nondom_grep = [192, 192, 193, 192, 193, 192, 193, 192, 193, 193]\n",
    "div_ga_nondom_grep = [70, 70, 70, 70, 70, 70, 70, 70, 70, 70]\n",
    "qaoa_nondom_grep = [321, 321, 321, 321, 321, 321, 321, 321, 321, 321]\n",
    "add_greedy_nondom_grep = [177,177,177,177,177,177,177,177,177,177]\n",
    "\n",
    "qtcs_nondom_gzip = [41, 40, 39, 40, 41, 41, 40, 41, 39, 41]\n",
    "div_ga_nondom_gzip = [105, 105, 105, 105, 105, 105, 105, 105, 105, 105]\n",
    "qaoa_nondom_gzip = [95, 95, 103, 96, 95, 104, 104, 95, 95, 104]\n",
    "add_greedy_nondom_gzip = [26,26,26,26,26,26,26,26,26,26]\n",
    "\n",
    "qtcs_nondom_sed = [86, 86, 86, 86, 86, 86, 86, 86, 86, 86]\n",
    "div_ga_nondom_sed = [89, 62, 103, 101, 89, 97, 97, 94, 101, 103]\n",
    "qaoa_nondom_sed = [183, 183, 183, 183, 183, 183, 183, 183, 183, 183]\n",
    "add_greedy_nondom_sed = [80,80,80,80,80,80,80,80,80,80]\n",
    "\n",
    "NOISE 1%\n",
    "qtcs_nondom_flex = [187, 187, 187, 187, 187, 187, 187, 187, 187, 187]\n",
    "div_ga_nondom_flex = [140, 140, 140, 140, 140, 140, 140, 140, 140, 140]\n",
    "qaoa_nondom_flex = [491, 492, 492, 491, 492, 492, 491, 491, 491, 491]\n",
    "add_greedy_nondom_flex = [205,205,205,205,205,205,205,205,205,205]\n",
    "\n",
    "qtcs_nondom_grep = [192, 192, 193, 192, 193, 192, 193, 192, 193, 193]\n",
    "div_ga_nondom_grep = [70, 70, 70, 70, 70, 70, 70, 70, 70, 70]\n",
    "qaoa_nondom_grep = [321, 321, 321, 321, 321, 321, 321, 321, 321, 321]\n",
    "add_greedy_nondom_grep = [177,177,177,177,177,177,177,177,177,177]\n",
    "\n",
    "qtcs_nondom_gzip = [41, 40, 39, 40, 41, 41, 40, 41, 39, 41]\n",
    "div_ga_nondom_gzip = [105, 105, 105, 105, 105, 105, 105, 105, 105, 105]\n",
    "qaoa_nondom_gzip = [100, 100, 76, 99, 101, 76, 99, 99, 101, 99]\n",
    "add_greedy_nondom_gzip = [26,26,26,26,26,26,26,26,26,26]\n",
    "\n",
    "qtcs_nondom_sed = [86, 86, 86, 86, 86, 86, 86, 86, 86, 86]\n",
    "div_ga_nondom_sed = [89, 62, 103, 101, 89, 97, 97, 94, 101, 103]\n",
    "qaoa_nondom_sed = [183, 183, 183, 183, 183, 183, 183, 183, 183, 183]\n",
    "add_greedy_nondom_sed = [80,80,80,80,80,80,80,80,80,80]\n",
    "\n",
    "NOISE 2%\n",
    "qtcs_nondom_flex = [187, 187, 187, 187, 187, 187, 187, 187, 187, 187]\n",
    "div_ga_nondom_flex = [140, 140, 140, 140, 140, 140, 140, 140, 140, 140]\n",
    "qaoa_nondom_flex = [492, 491, 492, 491, 492, 492, 491, 492, 491, 492]\n",
    "add_greedy_nondom_flex = [205,205,205,205,205,205,205,205,205,205]\n",
    "\n",
    "qtcs_nondom_grep = [192, 192, 193, 192, 193, 192, 193, 192, 193, 193]\n",
    "div_ga_nondom_grep = [70, 70, 70, 70, 70, 70, 70, 70, 70, 70]\n",
    "qaoa_nondom_grep = [321, 321, 321, 321, 321, 321, 321, 321, 321, 321]\n",
    "add_greedy_nondom_grep = [177,177,177,177,177,177,177,177,177,177]\n",
    "\n",
    "qtcs_nondom_gzip = [41, 40, 39, 40, 41, 41, 40, 41, 39, 41]\n",
    "div_ga_nondom_gzip = [105, 105, 105, 105, 105, 105, 105, 105, 105, 105]\n",
    "qaoa_nondom_gzip = [99, 75, 77, 76, 99, 99, 99, 76, 76, 99]\n",
    "add_greedy_nondom_gzip = [26,26,26,26,26,26,26,26,26,26]\n",
    "\n",
    "qtcs_nondom_sed = [86, 86, 86, 86, 86, 86, 86, 86, 86, 86]\n",
    "div_ga_nondom_sed = [89, 62, 103, 101, 89, 97, 97, 94, 101, 103]\n",
    "qaoa_nondom_sed = [183, 183, 183, 183, 183, 183, 183, 183, 183, 183]\n",
    "add_greedy_nondom_sed = [80,80,80,80,80,80,80,80,80,80]\n",
    "\n",
    "NOISE 5%\n",
    "qtcs_nondom_flex = [187, 187, 187, 187, 187, 187, 187, 187, 187, 187]\n",
    "div_ga_nondom_flex = [140, 140, 140, 140, 140, 140, 140, 140, 140, 140]\n",
    "qaoa_nondom_flex = [492, 491, 492, 491, 492, 492, 491, 492, 491, 492]\n",
    "add_greedy_nondom_flex = [205,205,205,205,205,205,205,205,205,205]\n",
    "\n",
    "qtcs_nondom_grep = [192, 192, 193, 192, 193, 192, 193, 192, 193, 193]\n",
    "div_ga_nondom_grep = [70, 70, 70, 70, 70, 70, 70, 70, 70, 70]\n",
    "qaoa_nondom_grep = [321, 321, 321, 321, 321, 321, 321, 321, 321, 321]\n",
    "add_greedy_nondom_grep = [177,177,177,177,177,177,177,177,177,177]\n",
    "\n",
    "qtcs_nondom_gzip = [41, 40, 39, 40, 41, 41, 40, 41, 39, 41]\n",
    "div_ga_nondom_gzip = [105, 105, 105, 105, 105, 105, 105, 105, 105, 105]\n",
    "qaoa_nondom_gzip = [78, 76, 100, 78, 77, 77, 77, 98, 76, 100]\n",
    "add_greedy_nondom_gzip = [26,26,26,26,26,26,26,26,26,26]\n",
    "\n",
    "qtcs_nondom_sed = [86, 86, 86, 86, 86, 86, 86, 86, 86, 86]\n",
    "div_ga_nondom_sed = [89, 62, 103, 101, 89, 97, 97, 94, 101, 103]\n",
    "qaoa_nondom_sed = [183, 183, 183, 183, 183, 183, 183, 183, 183, 183]\n",
    "add_greedy_nondom_sed = [80,80,80,80,80,80,80,80,80,80]\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4cc05bbf4e37a2",
   "metadata": {},
   "source": [
    "#the static values below have been manually stored after each run of the experiments\n",
    "\n",
    "qtcs_nondom_flex = [187, 187, 187, 187, 187, 187, 187, 187, 187, 187]\n",
    "div_ga_nondom_flex = [140, 140, 140, 140, 140, 140, 140, 140, 140, 140]\n",
    "qaoa_nondom_flex = [492, 491, 492, 491, 492, 492, 491, 492, 491, 492]\n",
    "add_greedy_nondom_flex = [205,205,205,205,205,205,205,205,205,205]\n",
    "\n",
    "qtcs_nondom_grep = [192, 192, 193, 192, 193, 192, 193, 192, 193, 193]\n",
    "div_ga_nondom_grep = [70, 70, 70, 70, 70, 70, 70, 70, 70, 70]\n",
    "qaoa_nondom_grep = [321, 321, 321, 321, 321, 321, 321, 321, 321, 321]\n",
    "add_greedy_nondom_grep = [177,177,177,177,177,177,177,177,177,177]\n",
    "\n",
    "qtcs_nondom_gzip = [41, 40, 39, 40, 41, 41, 40, 41, 39, 41]\n",
    "div_ga_nondom_gzip = [105, 105, 105, 105, 105, 105, 105, 105, 105, 105]\n",
    "qaoa_nondom_gzip = [78, 76, 100, 78, 77, 77, 77, 98, 76, 100]\n",
    "add_greedy_nondom_gzip = [26,26,26,26,26,26,26,26,26,26]\n",
    "\n",
    "qtcs_nondom_sed = [86, 86, 86, 86, 86, 86, 86, 86, 86, 86]\n",
    "div_ga_nondom_sed = [89, 62, 103, 101, 89, 97, 97, 94, 101, 103]\n",
    "qaoa_nondom_sed = [183, 183, 183, 183, 183, 183, 183, 183, 183, 183]\n",
    "add_greedy_nondom_sed = [80,80,80,80,80,80,80,80,80,80]\n",
    "\n",
    "lists = [qtcs_nondom_flex,div_ga_nondom_flex,qaoa_nondom_flex,qtcs_nondom_grep,div_ga_nondom_grep,qaoa_nondom_grep,qtcs_nondom_gzip,div_ga_nondom_gzip,qaoa_nondom_gzip,qtcs_nondom_sed,div_ga_nondom_sed,qaoa_nondom_sed]\n",
    "\n",
    "# we are interested in knowing if the obtained sequences are normally distributed to decide what statistical test we should apply\n",
    "for i, list in enumerate(lists, start=1):\n",
    "    stat, p_value = shapiro(list)\n",
    "    print(f\"List {i}: Statistic = {stat:.5f}, P-value = {p_value:.5f}\")\n",
    "    \n",
    "    # check if the sequence is normally distributed (using 0.05 as threshold)\n",
    "    if p_value > 0.05:\n",
    "        print(f\"List {i} seems to be normally distributed (p-value = {p_value:.5f})\")\n",
    "    else:\n",
    "        print(f\"List {i} NOT seems to be normally distributed (p-value = {p_value:.5f})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def a12(lst1,lst2,rev=True):\n",
    "  \"\"\"This method computes the vargha-Delaney's A12 Effect Size\"\"\"\n",
    "  more = same = 0.0\n",
    "  for x in lst1:\n",
    "    for y in lst2:\n",
    "      if x==y : same += 1\n",
    "      elif rev and x > y : more += 1\n",
    "      elif not rev and x < y : more += 1\n",
    "  return (more + 0.5*same)  / (len(lst1)*len(lst2))\n",
    "\n",
    "def stat_test(app1, app2):\n",
    "    \"\"\"This method executes the Mann-Whitney's U Test and the Vargha-Delaney's A12 Effect Size\"\"\"\n",
    "    statistic, pvalue = mannwhitneyu(app1, app2, alternative='greater')\n",
    "\n",
    "    # Calculate the A12 effect size using Vargha and Delaney's formula\n",
    "    a12_effect_size = a12(app1, app2)\n",
    "\n",
    "    return pvalue, a12_effect_size\n",
    "\n",
    "p_value, a_12 = stat_test(div_ga_nondom_gzip,qaoa_nondom_gzip)\n",
    "print(\"p_value: \" + str(p_value))\n",
    "print(\"a12: \" + str(a_12))"
   ],
   "id": "1363080a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Efficiency Empirical Evaluations\n",
    "Regarding the efficiency, we analyzed the algorithms’ **total execution times**. The total run time of **SelectQAOA** was computed by summing the **decomposition** and **execution times** of its run on a **Barbora (IT4Innovations supercomputer)** featuring 8 cores of the **Intel Xeon Gold 6240 CPU** with **2.60 GHz**. \n",
    "\n",
    "For **SelectQA**, executed on the **D-Wave hybrid_binary_quadratic_model_version2**, we considered the **D-Wave API \"total run time\"** metric. \n",
    "\n",
    "The other algorithms' execution times were computed on an **Apple MacBook Air** featuring an **M1 chip** and **16GB of RAM**.\n",
    "\n",
    "Finally, to ensure the empirical reliability of the results, we used the **Mann-Whitney U test** and the **Vargha-Delaney effect size** $\\hat{A}_{12}$."
   ],
   "id": "cb803100148cebcb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#the static values below have been manually stored after each run of the experiments\n",
    "#in MILLISECONDS\n",
    "\n",
    "qtcs_exectime_flex = [2997.246, 2990.427, 2994.078, 2990.127, 2996.457, 2999.987, 2996.602, 2999.604, 2996.952, 2998.930]\n",
    "div_ga_exectime_flex = [203222.63,204047.16,210975.58,225586.77,231729.42,211299.48,222097.04,214766.56,217488.39,252346.23]\n",
    "add_greedy_exectime_flex = [8335,8335,8335,8335,8335,8335,8335,8335,8335,8335]\n",
    "\n",
    "qtcs_exectime_grep = [2988.608, 2993.446, 2996.067, 2993.917, 2989.430, 2996.594, 3000.109, 2987.973, 2991.871, 2993.156]\n",
    "div_ga_exectime_grep = [99435.02,82829.09,92250.62,85914.50,86121.72,85333.98,88159.41,82616.48,84917.17,92114.53]\n",
    "add_greedy_exectime_grep = [8884.02,8884.02,8884.02,8884.02,8884.02,8884.02,8884.02,8884.02,8884.02,8884.02]\n",
    "\n",
    "qtcs_exectime_gzip = [2987.110, 2986.404, 2989.508, 2985.612, 2991.988, 2990.904, 2988.741, 2985.020, 2995.317, 2994.872]\n",
    "div_ga_exectime_gzip = [17102.16, 17278.33, 17811.58, 18235.34, 18301.97, 18632.74, 20023.70, 20778.76, 22085.81, 23753.74]\n",
    "add_greedy_exectime_gzip = [232,232.7,232.7,232.7,232.7,232.7,232.7,232.7,232.7,232.7]\n",
    "\n",
    "qtcs_exectime_sed = [2997.188, 2998.320, 2988.932, 2998.150, 2989.302, 2991.186, 2992.056, 2994.666, 2998.397, 2998.455]\n",
    "div_ga_exectime_sed = [68588.03,74543.25,75144.77,81220.53,70520.88,72081.48,88311.10,85676.48,91617.75,84372.59]\n",
    "add_greedy_exectime_sed = [1896.88,1896.88,1896.88,1896.88,1896.88,1896.88,1896.88,1896.88,1896.88,1896.88]\n"
   ],
   "id": "1313828a3ae56244",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('./results/add-greedy/sed_data.json', 'r') as file:\n",
    "    pareto_fronts_json = json.load(file)\n",
    "\n",
    "greedy_execution_time = pareto_fronts_json['resolution_time(ms)']\n",
    "print(\"Mean Additional Greedy Execution Time(ms)\")\n",
    "print(greedy_execution_time)\n",
    "print(\"Additional Greedy Execution Time St.Dev.\")\n",
    "print(statistics.stdev(add_greedy_exectime_sed))\n",
    "\n",
    "with open('results/selectqa/old/sed-data.json', 'r') as file:\n",
    "    pareto_fronts_json = json.load(file)\n",
    "\n",
    "qtcs_qpu_execution_time = pareto_fronts_json['mean_qpu_run_time(ms)']\n",
    "print(\"Mean SelectQA QPU Execution Time\")\n",
    "print(qtcs_qpu_execution_time)\n",
    "print(\"SelectQA QPU Execution Time St.Dev.\")\n",
    "print(statistics.stdev(qtcs_exectime_sed))\n",
    "\n",
    "with open('./results/divga/sed_pareto_fronts_divga.json', 'r') as file:\n",
    "    pareto_fronts_json = json.load(file)\n",
    "\n",
    "divga_execution_time = pareto_fronts_json['DIVGA_II_mean_execution_time_ms']\n",
    "print(\"Mean DIV-GA Execution Time\")\n",
    "print(divga_execution_time)\n",
    "print(\"DIV-GA Execution Time St.Dev.\")\n",
    "print(statistics.stdev(div_ga_exectime_sed))"
   ],
   "id": "5adc0727-1230-43c8-8e54-e3de645c1029",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open('results/selectqaoa/statevector_sim/sed-data.json', 'r') as file:\n",
    "    pareto_fronts_json = json.load(file)\n",
    "\n",
    "qaoa_qpu_execution_times_sv = pareto_fronts_json[\"all_qpu_run_times(ms)\"]\n",
    "qaoa_qpu_execution_time_sv = pareto_fronts_json['mean_qpu_run_time(ms)']\n",
    "qaoa_pareto_execution_time_stdev_sv = pareto_fronts_json['stdev_qpu_run_time(ms)']\n",
    "print(\"Mean SelectQAOA Execution Time (Statevector)\")\n",
    "print(qaoa_qpu_execution_time_sv)\n",
    "print(\"SelectQAOA Execution Time St.Dev. (Statevector)\")\n",
    "print(qaoa_pareto_execution_time_stdev_sv)\n",
    "\n",
    "with open('results/selectqaoa/ideal/sed-data.json', 'r') as file:\n",
    "    pareto_fronts_json = json.load(file)\n",
    "\n",
    "qaoa_qpu_execution_times_id = pareto_fronts_json[\"all_qpu_run_times(ms)\"]\n",
    "qaoa_qpu_execution_time_id = pareto_fronts_json['mean_qpu_run_time(ms)']\n",
    "qaoa_pareto_execution_time_stdev_id = pareto_fronts_json['stdev_qpu_run_time(ms)']\n",
    "print(\"Mean SelectQAOA Execution Time (Ideal)\")\n",
    "print(qaoa_qpu_execution_time_id)\n",
    "print(\"SelectQAOA Execution Time St.Dev. (Ideal)\")\n",
    "print(qaoa_pareto_execution_time_stdev_id)\n",
    "\n",
    "with open('results/selectqaoa/fake_vigo/sed-data.json', 'r') as file:\n",
    "    pareto_fronts_json = json.load(file)\n",
    "\n",
    "qaoa_qpu_execution_times_fv = pareto_fronts_json[\"all_qpu_run_times(ms)\"]\n",
    "qaoa_qpu_execution_time_fv = pareto_fronts_json['mean_qpu_run_time(ms)']\n",
    "qaoa_pareto_execution_time_stdev_fv = pareto_fronts_json['stdev_qpu_run_time(ms)']\n",
    "print(\"Mean SelectQAOA Execution Time (Fake Vigo)\")\n",
    "print(qaoa_qpu_execution_time_fv)\n",
    "print(\"SelectQAOA Execution Time St.Dev. (Fake Vigo)\")\n",
    "print(qaoa_pareto_execution_time_stdev_fv)\n",
    "\n",
    "with open('results/selectqaoa/depolarizing_sim/01/sed-data.json', 'r') as file:\n",
    "    pareto_fronts_json = json.load(file)\n",
    "\n",
    "qaoa_qpu_execution_times_d1 = pareto_fronts_json[\"all_qpu_run_times(ms)\"]\n",
    "qaoa_qpu_execution_time_d1 = pareto_fronts_json['mean_qpu_run_time(ms)']\n",
    "qaoa_pareto_execution_time_stdev_d1 = pareto_fronts_json['stdev_qpu_run_time(ms)']\n",
    "print(\"Mean SelectQAOA Execution Time (Dep Error 1%)\")\n",
    "print(qaoa_qpu_execution_time_d1)\n",
    "print(\"SelectQAOA Execution Time St.Dev. (Dep Error 1%)\")\n",
    "print(qaoa_pareto_execution_time_stdev_d1)\n",
    "\n",
    "with open('results/selectqaoa/depolarizing_sim/02/sed-data.json', 'r') as file:\n",
    "    pareto_fronts_json = json.load(file)\n",
    "\n",
    "qaoa_qpu_execution_times_d2 = pareto_fronts_json[\"all_qpu_run_times(ms)\"]\n",
    "qaoa_qpu_execution_time_d2 = pareto_fronts_json['mean_qpu_run_time(ms)']\n",
    "qaoa_pareto_execution_time_stdev_d2 = pareto_fronts_json['stdev_qpu_run_time(ms)']\n",
    "print(\"Mean SelectQAOA Execution Time (Dep Error 2%)\")\n",
    "print(qaoa_qpu_execution_time_d2)\n",
    "print(\"SelectQAOA Execution Time St.Dev. (Dep Error 2%)\")\n",
    "print(qaoa_pareto_execution_time_stdev_d2)\n",
    "\n",
    "with open('results/selectqaoa/depolarizing_sim/05/sed-data.json', 'r') as file:\n",
    "    pareto_fronts_json = json.load(file)\n",
    "\n",
    "qaoa_qpu_execution_times_d5 = pareto_fronts_json[\"all_qpu_run_times(ms)\"]\n",
    "qaoa_qpu_execution_time_d5 = pareto_fronts_json['mean_qpu_run_time(ms)']\n",
    "qaoa_pareto_execution_time_stdev_d5 = pareto_fronts_json['stdev_qpu_run_time(ms)']\n",
    "print(\"Mean SelectQAOA Execution Time (Dep Error 5%)\")\n",
    "print(qaoa_qpu_execution_time_d5)\n",
    "print(\"SelectQAOA Execution Time St.Dev. (Dep Error 5%)\")\n",
    "print(qaoa_pareto_execution_time_stdev_d5)"
   ],
   "id": "e389bcaa1ac9dc93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lists = [qtcs_exectime_flex,div_ga_exectime_flex,add_greedy_exectime_flex,qtcs_exectime_grep,div_ga_exectime_grep,add_greedy_exectime_grep,qtcs_exectime_gzip,div_ga_exectime_gzip,add_greedy_exectime_gzip,qtcs_exectime_sed,div_ga_exectime_sed,add_greedy_exectime_sed,qaoa_qpu_execution_times_sv,qaoa_qpu_execution_times_id,qaoa_qpu_execution_times_fv,qaoa_qpu_execution_times_d1,qaoa_qpu_execution_times_d2,qaoa_qpu_execution_times_d5]\n",
    "\n",
    "for list in lists:\n",
    "    print(statistics.stdev(list))\n",
    "\n",
    "# we are interested in knowing if the obtained sequences are normally distributed to decide what statistical test we should apply\n",
    "for i, list in enumerate(lists, start=1):\n",
    "    stat, p_value = shapiro(list)\n",
    "    print(f\"List {i}: Statistic = {stat:.5f}, P-value = {p_value:.5f}\")\n",
    "    \n",
    "    # check if the sequence is normally distributed (using 0.05 as threshold)\n",
    "    if p_value > 0.05:\n",
    "        print(f\"List {i} seems to be normally distributed (p-value = {p_value:.5f})\")\n",
    "    else:\n",
    "        print(f\"List {i} NOT seems to be normally distributed (p-value = {p_value:.5f})\")"
   ],
   "id": "511103206159aad9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def a12(lst1,lst2,rev=True):\n",
    "    \"\"\"This function determines the a12 metric value\"\"\"\n",
    "    #how often is x in lst1 more than y in lst2?\n",
    "    more = same = 0.0\n",
    "    for x in lst1:\n",
    "        for y in lst2:\n",
    "          if x==y : same += 1\n",
    "          elif rev and x > y : more += 1\n",
    "          elif not rev and x < y : more += 1\n",
    "    return (more + 0.5*same)  / (len(lst1)*len(lst2))\n",
    "\n",
    "def stat_test(app1, app2):\n",
    "    \"\"\"This function computes the p_value for the U Test\"\"\"\n",
    "    statistic, pvalue = mannwhitneyu(app1, app2, alternative='less')\n",
    "\n",
    "    # Calculate the A12 effect size using Vargha and Delaney's formula\n",
    "    a12_effect_size = a12(app1, app2)\n",
    "\n",
    "    return pvalue, a12_effect_size\n",
    "\n",
    "p_value, a_12 = stat_test(add_greedy_exectime_sed,div_ga_exectime_sed)\n",
    "print(\"p_value: \" + str(p_value))\n",
    "print(\"a12: \" + str(a_12))"
   ],
   "id": "a1dfa21571409a55",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1ed58fef23cbc91f",
   "metadata": {},
   "source": [
    "## Single-Objective SelectQAOA\n",
    "Since the quantum state-of-the-art methods IGDec-QAOA, BootQA, and Single-Objective SelectQA formalize the Test Case Selection problem as a single-objective problem with different objectives than those used by the previously used Multi-Objective algorithms, we implemented the Single-Objective SelectQAOA version. \n",
    "\n",
    "### Chosen Datasets\n",
    "\n",
    "We used datasets chosen for the quantum state-of-the-art original works:\n",
    "\n",
    "- PaintControl: dataset gathered from ABB Robotics Norway;\n",
    "- GSDTSR: dataset gathered from Google.\n",
    "\n",
    " Each test case in both the datasets has values related to the properties:\n",
    " \n",
    "- \"execution time\": time needed by the test case to be executed;\n",
    "- \"failure rate\": the ability, in percentage, of a test case to spot a failure."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:26:08.374453Z",
     "start_time": "2025-03-11T16:26:08.371341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bootqa_programs = [\"gsdtsr\",\"paintcontrol\", \"iofrol\", \"elevator\", \"elevator2\"]\n",
    "bootqa_programs_rep_values = {\"gsdtsr\":1,\"paintcontrol\":1,\"iofrol\":1, \"elevator\":1, \"elevator2\":1}\n",
    "experiments = 10"
   ],
   "id": "af2fd91994c47932",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "5cce857b18a69eed",
   "metadata": {},
   "source": [
    "def bootstrap_confidence_interval(data, num_samples, confidence_alpha=0.95):\n",
    "    \"\"\"This function determines the statistical range within we would expect the mean value of execution times to fall; it relies on the bootstrapping strategy, which allows the calculation of the confidence interval by repeated sampling (with replacement) from the existing data to obtain an estimate of the confidence interval.\"\"\"\n",
    "    sample_means = []\n",
    "    for _ in range(num_samples):\n",
    "        bootstrap_sample = [random.choice(data) for _ in range(len(data))]\n",
    "        sample_mean = np.mean(bootstrap_sample)\n",
    "        sample_means.append(sample_mean)\n",
    "    \n",
    "    lower_percentile = (1 - confidence_alpha) / 2 * 100\n",
    "    upper_percentile = (confidence_alpha + (1 - confidence_alpha) / 2) * 100\n",
    "    lower_bound = np.percentile(sample_means, lower_percentile)\n",
    "    upper_bound = np.percentile(sample_means, upper_percentile)\n",
    "    \n",
    "    return lower_bound, upper_bound"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "846f18771ca2956d",
   "metadata": {},
   "source": [
    "def make_linear_terms_bootqa(cluster_test_cases, test_cases_costs, test_cases_rates, alpha):\n",
    "    \"\"\"Making the linear terms of the QUBO\"\"\"\n",
    "    max_cost = max(test_cases_costs)\n",
    "    \n",
    "    estimated_costs = []\n",
    "\n",
    "    #linear coefficients, that are the diagonal of the matrix encoding the QUBO\n",
    "    for test_case in cluster_test_cases:\n",
    "        estimated_costs.append((alpha * ((test_cases_costs[test_case])/max_cost)) - ((1-alpha)*test_cases_rates[test_case]))\n",
    "    \n",
    "    return np.array(estimated_costs)\n",
    "\n",
    "def make_linear_terms_bootqa2(cluster_test_cases, test_cases_costs, pcount, dist, alpha):\n",
    "    \"\"\"Making the linear terms of the QUBO\"\"\"\n",
    "    max_cost = max(test_cases_costs)\n",
    "    max_pcount = max(pcount)\n",
    "    max_dist = max(dist)\n",
    "    \n",
    "    estimated_costs = []\n",
    "\n",
    "    #linear coefficients, that are the diagonal of the matrix encoding the QUBO\n",
    "    for test_case in cluster_test_cases:\n",
    "        estimated_costs.append(((alpha/3) * ((test_cases_costs[test_case])/max_cost)) - ((alpha/3) * ((pcount[test_case])/max_pcount)) - ((alpha/3) * ((dist[test_case])/max_dist)))\n",
    "    \n",
    "    return np.array(estimated_costs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5209356b015e7dd",
   "metadata": {},
   "source": [
    "def create_linear_qubo(linear_terms):\n",
    "    \"\"\"This function is the one that has to encode the QUBO problem that QAOA will have to solve. The QUBO problem specifies the optimization to solve and a quadratic binary unconstrained problem\"\"\"\n",
    "    qubo = QuadraticProgram()\n",
    "    \n",
    "    for i in range(0,len(linear_terms)):\n",
    "        qubo.binary_var('x%s' % (i))\n",
    "\n",
    "    qubo.minimize(linear=linear_terms)\n",
    "\n",
    "    return qubo"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Clustering Decomposition",
   "id": "b164c120083a967e"
  },
  {
   "cell_type": "code",
   "id": "c3f636fd6c3c18d3",
   "metadata": {},
   "source": [
    "def get_data(data_name):\n",
    "    \"\"\"Read the datasets\"\"\"\n",
    "    if data_name == \"elevator\":\n",
    "        data = pd.read_csv(\"datasets/quantum_sota_datasets/elevator.csv\", dtype={\"cost\": int, \"input_div\": float})\n",
    "    elif data_name == \"elevator2\":\n",
    "        data = pd.read_csv(\"datasets/quantum_sota_datasets/elevator.csv\", dtype={\"cost\": int, \"pcount\": int, \"dist\": int})\n",
    "    else:\n",
    "        data = pd.read_csv(\"datasets/quantum_sota_datasets/\" + data_name + \".csv\", dtype={\"time\": float, \"rate\": float})\n",
    "        data = data[data['rate'] > 0]\n",
    "    return data\n",
    "\n",
    "bootqa_clusters = dict()\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    data = get_data(bootqa_program)\n",
    "    \n",
    "    # Total suite metrics\n",
    "    if bootqa_program == \"elevator\" or bootqa_program == \"elevator2\":\n",
    "        test_cases_costs = data[\"cost\"].tolist()\n",
    "    else:\n",
    "        test_cases_costs = data[\"time\"].tolist()\n",
    "    if bootqa_program == \"elevator\":\n",
    "        test_cases_rates = data[\"input_div\"].tolist()\n",
    "        print(f\"Tot suite cost: {sum(test_cases_costs)}\")\n",
    "        print(f\"Tot suite input div: {sum(test_cases_rates)}\")\n",
    "    elif bootqa_program == \"elevator2\":\n",
    "        test_cases_pcount = data[\"pcount\"].tolist()\n",
    "        test_cases_dist = data[\"dist\"].tolist()\n",
    "        print(f\"Tot suite cost: {sum(test_cases_costs)}\")\n",
    "        print(f\"Tot suite pcount: {sum(test_cases_pcount)}\")\n",
    "        print(f\"Tot suite dist: {sum(test_cases_dist)}\")\n",
    "    else:\n",
    "        test_cases_rates = data[\"rate\"].tolist()\n",
    "        print(f\"Tot suite cost: {sum(test_cases_costs)}\")\n",
    "        print(f\"Tot suite rate: {sum(test_cases_rates)}\")\n",
    "        \n",
    "    # Normalize data\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        cluster_data = np.column_stack((test_cases_costs, test_cases_rates))\n",
    "    else:\n",
    "        cluster_data = np.column_stack((test_cases_costs, test_cases_pcount, test_cases_dist))\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = scaler.fit_transform(cluster_data)\n",
    "    \n",
    "    num_clusters = 50\n",
    "        \n",
    "    max_cluster_dim = 24\n",
    "    \n",
    "    start = time.time()\n",
    "    linkage_matrix = linkage(normalized_data, method='ward')\n",
    "    clusters = fcluster(linkage_matrix, t=num_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Organize test cases by cluster\n",
    "    clustered_data = defaultdict(list)\n",
    "    for idx, cluster_id in enumerate(clusters):\n",
    "        clustered_data[cluster_id].append(idx)\n",
    "    \n",
    "    # Process clusters to ensure none exceed max_cluster_dim\n",
    "    new_cluster_id = max(clustered_data.keys()) + 1  # Start new IDs after existing ones\n",
    "    to_add = []  # Collect new smaller clusters\n",
    "    \n",
    "    for cluster_id, elements in list(clustered_data.items()):  # Avoid modifying dict during iteration\n",
    "        if len(elements) > max_cluster_dim:\n",
    "            num_splits = -(-len(elements) // max_cluster_dim)  # Ceiling division to get the required number of splits\n",
    "            split_size = -(-len(elements) // num_splits)  # Recalculate to distribute elements evenly\n",
    "            \n",
    "            # Split while keeping sizes balanced\n",
    "            parts = [elements[i:i + split_size] for i in range(0, len(elements), split_size)]\n",
    "    \n",
    "            # Ensure all new clusters are within max_cluster_dim\n",
    "            for part in parts:\n",
    "                if len(part) > max_cluster_dim:\n",
    "                    raise ValueError(f\"A split cluster still exceeds max_cluster_dim ({len(part)} > {max_cluster_dim})!\")\n",
    "    \n",
    "            # Add new parts to the new clusters\n",
    "            to_add.extend(parts)\n",
    "    \n",
    "            # Remove original large cluster\n",
    "            del clustered_data[cluster_id]\n",
    "    \n",
    "    # Assign new IDs to split parts\n",
    "    for part in to_add:\n",
    "        if part:  # Only add if the part is non-empty\n",
    "            clustered_data[new_cluster_id] = part\n",
    "            new_cluster_id += 1\n",
    "    end = time.time()\n",
    "    print(\"SelectQAOA Decomposition Time(ms): \" + str((end-start)*1000))\n",
    "    \n",
    "    bootqa_clusters[bootqa_program] = clustered_data\n",
    "    \n",
    "    # Step 3: Calculate the metrics for each refined cluster\n",
    "    cluster_metrics = {}\n",
    "    for cluster_id, members in clustered_data.items():\n",
    "        tot_cluster_costs = sum(test_cases_costs[i] for i in members)\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            tot_cluster_rates = sum(test_cases_rates[i] for i in members)\n",
    "        else:\n",
    "            tot_cluster_pcount = sum(test_cases_pcount[i] for i in members)\n",
    "            tot_cluster_dist = sum(test_cases_dist[i] for i in members)\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            cluster_metrics[cluster_id] = {\n",
    "                \"tot_cluster_cost\": tot_cluster_costs,\n",
    "                \"tot_cluster_rates\": tot_cluster_rates\n",
    "            }\n",
    "        else:\n",
    "            cluster_metrics[cluster_id] = {\n",
    "                \"tot_cluster_cost\": tot_cluster_costs,\n",
    "                \"tot_cluster_pcount\": tot_cluster_pcount,\n",
    "                \"tot_cluster_dist\": tot_cluster_dist\n",
    "            }\n",
    "        print(f\"Cluster {cluster_id + 1} metrics:\")\n",
    "        print(f\"Test Cases: {members}\")\n",
    "        print(f\" - Num. Test Cases: {len(members):.2f}\")\n",
    "        print(f\" - Execution Cost: {tot_cluster_costs:.2f}\")\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            print(f\" - Failure Rate: {tot_cluster_rates}\")\n",
    "        else:\n",
    "            print(f\" - PCount: {tot_cluster_pcount}\")\n",
    "            print(f\" - Dist: {tot_cluster_dist}\")\n",
    "    \n",
    "    print(\"===========================================================================\")    \n",
    "    \n",
    "    for cluster_id in clustered_data.keys():\n",
    "        if len(clustered_data[cluster_id]) > max_cluster_dim:\n",
    "            print(\"Program: \" + bootqa_program)\n",
    "            print(\"Test cases of cluster \" + str(cluster_id) + \": \" + str(len(clustered_data[cluster_id])))\n",
    "    \n",
    "    # Plotting the clusters in 3D space\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Extracting data for plotting\n",
    "    exec_costs = np.array(test_cases_costs)\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        rates = np.array(test_cases_rates)\n",
    "    else:\n",
    "        pcounts = np.array(test_cases_pcount)\n",
    "        dists = np.array(test_cases_dist)\n",
    "    \n",
    "    # Plot each refined cluster with a different color\n",
    "    colors = plt.cm.get_cmap(\"tab10\", len(clustered_data))  # A colormap with as many colors as clusters\n",
    "    for cluster_id, members in clustered_data.items():\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            ax.scatter(\n",
    "                exec_costs[members], \n",
    "                rates[members], \n",
    "                color=colors(cluster_id % 10), \n",
    "                label=f\"Cluster {cluster_id + 1}\"\n",
    "            )\n",
    "        else:\n",
    "            ax.scatter(\n",
    "                exec_costs[members], \n",
    "                pcounts[members], \n",
    "                dists[members],\n",
    "                color=colors(cluster_id % 10), \n",
    "                label=f\"Cluster {cluster_id + 1}\"\n",
    "            )\n",
    "    \n",
    "    # Label the axes\n",
    "    ax.set_xlabel(\"Execution Cost\")\n",
    "    ax.set_ylabel(\"Failure Rate\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Test Case Clustering Visualization\")\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9c603089358add45",
   "metadata": {},
   "source": [
    "\n",
    "## Optimal Single-Objective Depth Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "12bc23d05ffe997c",
   "metadata": {},
   "source": [
    "bootqa_alphas = {\"gsdtsr\": 0.2,\"paintcontrol\": 0.80, \"iofrol\": 0.82, \"elevator\": 0.50, \"elevator2\": 2.9}\n",
    "run_times_dictionary = {\"gsdtsr\": [],\"paintcontrol\": [], \"iofrol\": [], \"elevator\": [], \"elevator2\": []}\n",
    "\n",
    "sim_ideal = AerSimulator()\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    reps = 1\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_ideal), optimizer=COBYLA(100), reps=reps)\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    data = get_data(bootqa_program)\n",
    "    # Total suite metrics\n",
    "    if bootqa_program != \"elevator2\" and bootqa_program != \"elevator\":\n",
    "        test_cases_costs = data[\"time\"].tolist()\n",
    "    else:\n",
    "        test_cases_costs = data[\"cost\"].tolist()\n",
    "    test_cases_rates = None\n",
    "    test_cases_pcount = None\n",
    "    test_cases_dist = None\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        test_cases_rates = data[\"input_div\"].tolist() if bootqa_program == \"elevator\" else data[\"rate\"].tolist()\n",
    "    else:\n",
    "        test_cases_pcount = data[\"pcount\"].tolist()\n",
    "        test_cases_dist = data[\"dist\"].tolist()\n",
    "    \n",
    "    final_test_suite_costs = []\n",
    "    final_failure_rates = []\n",
    "    final_pcounts = []\n",
    "    final_dists = []\n",
    "    for i in range(2):\n",
    "        final_selected_cases = []\n",
    "        cluster_number = 0\n",
    "        for cluster_id in bootqa_clusters[bootqa_program]:\n",
    "            print(\"Cluster: \" + str(bootqa_clusters[bootqa_program][cluster_id]))\n",
    "            linear_terms = None\n",
    "            if bootqa_program != \"elevator2\":\n",
    "                linear_terms = make_linear_terms_bootqa(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs, test_cases_rates, bootqa_alphas[bootqa_program])\n",
    "            else:\n",
    "                linear_terms = make_linear_terms_bootqa2(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs,test_cases_pcount, test_cases_dist, bootqa_alphas[bootqa_program])\n",
    "            linear_qubo = create_linear_qubo(linear_terms)\n",
    "            print(\"Linear QUBO: \" + str(linear_qubo))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(linear_qubo)\n",
    "            print(\"QAOA Result: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            run_times_dictionary[bootqa_program].append((e-s)*1000)\n",
    "            \n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_cases = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_cases))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_cases:\n",
    "                selected_tests.append(bootqa_clusters[bootqa_program][cluster_id][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_test_suite_costs:\n",
    "                    final_selected_cases.append(selected_test)\n",
    "            \n",
    "        #compute the final test suite cost\n",
    "        final_test_suite_cost = 0\n",
    "        for selected_test_case in final_selected_cases:\n",
    "            final_test_suite_cost += test_cases_costs[selected_test_case]\n",
    "        final_test_suite_costs.append(final_test_suite_cost)\n",
    "            \n",
    "        #compute the total failure rate\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            final_failure_rate = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_failure_rate += test_cases_rates[selected_test_case]\n",
    "            final_failure_rates.append(final_failure_rate)\n",
    "        else:\n",
    "            final_pcount = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_pcount += test_cases_pcount[selected_test_case]\n",
    "            final_pcounts.append(final_pcount)\n",
    "            \n",
    "            final_dist = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_dist += test_cases_dist[selected_test_case]\n",
    "            final_dists.append(final_dist)\n",
    "    \n",
    "    print(\"Final Test Suite: \" + str(final_selected_cases))\n",
    "    #compute the qpu access times\n",
    "    qpu_run_times_without_zeros = []\n",
    "    for access_time in run_times_dictionary[bootqa_program]:\n",
    "      if access_time != 0:\n",
    "        qpu_run_times_without_zeros.append(access_time)\n",
    "    lower_bound, upper_bound = bootstrap_confidence_interval(qpu_run_times_without_zeros, 1000, 0.95)\n",
    "    for i in range(len(run_times_dictionary[bootqa_program])):\n",
    "      if run_times_dictionary[bootqa_program][i] == 0:\n",
    "          run_times_dictionary[bootqa_program][i] = upper_bound\n",
    "    average_qpu_access_time = statistics.mean(run_times_dictionary[bootqa_program]) \n",
    "    \n",
    "    if bootqa_program == \"elevator\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_input_divs\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_failure_rates, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    if bootqa_program == \"elevator2\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_pcounts\", \"final_dists\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_pcounts, final_dists, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    else:\n",
    "        var_names = [\"final_test_suite_costs\", \"final_failure_rates\",\n",
    "                     \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                     \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_failure_rates, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    output_dir = \"results/selectqaoa/ideal\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Path to save the file\n",
    "    file_path = os.path.join(output_dir, f\"{bootqa_program}-rep-{reps}.csv\")\n",
    "    \n",
    "    # Writing results to the file\n",
    "    with open(file_path, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(var_names)\n",
    "        writer.writerow(values)\n",
    "    print(f\"Results saved to {file_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T16:47:49.326509Z",
     "start_time": "2025-03-11T16:47:49.321258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "GSDTSR (no differences for rep argument)\n",
    "PAINTCONTROL (no differences for rep argument)\n",
    "IOFROL (rep=1 BEST for costs, no differences for failure rates)\n",
    "ELEVATOR (rep=1 BEST for costs, no differences for input divs)\n",
    "ELEVATOR2 (rep=1 BEST for pcounts, no differences for dists and costs)\n",
    "\"\"\""
   ],
   "id": "be92854506c53d90",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGSDTSR (no differences for rep argument)\\nPAINTCONTROL (no differences for rep argument)\\nIOFROL (rep=1 BEST for costs, no differences for failure rates)\\nELEVATOR (rep=1 BEST for costs, no differences for input divs)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T17:01:16.547796Z",
     "start_time": "2025-03-11T17:01:16.528896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vargha_delaney_a12(list1, list2):\n",
    "    \"\"\"\n",
    "    Computes Vargha-Delaney's A12 effect size for two lists.\n",
    "    The output is the probability that an element from list1 is greater than an element from list2.\n",
    "    \"\"\"\n",
    "    n1, n2 = len(list1), len(list2)\n",
    "    count = sum(1 for x in list1 for y in list2 if x > y)\n",
    "    count += 0.5 * sum(1 for x in list1 for y in list2 if x == y)  # Consider ties equally\n",
    "    return count / (n1*n2)\n",
    "\n",
    "data_names = [\"gsdtsr\",\"paintcontrol\",\"iofrol\",\"elevator\",\"elevator2\"]\n",
    "\n",
    "for data_name in data_names:\n",
    "    print(\"Dataset: \" + data_name)\n",
    "    \n",
    "    file_path_qaoa_rep_1 = (f\"./results/selectqaoa/ideal/{data_name}-rep-1.csv\")\n",
    "    file_path_qaoa_rep_2 = (f\"./results/selectqaoa/ideal/{data_name}-rep-2.csv\")\n",
    "    file_path_qaoa_rep_4 = (f\"./results/selectqaoa/ideal/{data_name}-rep-4.csv\")\n",
    "    file_path_qaoa_rep_8 = (f\"./results/selectqaoa/ideal/{data_name}-rep-8.csv\")\n",
    "    file_path_qaoa_rep_16 = (f\"./results/selectqaoa/ideal/{data_name}-rep-16.csv\")\n",
    "    \n",
    "    # read qaoa results\n",
    "    sum_df_qaoa_rep_1 = pd.read_csv(file_path_qaoa_rep_1)\n",
    "    sum_df_qaoa_rep_2 = pd.read_csv(file_path_qaoa_rep_2)\n",
    "    sum_df_qaoa_rep_4 = pd.read_csv(file_path_qaoa_rep_4)\n",
    "    sum_df_qaoa_rep_8 = pd.read_csv(file_path_qaoa_rep_8)\n",
    "    sum_df_qaoa_rep_16 = pd.read_csv(file_path_qaoa_rep_16)\n",
    "    \n",
    "    # Get the lists time_list and rate_list and the variable coverage_level\n",
    "    final_test_suite_costs_qaoa_rep_1 = ast.literal_eval(sum_df_qaoa_rep_1['final_test_suite_costs'].iloc[-1])\n",
    "    final_test_suite_costs_qaoa_rep_2 = ast.literal_eval(sum_df_qaoa_rep_2['final_test_suite_costs'].iloc[-1])\n",
    "    final_test_suite_costs_qaoa_rep_4 = ast.literal_eval(sum_df_qaoa_rep_4['final_test_suite_costs'].iloc[-1])\n",
    "    final_test_suite_costs_qaoa_rep_8 = ast.literal_eval(sum_df_qaoa_rep_8['final_test_suite_costs'].iloc[-1])\n",
    "    final_test_suite_costs_qaoa_rep_16 = ast.literal_eval(sum_df_qaoa_rep_16['final_test_suite_costs'].iloc[-1])\n",
    "    \n",
    "    if data_name != \"elevator2\":\n",
    "        final_failure_rates_qaoa_rep_1 = ast.literal_eval(sum_df_qaoa_rep_1['final_failure_rates'].iloc[-1])\n",
    "        final_failure_rates_qaoa_rep_2 = ast.literal_eval(sum_df_qaoa_rep_2['final_failure_rates'].iloc[-1])\n",
    "        final_failure_rates_qaoa_rep_4 = ast.literal_eval(sum_df_qaoa_rep_4['final_failure_rates'].iloc[-1])\n",
    "        final_failure_rates_qaoa_rep_8 = ast.literal_eval(sum_df_qaoa_rep_8['final_failure_rates'].iloc[-1])\n",
    "        final_failure_rates_qaoa_rep_16 = ast.literal_eval(sum_df_qaoa_rep_16['final_failure_rates'].iloc[-1])\n",
    "    else :\n",
    "        final_pcounts_qaoa_rep_1 = ast.literal_eval(sum_df_qaoa_rep_1['final_pcounts'].iloc[-1])\n",
    "        final_pcounts_qaoa_rep_2 = ast.literal_eval(sum_df_qaoa_rep_2['final_pcounts'].iloc[-1])\n",
    "        final_pcounts_qaoa_rep_4 = ast.literal_eval(sum_df_qaoa_rep_4['final_pcounts'].iloc[-1])\n",
    "        final_pcounts_qaoa_rep_8 = ast.literal_eval(sum_df_qaoa_rep_8['final_pcounts'].iloc[-1])\n",
    "        final_pcounts_qaoa_rep_16 = ast.literal_eval(sum_df_qaoa_rep_16['final_pcounts'].iloc[-1])\n",
    "        final_dists_qaoa_rep_1 = ast.literal_eval(sum_df_qaoa_rep_1['final_dists'].iloc[-1])\n",
    "        final_dists_qaoa_rep_2 = ast.literal_eval(sum_df_qaoa_rep_2['final_dists'].iloc[-1])\n",
    "        final_dists_qaoa_rep_4 = ast.literal_eval(sum_df_qaoa_rep_4['final_dists'].iloc[-1])\n",
    "        final_dists_qaoa_rep_8 = ast.literal_eval(sum_df_qaoa_rep_8['final_dists'].iloc[-1])\n",
    "        final_dists_qaoa_rep_16 = ast.literal_eval(sum_df_qaoa_rep_16['final_dists'].iloc[-1])\n",
    "        \n",
    "    \n",
    "    # Execute H-test\n",
    "    identical_costs = False\n",
    "    identical_rates = False\n",
    "    identical_pcounts = False\n",
    "    identical_dists = False\n",
    "    stat_costs = 0\n",
    "    p_value_costs = 0\n",
    "    stat_rates = 0\n",
    "    p_value_rates = 0\n",
    "    stat_pcounts = 0\n",
    "    p_value_pcounts = 0\n",
    "    stat_dists = 0\n",
    "    p_value_dists = 0\n",
    "    \n",
    "    try:\n",
    "        stat_costs, p_value_costs = kruskal(final_test_suite_costs_qaoa_rep_1, final_test_suite_costs_qaoa_rep_2, final_test_suite_costs_qaoa_rep_4, final_test_suite_costs_qaoa_rep_8, final_test_suite_costs_qaoa_rep_16)\n",
    "    except:\n",
    "        identical_costs = True\n",
    "    if data_name != \"elevator2\":\n",
    "        try:    \n",
    "            stat_rates, p_value_rates = kruskal(final_failure_rates_qaoa_rep_1, final_failure_rates_qaoa_rep_2, final_failure_rates_qaoa_rep_4, final_failure_rates_qaoa_rep_8, final_failure_rates_qaoa_rep_16)\n",
    "        except:\n",
    "            identical_rates = True\n",
    "    else:\n",
    "        try:\n",
    "            stat_pcounts, stat_dists = kruskal(final_pcounts_qaoa_rep_1, final_pcounts_qaoa_rep_2, final_pcounts_qaoa_rep_4, final_pcounts_qaoa_rep_8, final_pcounts_qaoa_rep_16)\n",
    "        except:\n",
    "            identical_pcounts = True\n",
    "        try:\n",
    "            stat_dists, p_value_dists = kruskal(final_dists_qaoa_rep_1, final_dists_qaoa_rep_2, final_dists_qaoa_rep_4, final_dists_qaoa_rep_8, final_dists_qaoa_rep_16)\n",
    "        except:\n",
    "            identical_dists = True\n",
    "    \n",
    "    # Show results\n",
    "    print(\"Kruskal-Wallis Statistics for Execution Costs: \", stat_costs)\n",
    "    print(\"p-value for Execution Costs: \", p_value_costs)\n",
    "    if data_name == \"elevator\":\n",
    "        print(\"Kruskal-Wallis Statistics for Input Divs: \", stat_rates)\n",
    "        print(\"p-value for Input Divs: \", p_value_rates)\n",
    "    elif data_name != \"elevator2\": \n",
    "        print(\"Kruskal-Wallis Statistics for Failure Rates: \", stat_rates)\n",
    "        print(\"p-value for Failure Rates: \", p_value_rates)\n",
    "    else:\n",
    "        print(\"Kruskal-Wallis Statistics for PCounts: \", stat_rates)\n",
    "        print(\"p-value for PCounts: \", p_value_rates)\n",
    "        print(\"Kruskal-Wallis Statistics for Dists: \", stat_rates)\n",
    "        print(\"p-value for Dists: \", p_value_rates)\n",
    "        \n",
    "    \n",
    "    print(\"Analyzing the Execution Costs\")\n",
    "    \n",
    "    # If p-value indicates significant differences, compute A12 effect size for each pair\n",
    "    if not identical_costs:\n",
    "        if p_value_costs < 0.05:\n",
    "            print(\"\\nSignificant result! Null hypothesis refused.\"\n",
    "                  \"\\nAt least one of the observations came from another population.\"\n",
    "                  \"\\nComputing A12 effect size for each pair of lists:\")\n",
    "            lists = {\"Rep 1\": final_test_suite_costs_qaoa_rep_1, \"Rep 2\": final_test_suite_costs_qaoa_rep_2, \"Rep 4\": final_test_suite_costs_qaoa_rep_4, \"Rep 8\": final_test_suite_costs_qaoa_rep_8, \"Rep 16:\": final_test_suite_costs_qaoa_rep_16}\n",
    "            \n",
    "            for (name1, l1), (name2, l2) in combinations(lists.items(), 2):\n",
    "                a12 = vargha_delaney_a12(l1, l2)\n",
    "                print(f\"A12({name1} > {name2}): {a12:.3f}\")\n",
    "                print(f\"A12({name2} > {name1}): {1-a12:.3f}\\n\")  # Complementary comparison\n",
    "        else:\n",
    "            print(\"Cannot reject the null hypothesis: the lists may come from the same population.\")\n",
    "    else:\n",
    "        print(\"NO DIFFERENCES IN COSTS\")\n",
    "    \n",
    "    if data_name != \"elevator2\": \n",
    "        if not identical_rates:\n",
    "            \n",
    "            if data_name == \"elevator\":\n",
    "                print(\"Analyzing the Input Divs\")\n",
    "            else:\n",
    "                print(\"Analyzing the Failure Rates\")\n",
    "                \n",
    "            if p_value_rates < 0.05:\n",
    "                print(\"\\nSignificant result! Null hypothesis refused.\"\n",
    "                      \"\\nAt least one of the observations came from another population.\"\n",
    "                      \"\\nComputing A12 effect size for each pair of lists:\")\n",
    "                lists = {\"Rep 1\": final_failure_rates_qaoa_rep_1, \"Rep 2\": final_failure_rates_qaoa_rep_2, \"Rep 4\": final_failure_rates_qaoa_rep_4, \"Rep 8\": final_failure_rates_qaoa_rep_8, \"Rep 16:\": final_failure_rates_qaoa_rep_16}\n",
    "                \n",
    "                for (name1, l1), (name2, l2) in combinations(lists.items(), 2):\n",
    "                    a12 = vargha_delaney_a12(l1, l2)\n",
    "                    print(f\"A12({name1} > {name2}): {a12:.3f}\")\n",
    "                    print(f\"A12({name2} > {name1}): {1-a12:.3f}\\n\")  # Complementary comparison\n",
    "            else:\n",
    "                print(\"Cannot reject the null hypothesis: the lists may come from the same population.\")\n",
    "        else:\n",
    "            if data_names == \"elevator\":\n",
    "                print(\"NO DIFFERENCS IN INPUT DIVS\")\n",
    "            else:\n",
    "                print(\"NO DIFFERENCES IN RATES\")\n",
    "            \n",
    "        if identical_rates and identical_costs:\n",
    "            print(\"NO DIFFERENCES\")\n",
    "    else:\n",
    "        if not identical_pcounts:\n",
    "            \n",
    "            print(\"Analyzing the PCounts\")\n",
    "                \n",
    "            if p_value_pcounts < 0.05:\n",
    "                print(\"\\nSignificant result! Null hypothesis refused.\"\n",
    "                      \"\\nAt least one of the observations came from another population.\"\n",
    "                      \"\\nComputing A12 effect size for each pair of lists:\")\n",
    "                lists = {\"Rep 1\": final_pcounts_qaoa_rep_1, \"Rep 2\": final_pcounts_qaoa_rep_2, \"Rep 4\": final_pcounts_qaoa_rep_4, \"Rep 8\": final_pcounts_qaoa_rep_8, \"Rep 16\": final_pcounts_qaoa_rep_16}\n",
    "                \n",
    "                for (name1, l1), (name2, l2) in combinations(lists.items(), 2):\n",
    "                    a12 = vargha_delaney_a12(l1, l2)\n",
    "                    print(f\"A12({name1} > {name2}): {a12:.3f}\")\n",
    "                    print(f\"A12({name2} > {name1}): {1-a12:.3f}\\n\")  # Complementary comparison\n",
    "            else:\n",
    "                print(\"Cannot reject the null hypothesis: the lists may come from the same population.\")\n",
    "        else:\n",
    "            print(\"NO DIFFERENCES IN PCOUNTS\")\n",
    "        \n",
    "        if not identical_dists:\n",
    "            \n",
    "            print(\"Analyzing the Dists\")\n",
    "                \n",
    "            if p_value_dists < 0.05:\n",
    "                print(\"\\nSignificant result! Null hypothesis refused.\"\n",
    "                      \"\\nAt least one of the observations came from another population.\"\n",
    "                      \"\\nComputing A12 effect size for each pair of lists:\")\n",
    "                lists = {\"Rep 1\": final_dists_qaoa_rep_1, \"Rep 2\": final_dists_qaoa_rep_2, \"Rep 4\": final_dists_qaoa_rep_4, \"Rep 8\": final_dists_qaoa_rep_8, \"Rep 16\": final_dists_qaoa_rep_16}\n",
    "                \n",
    "                for (name1, l1), (name2, l2) in combinations(lists.items(), 2):\n",
    "                    a12 = vargha_delaney_a12(l1, l2)\n",
    "                    print(f\"A12({name1} > {name2}): {a12:.3f}\")\n",
    "                    print(f\"A12({name2} > {name1}): {1-a12:.3f}\\n\")  # Complementary comparison\n",
    "            else:\n",
    "                print(\"Cannot reject the null hypothesis: the lists may come from the same population.\")\n",
    "        else:\n",
    "            print(\"NO DIFFERENCES IN DISTS\")\n",
    "    \n",
    "        if identical_pcounts and identical_dists:\n",
    "            print(\"NO DIFFERENCES\")"
   ],
   "id": "b3596b36f9770ce3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: elevator2\n",
      "Kruskal-Wallis Statistics for Execution Costs:  3.9999999999997433\n",
      "p-value for Execution Costs:  0.40600584970987286\n",
      "Kruskal-Wallis Statistics for PCounts:  0\n",
      "p-value for PCounts:  0\n",
      "Kruskal-Wallis Statistics for Dists:  0\n",
      "p-value for Dists:  0\n",
      "Analyzing the Execution Costs\n",
      "Cannot reject the null hypothesis: the lists may come from the same population.\n",
      "Analyzing the PCounts\n",
      "\n",
      "Significant result! Null hypothesis refused.\n",
      "At least one of the observations came from another population.\n",
      "Computing A12 effect size for each pair of lists:\n",
      "A12(Rep 1 > Rep 2): 0.550\n",
      "A12(Rep 2 > Rep 1): 0.450\n",
      "\n",
      "A12(Rep 1 > Rep 4): 0.550\n",
      "A12(Rep 4 > Rep 1): 0.450\n",
      "\n",
      "A12(Rep 1 > Rep 8): 0.550\n",
      "A12(Rep 8 > Rep 1): 0.450\n",
      "\n",
      "A12(Rep 1 > Rep 16:): 0.550\n",
      "A12(Rep 16: > Rep 1): 0.450\n",
      "\n",
      "A12(Rep 2 > Rep 4): 0.500\n",
      "A12(Rep 4 > Rep 2): 0.500\n",
      "\n",
      "A12(Rep 2 > Rep 8): 0.500\n",
      "A12(Rep 8 > Rep 2): 0.500\n",
      "\n",
      "A12(Rep 2 > Rep 16:): 0.500\n",
      "A12(Rep 16: > Rep 2): 0.500\n",
      "\n",
      "A12(Rep 4 > Rep 8): 0.500\n",
      "A12(Rep 8 > Rep 4): 0.500\n",
      "\n",
      "A12(Rep 4 > Rep 16:): 0.500\n",
      "A12(Rep 16: > Rep 4): 0.500\n",
      "\n",
      "A12(Rep 8 > Rep 16:): 0.500\n",
      "A12(Rep 16: > Rep 8): 0.500\n",
      "\n",
      "Analyzing the Dists\n",
      "Cannot reject the null hypothesis: the lists may come from the same population.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Statevector Simulator",
   "id": "95b138e553222615"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bootqa_alphas = {\"gsdtsr\": 0.2,\"paintcontrol\": 0.80, \"iofrol\": 0.82, \"elevator\": 0.50, \"elevator2\": 2.9}\n",
    "run_times_dictionary = {\"gsdtsr\": [],\"paintcontrol\": [], \"iofrol\": [], \"elevator\": [], \"elevator2\": []}\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "backend = Aer.get_backend(\"statevector_simulator\")\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=backend), optimizer=COBYLA(100), reps=bootqa_programs_rep_values[bootqa_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)\n",
    "    data = get_data(bootqa_program)\n",
    "    # Total suite metrics\n",
    "    if bootqa_program != \"elevator2\" and bootqa_program != \"elevator\":\n",
    "        test_cases_costs = data[\"time\"].tolist()\n",
    "    else:\n",
    "        test_cases_costs = data[\"cost\"].tolist()\n",
    "    test_cases_rates = None\n",
    "    test_cases_pcount = None\n",
    "    test_cases_dist = None\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        test_cases_rates = data[\"input_div\"].tolist() if bootqa_program == \"elevator\" else data[\"rate\"].tolist()\n",
    "    else:\n",
    "        test_cases_pcount = data[\"pcount\"].tolist()\n",
    "        test_cases_dist = data[\"dist\"].tolist()\n",
    "    \n",
    "    final_test_suite_costs = []\n",
    "    final_failure_rates = []\n",
    "    final_pcounts = []\n",
    "    final_dists = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_cases = []\n",
    "        cluster_number = 0\n",
    "        for cluster_id in bootqa_clusters[bootqa_program]:\n",
    "            print(\"Cluster: \" + str(bootqa_clusters[bootqa_program][cluster_id]))\n",
    "            linear_terms = None\n",
    "            if bootqa_program != \"elevator2\":\n",
    "                linear_terms = make_linear_terms_bootqa(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs, test_cases_rates, bootqa_alphas[bootqa_program])\n",
    "            else:\n",
    "                linear_terms = make_linear_terms_bootqa2(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs,test_cases_pcount, test_cases_dist, bootqa_alphas[bootqa_program])\n",
    "            linear_qubo = create_linear_qubo(linear_terms)\n",
    "            print(\"Linear QUBO: \" + str(linear_qubo))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(linear_qubo)\n",
    "            print(\"QAOA Result: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            run_times_dictionary[bootqa_program].append((e-s)*1000)\n",
    "            \n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_cases = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_cases))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_cases:\n",
    "                selected_tests.append(bootqa_clusters[bootqa_program][cluster_id][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_test_suite_costs:\n",
    "                    final_selected_cases.append(selected_test)\n",
    "            \n",
    "        #compute the final test suite cost\n",
    "        final_test_suite_cost = 0\n",
    "        for selected_test_case in final_selected_cases:\n",
    "            final_test_suite_cost += test_cases_costs[selected_test_case]\n",
    "        final_test_suite_costs.append(final_test_suite_cost)\n",
    "            \n",
    "        #compute the total failure rate\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            final_failure_rate = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_failure_rate += test_cases_rates[selected_test_case]\n",
    "            final_failure_rates.append(final_failure_rate)\n",
    "        else:\n",
    "            final_pcount = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_pcount += test_cases_pcount[selected_test_case]\n",
    "            final_pcounts.append(final_pcount)\n",
    "            \n",
    "            final_dist = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_dist += test_cases_dist[selected_test_case]\n",
    "            final_dists.append(final_dist)\n",
    "    \n",
    "    print(\"Final Test Suite: \" + str(final_selected_cases))\n",
    "    #compute the qpu access times\n",
    "    qpu_run_times_without_zeros = []\n",
    "    for access_time in run_times_dictionary[bootqa_program]:\n",
    "      if access_time != 0:\n",
    "        qpu_run_times_without_zeros.append(access_time)\n",
    "    lower_bound, upper_bound = bootstrap_confidence_interval(qpu_run_times_without_zeros, 1000, 0.95)\n",
    "    for i in range(len(run_times_dictionary[bootqa_program])):\n",
    "      if run_times_dictionary[bootqa_program][i] == 0:\n",
    "          run_times_dictionary[bootqa_program][i] = upper_bound\n",
    "    average_qpu_access_time = statistics.mean(run_times_dictionary[bootqa_program]) \n",
    "    \n",
    "    if bootqa_program == \"elevator\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_input_divs\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_failure_rates, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    if bootqa_program == \"elevator2\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_pcounts\", \"final_dists\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_pcounts, final_dists, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    else:\n",
    "        var_names = [\"final_test_suite_costs\", \"final_failure_rates\",\n",
    "                     \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                     \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_failure_rates, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    output_dir = \"results/selectqaoa/statevector_sim\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Path to save the file\n",
    "    file_path = os.path.join(output_dir, f\"{bootqa_program}.csv\")\n",
    "    \n",
    "    # Writing results to the file\n",
    "    with open(file_path, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(var_names)\n",
    "        writer.writerow(values)\n",
    "    print(f\"Results saved to {file_path}\")"
   ],
   "id": "1dc0f4aa-da57-42e3-9dc2-de9f8089400c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "61af3d3afde081b",
   "metadata": {},
   "source": [
    "## Fake Vigo Noise Simulator"
   ]
  },
  {
   "cell_type": "code",
   "id": "edab03cf4dfcc5f6",
   "metadata": {},
   "source": [
    "bootqa_alphas = {\"gsdtsr\": 0.2,\"paintcontrol\": 0.80, \"iofrol\": 0.82, \"elevator\": 0.50, \"elevator2\": 2.9}\n",
    "run_times_dictionary = {\"gsdtsr\": [],\"paintcontrol\": [], \"iofrol\": [], \"elevator\": [], \"elevator2\": []}\n",
    "\n",
    "device_backend = FakeVigoV2()\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=device_backend), optimizer=COBYLA(100), reps=bootqa_programs_rep_values[bootqa_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    data = get_data(bootqa_program)\n",
    "    # Total suite metrics\n",
    "    if bootqa_program != \"elevator2\" and bootqa_program != \"elevator\":\n",
    "        test_cases_costs = data[\"time\"].tolist()\n",
    "    else:\n",
    "        test_cases_costs = data[\"cost\"].tolist()\n",
    "    test_cases_rates = None\n",
    "    test_cases_pcount = None\n",
    "    test_cases_dist = None\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        test_cases_rates = data[\"input_div\"].tolist() if bootqa_program == \"elevator\" else data[\"rate\"].tolist()\n",
    "    else:\n",
    "        test_cases_pcount = data[\"pcount\"].tolist()\n",
    "        test_cases_dist = data[\"dist\"].tolist()\n",
    "    \n",
    "    final_test_suite_costs = []\n",
    "    final_failure_rates = []\n",
    "    final_pcounts = []\n",
    "    final_dists = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_cases = []\n",
    "        cluster_number = 0\n",
    "        for cluster_id in bootqa_clusters[bootqa_program]:\n",
    "            print(\"Cluster: \" + str(bootqa_clusters[bootqa_program][cluster_id]))\n",
    "            linear_terms = None\n",
    "            if bootqa_program != \"elevator2\":\n",
    "                linear_terms = make_linear_terms_bootqa(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs, test_cases_rates, bootqa_alphas[bootqa_program])\n",
    "            else:\n",
    "                linear_terms = make_linear_terms_bootqa2(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs,test_cases_pcount, test_cases_dist, bootqa_alphas[bootqa_program])\n",
    "            linear_qubo = create_linear_qubo(linear_terms)\n",
    "            print(\"Linear QUBO: \" + str(linear_qubo))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(linear_qubo)\n",
    "            print(\"Program: \" + str(bootqa_program) + \". It: \" + str(i))\n",
    "            print(\"QAOA Result: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            run_times_dictionary[bootqa_program].append((e-s)*1000)\n",
    "            \n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_cases = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_cases))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_cases:\n",
    "                selected_tests.append(bootqa_clusters[bootqa_program][cluster_id][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_test_suite_costs:\n",
    "                    final_selected_cases.append(selected_test)\n",
    "            \n",
    "        #compute the final test suite cost\n",
    "        final_test_suite_cost = 0\n",
    "        for selected_test_case in final_selected_cases:\n",
    "            final_test_suite_cost += test_cases_costs[selected_test_case]\n",
    "        final_test_suite_costs.append(final_test_suite_cost)\n",
    "            \n",
    "        #compute the total failure rate\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            final_failure_rate = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_failure_rate += test_cases_rates[selected_test_case]\n",
    "            final_failure_rates.append(final_failure_rate)\n",
    "        else:\n",
    "            final_pcount = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_pcount += test_cases_pcount[selected_test_case]\n",
    "            final_pcounts.append(final_pcount)\n",
    "            \n",
    "            final_dist = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_dist += test_cases_dist[selected_test_case]\n",
    "            final_dists.append(final_dist)\n",
    "    \n",
    "    print(\"Final Test Suite: \" + str(final_selected_cases))\n",
    "    #compute the qpu access times\n",
    "    qpu_run_times_without_zeros = []\n",
    "    for access_time in run_times_dictionary[bootqa_program]:\n",
    "      if access_time != 0:\n",
    "        qpu_run_times_without_zeros.append(access_time)\n",
    "    lower_bound, upper_bound = bootstrap_confidence_interval(qpu_run_times_without_zeros, 1000, 0.95)\n",
    "    for i in range(len(run_times_dictionary[bootqa_program])):\n",
    "      if run_times_dictionary[bootqa_program][i] == 0:\n",
    "          run_times_dictionary[bootqa_program][i] = upper_bound\n",
    "    average_qpu_access_time = statistics.mean(run_times_dictionary[bootqa_program]) \n",
    "    \n",
    "    if bootqa_program == \"elevator\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_input_divs\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_failure_rates, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    if bootqa_program == \"elevator2\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_pcounts\", \"final_dists\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_pcounts, final_dists, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    else:\n",
    "        var_names = [\"final_test_suite_costs\", \"final_failure_rates\",\n",
    "                     \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                     \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_failure_rates, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    output_dir = \"results/selectqaoa/fake_vigo\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Path to save the file\n",
    "    file_path = os.path.join(output_dir, f\"{bootqa_program}.csv\")\n",
    "    \n",
    "    # Writing results to the file\n",
    "    with open(file_path, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(var_names)\n",
    "        writer.writerow(values)\n",
    "    print(f\"Results saved to {file_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bdaac2eb4df95a77",
   "metadata": {},
   "source": [
    "## Depolarizing Error Simulator"
   ]
  },
  {
   "cell_type": "code",
   "id": "59773f7ef37eb27f",
   "metadata": {},
   "source": [
    "bootqa_alphas = {\"gsdtsr\": 0.2,\"paintcontrol\": 0.80, \"iofrol\": 0.82, \"elevator\": 0.50, \"elevator2\": 2.9}\n",
    "run_times_dictionary = {\"gsdtsr\": [],\"paintcontrol\": [], \"iofrol\": [], \"elevator\": [], \"elevator2\": []}\n",
    "\n",
    "noise_dep = NoiseModel()\n",
    "\n",
    "# Define a 1% depolarizing error for single-qubit and two-qubit gates\n",
    "depolarizing_error_1q = depolarizing_error(0.01, 1)  # 1% depolarizing error for single-qubit gates\n",
    "depolarizing_error_2q = depolarizing_error(0.01, 2)  # 1% depolarizing error for two-qubit gates\n",
    "\n",
    "# Apply depolarizing noise to single-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_1q, ['rx', 'rz', 'sx', 'x', 'h'])\n",
    "\n",
    "# Apply depolarizing noise to two-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_2q, ['cx', 'cz', 'swap'])\n",
    "\n",
    "sim_dep = AerSimulator(noise_model=noise_dep)\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_dep), optimizer=COBYLA(100), reps=bootqa_programs_rep_values[bootqa_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    data = get_data(bootqa_program)\n",
    "    # Total suite metrics\n",
    "    if bootqa_program != \"elevator2\" and bootqa_program != \"elevator\":\n",
    "        test_cases_costs = data[\"time\"].tolist()\n",
    "    else:\n",
    "        test_cases_costs = data[\"cost\"].tolist()\n",
    "    test_cases_rates = None\n",
    "    test_cases_pcount = None\n",
    "    test_cases_dist = None\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        test_cases_rates = data[\"input_div\"].tolist() if bootqa_program == \"elevator\" else data[\"rate\"].tolist()\n",
    "    else:\n",
    "        test_cases_pcount = data[\"pcount\"].tolist()\n",
    "        test_cases_dist = data[\"dist\"].tolist()\n",
    "    \n",
    "    final_test_suite_costs = []\n",
    "    final_failure_rates = []\n",
    "    final_pcounts = []\n",
    "    final_dists = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_cases = []\n",
    "        cluster_number = 0\n",
    "        for cluster_id in bootqa_clusters[bootqa_program]:\n",
    "            print(\"Cluster: \" + str(bootqa_clusters[bootqa_program][cluster_id]))\n",
    "            linear_terms = None\n",
    "            if bootqa_program != \"elevator2\":\n",
    "                linear_terms = make_linear_terms_bootqa(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs, test_cases_rates, bootqa_alphas[bootqa_program])\n",
    "            else:\n",
    "                linear_terms = make_linear_terms_bootqa2(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs,test_cases_pcount, test_cases_dist, bootqa_alphas[bootqa_program])\n",
    "            linear_qubo = create_linear_qubo(linear_terms)\n",
    "            print(\"Linear QUBO: \" + str(linear_qubo))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(linear_qubo)\n",
    "            print(\"Program: \" + str(bootqa_program) + \". It: \" + str(i))\n",
    "            print(\"QAOA Result: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            run_times_dictionary[bootqa_program].append((e-s)*1000)\n",
    "            \n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_cases = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_cases))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_cases:\n",
    "                selected_tests.append(bootqa_clusters[bootqa_program][cluster_id][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_test_suite_costs:\n",
    "                    final_selected_cases.append(selected_test)\n",
    "            \n",
    "        #compute the final test suite cost\n",
    "        final_test_suite_cost = 0\n",
    "        for selected_test_case in final_selected_cases:\n",
    "            final_test_suite_cost += test_cases_costs[selected_test_case]\n",
    "        final_test_suite_costs.append(final_test_suite_cost)\n",
    "            \n",
    "        #compute the total failure rate\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            final_failure_rate = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_failure_rate += test_cases_rates[selected_test_case]\n",
    "            final_failure_rates.append(final_failure_rate)\n",
    "        else:\n",
    "            final_pcount = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_pcount += test_cases_pcount[selected_test_case]\n",
    "            final_pcounts.append(final_pcount)\n",
    "            \n",
    "            final_dist = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_dist += test_cases_dist[selected_test_case]\n",
    "            final_dists.append(final_dist)\n",
    "    \n",
    "    print(\"Final Test Suite: \" + str(final_selected_cases))\n",
    "    #compute the qpu access times\n",
    "    qpu_run_times_without_zeros = []\n",
    "    for access_time in run_times_dictionary[bootqa_program]:\n",
    "      if access_time != 0:\n",
    "        qpu_run_times_without_zeros.append(access_time)\n",
    "    lower_bound, upper_bound = bootstrap_confidence_interval(qpu_run_times_without_zeros, 1000, 0.95)\n",
    "    for i in range(len(run_times_dictionary[bootqa_program])):\n",
    "      if run_times_dictionary[bootqa_program][i] == 0:\n",
    "          run_times_dictionary[bootqa_program][i] = upper_bound\n",
    "    average_qpu_access_time = statistics.mean(run_times_dictionary[bootqa_program]) \n",
    "    \n",
    "    if bootqa_program == \"elevator\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_input_divs\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_failure_rates, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    if bootqa_program == \"elevator2\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_pcounts\", \"final_dists\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_pcounts, final_dists, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    else:\n",
    "        var_names = [\"final_test_suite_costs\", \"final_failure_rates\",\n",
    "                     \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                     \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_failure_rates, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    output_dir = \"results/selectqaoa/depolarizing_sim/01\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Path to save the file\n",
    "    file_path = os.path.join(output_dir, f\"{bootqa_program}.csv\")\n",
    "    \n",
    "    # Writing results to the file\n",
    "    with open(file_path, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(var_names)\n",
    "        writer.writerow(values)\n",
    "    print(f\"Results saved to {file_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1866e5d1144d73d7",
   "metadata": {},
   "source": [
    "bootqa_alphas = {\"gsdtsr\": 0.2,\"paintcontrol\": 0.80, \"iofrol\": 0.82, \"elevator\": 0.50, \"elevator2\": 2.9}\n",
    "run_times_dictionary = {\"gsdtsr\": [],\"paintcontrol\": [], \"iofrol\": [], \"elevator\": [], \"elevator2\": []}\n",
    "\n",
    "noise_dep = NoiseModel()\n",
    "\n",
    "# Define a 2% depolarizing error for single-qubit and two-qubit gates\n",
    "depolarizing_error_1q = depolarizing_error(0.02, 1)  # 2% depolarizing error for single-qubit gates\n",
    "depolarizing_error_2q = depolarizing_error(0.02, 2)  # 2% depolarizing error for two-qubit gates\n",
    "\n",
    "# Apply depolarizing noise to single-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_1q, ['rx', 'rz', 'sx', 'x', 'h'])\n",
    "\n",
    "# Apply depolarizing noise to two-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_2q, ['cx', 'cz', 'swap'])\n",
    "\n",
    "sim_dep = AerSimulator(noise_model=noise_dep)\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_dep), optimizer=COBYLA(100), reps=bootqa_programs_rep_values[bootqa_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    data = get_data(bootqa_program)\n",
    "    # Total suite metrics\n",
    "    if bootqa_program != \"elevator2\" and bootqa_program != \"elevator\":\n",
    "        test_cases_costs = data[\"time\"].tolist()\n",
    "    else:\n",
    "        test_cases_costs = data[\"cost\"].tolist()\n",
    "    test_cases_rates = None\n",
    "    test_cases_pcount = None\n",
    "    test_cases_dist = None\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        test_cases_rates = data[\"input_div\"].tolist() if bootqa_program == \"elevator\" else data[\"rate\"].tolist()\n",
    "    else:\n",
    "        test_cases_pcount = data[\"pcount\"].tolist()\n",
    "        test_cases_dist = data[\"dist\"].tolist()\n",
    "    \n",
    "    final_test_suite_costs = []\n",
    "    final_failure_rates = []\n",
    "    final_pcounts = []\n",
    "    final_dists = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_cases = []\n",
    "        cluster_number = 0\n",
    "        for cluster_id in bootqa_clusters[bootqa_program]:\n",
    "            print(\"Cluster: \" + str(bootqa_clusters[bootqa_program][cluster_id]))\n",
    "            linear_terms = None\n",
    "            if bootqa_program != \"elevator2\":\n",
    "                linear_terms = make_linear_terms_bootqa(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs, test_cases_rates, bootqa_alphas[bootqa_program])\n",
    "            else:\n",
    "                linear_terms = make_linear_terms_bootqa2(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs,test_cases_pcount, test_cases_dist, bootqa_alphas[bootqa_program])\n",
    "            linear_qubo = create_linear_qubo(linear_terms)\n",
    "            print(\"Linear QUBO: \" + str(linear_qubo))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(linear_qubo)\n",
    "            print(\"Program: \" + str(bootqa_program) + \". It: \" + str(i))\n",
    "            print(\"QAOA Result: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            run_times_dictionary[bootqa_program].append((e-s)*1000)\n",
    "            \n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_cases = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_cases))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_cases:\n",
    "                selected_tests.append(bootqa_clusters[bootqa_program][cluster_id][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_test_suite_costs:\n",
    "                    final_selected_cases.append(selected_test)\n",
    "            \n",
    "        #compute the final test suite cost\n",
    "        final_test_suite_cost = 0\n",
    "        for selected_test_case in final_selected_cases:\n",
    "            final_test_suite_cost += test_cases_costs[selected_test_case]\n",
    "        final_test_suite_costs.append(final_test_suite_cost)\n",
    "            \n",
    "        #compute the total failure rate\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            final_failure_rate = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_failure_rate += test_cases_rates[selected_test_case]\n",
    "            final_failure_rates.append(final_failure_rate)\n",
    "        else:\n",
    "            final_pcount = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_pcount += test_cases_pcount[selected_test_case]\n",
    "            final_pcounts.append(final_pcount)\n",
    "            \n",
    "            final_dist = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_dist += test_cases_dist[selected_test_case]\n",
    "            final_dists.append(final_dist)\n",
    "    \n",
    "    print(\"Final Test Suite: \" + str(final_selected_cases))\n",
    "    #compute the qpu access times\n",
    "    qpu_run_times_without_zeros = []\n",
    "    for access_time in run_times_dictionary[bootqa_program]:\n",
    "      if access_time != 0:\n",
    "        qpu_run_times_without_zeros.append(access_time)\n",
    "    lower_bound, upper_bound = bootstrap_confidence_interval(qpu_run_times_without_zeros, 1000, 0.95)\n",
    "    for i in range(len(run_times_dictionary[bootqa_program])):\n",
    "      if run_times_dictionary[bootqa_program][i] == 0:\n",
    "          run_times_dictionary[bootqa_program][i] = upper_bound\n",
    "    average_qpu_access_time = statistics.mean(run_times_dictionary[bootqa_program]) \n",
    "    \n",
    "    if bootqa_program == \"elevator\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_input_divs\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_failure_rates, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    if bootqa_program == \"elevator2\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_pcounts\", \"final_dists\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_pcounts, final_dists, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    else:\n",
    "        var_names = [\"final_test_suite_costs\", \"final_failure_rates\",\n",
    "                     \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                     \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_failure_rates, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    output_dir = \"results/selectqaoa/depolarizing_sim/02\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Path to save the file\n",
    "    file_path = os.path.join(output_dir, f\"{bootqa_program}.csv\")\n",
    "    \n",
    "    # Writing results to the file\n",
    "    with open(file_path, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(var_names)\n",
    "        writer.writerow(values)\n",
    "    print(f\"Results saved to {file_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3831e7f5a8bae7e2",
   "metadata": {},
   "source": [
    "bootqa_alphas = {\"gsdtsr\": 0.2,\"paintcontrol\": 0.80, \"iofrol\": 0.82, \"elevator\": 0.50, \"elevator2\": 2.9}\n",
    "run_times_dictionary = {\"gsdtsr\": [],\"paintcontrol\": [], \"iofrol\": [], \"elevator\": [], \"elevator2\": []}\n",
    "\n",
    "noise_dep = NoiseModel()\n",
    "\n",
    "# Define a 5% depolarizing error for single-qubit and two-qubit gates\n",
    "depolarizing_error_1q = depolarizing_error(0.05, 1)  # 5% depolarizing error for single-qubit gates\n",
    "depolarizing_error_2q = depolarizing_error(0.05, 2)  # 5% depolarizing error for two-qubit gates\n",
    "\n",
    "# Apply depolarizing noise to single-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_1q, ['rx', 'rz', 'sx', 'x', 'h'])\n",
    "\n",
    "# Apply depolarizing noise to two-qubit gates\n",
    "noise_dep.add_all_qubit_quantum_error(depolarizing_error_2q, ['cx', 'cz', 'swap'])\n",
    "\n",
    "sim_dep = AerSimulator(noise_model=noise_dep)\n",
    "\n",
    "algorithm_globals.random_seed = 10598\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    qaoa_mes = QAOA(sampler=BackendSampler(backend=sim_dep), optimizer=COBYLA(100), reps=bootqa_programs_rep_values[bootqa_program])\n",
    "    qaoa = MinimumEigenOptimizer(qaoa_mes)  # using QAOA\n",
    "    data = get_data(bootqa_program)\n",
    "    # Total suite metrics\n",
    "    if bootqa_program != \"elevator2\" and bootqa_program != \"elevator\":\n",
    "        test_cases_costs = data[\"time\"].tolist()\n",
    "    else:\n",
    "        test_cases_costs = data[\"cost\"].tolist()\n",
    "    test_cases_rates = None\n",
    "    test_cases_pcount = None\n",
    "    test_cases_dist = None\n",
    "    if bootqa_program != \"elevator2\":\n",
    "        test_cases_rates = data[\"input_div\"].tolist() if bootqa_program == \"elevator\" else data[\"rate\"].tolist()\n",
    "    else:\n",
    "        test_cases_pcount = data[\"pcount\"].tolist()\n",
    "        test_cases_dist = data[\"dist\"].tolist()\n",
    "    \n",
    "    final_test_suite_costs = []\n",
    "    final_failure_rates = []\n",
    "    final_pcounts = []\n",
    "    final_dists = []\n",
    "    for i in range(experiments):\n",
    "        final_selected_cases = []\n",
    "        cluster_number = 0\n",
    "        for cluster_id in bootqa_clusters[bootqa_program]:\n",
    "            print(\"Cluster: \" + str(bootqa_clusters[bootqa_program][cluster_id]))\n",
    "            linear_terms = None\n",
    "            if bootqa_program != \"elevator2\":\n",
    "                linear_terms = make_linear_terms_bootqa(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs, test_cases_rates, bootqa_alphas[bootqa_program])\n",
    "            else:\n",
    "                linear_terms = make_linear_terms_bootqa2(bootqa_clusters[bootqa_program][cluster_id], test_cases_costs,test_cases_pcount, test_cases_dist, bootqa_alphas[bootqa_program])\n",
    "            linear_qubo = create_linear_qubo(linear_terms)\n",
    "            print(\"Linear QUBO: \" + str(linear_qubo))\n",
    "            #for each iteration get the result\n",
    "            s = time.time()\n",
    "            qaoa_result = qaoa.solve(linear_qubo)\n",
    "            print(\"Program: \" + str(bootqa_program) + \". It: \" + str(i))\n",
    "            print(\"QAOA Result: \" + str(qaoa_result))\n",
    "            e = time.time()\n",
    "            run_times_dictionary[bootqa_program].append((e-s)*1000)\n",
    "            \n",
    "            variable_values = qaoa_result.x\n",
    "            indexes_selected_cases = [index for index, value in enumerate(variable_values) if value == 1]\n",
    "            print(\"Indexes of selected tests to convert. \" + str(indexes_selected_cases))\n",
    "            selected_tests = []\n",
    "            for index in indexes_selected_cases:\n",
    "                selected_tests.append(bootqa_clusters[bootqa_program][cluster_id][index])\n",
    "            print(\"Selected tests: \" + str(selected_tests))\n",
    "            for selected_test in selected_tests:\n",
    "                if selected_test not in final_test_suite_costs:\n",
    "                    final_selected_cases.append(selected_test)\n",
    "            \n",
    "        #compute the final test suite cost\n",
    "        final_test_suite_cost = 0\n",
    "        for selected_test_case in final_selected_cases:\n",
    "            final_test_suite_cost += test_cases_costs[selected_test_case]\n",
    "        final_test_suite_costs.append(final_test_suite_cost)\n",
    "            \n",
    "        #compute the total failure rate\n",
    "        if bootqa_program != \"elevator2\":\n",
    "            final_failure_rate = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_failure_rate += test_cases_rates[selected_test_case]\n",
    "            final_failure_rates.append(final_failure_rate)\n",
    "        else:\n",
    "            final_pcount = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_pcount += test_cases_pcount[selected_test_case]\n",
    "            final_pcounts.append(final_pcount)\n",
    "            \n",
    "            final_dist = 0\n",
    "            for selected_test_case in final_selected_cases:\n",
    "                final_dist += test_cases_dist[selected_test_case]\n",
    "            final_dists.append(final_dist)\n",
    "    \n",
    "    print(\"Final Test Suite: \" + str(final_selected_cases))\n",
    "    #compute the qpu access times\n",
    "    qpu_run_times_without_zeros = []\n",
    "    for access_time in run_times_dictionary[bootqa_program]:\n",
    "      if access_time != 0:\n",
    "        qpu_run_times_without_zeros.append(access_time)\n",
    "    lower_bound, upper_bound = bootstrap_confidence_interval(qpu_run_times_without_zeros, 1000, 0.95)\n",
    "    for i in range(len(run_times_dictionary[bootqa_program])):\n",
    "      if run_times_dictionary[bootqa_program][i] == 0:\n",
    "          run_times_dictionary[bootqa_program][i] = upper_bound\n",
    "    average_qpu_access_time = statistics.mean(run_times_dictionary[bootqa_program]) \n",
    "    \n",
    "    if bootqa_program == \"elevator\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_input_divs\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_failure_rates, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    if bootqa_program == \"elevator2\":\n",
    "        var_names = [\"final_test_suite_costs\", \"final_pcounts\", \"final_dists\",\n",
    "                 \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                 \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_pcounts, final_dists, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    else:\n",
    "        var_names = [\"final_test_suite_costs\", \"final_failure_rates\",\n",
    "                     \"average_qpu_access_time(ms)\", \"stdev_qpu_access_time(ms)\", \"all_qpu_access_times(ms)\",\n",
    "                     \"qpu_lower_bound(ms)\", \"qpu_upper_bound(ms)\", \"qpu_run_times(ms)\"]\n",
    "        values = [final_test_suite_costs, final_failure_rates, average_qpu_access_time, \n",
    "                  statistics.stdev(run_times_dictionary[bootqa_program]), run_times_dictionary[bootqa_program],\n",
    "                  lower_bound, upper_bound, run_times_dictionary[bootqa_program]]\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    output_dir = \"results/selectqaoa/depolarizing_sim/05\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Path to save the file\n",
    "    file_path = os.path.join(output_dir, f\"{bootqa_program}.csv\")\n",
    "    \n",
    "    # Writing results to the file\n",
    "    with open(file_path, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(var_names)\n",
    "        writer.writerow(values)\n",
    "    print(f\"Results saved to {file_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Effectiveness Empirical Evaluations\n",
    "\n",
    "Two compare the Single-Objective SelectQAOA with SelectQA, BootQA, and IGDec-QAOA in terms of effectiveness, we compared the solutions by evaluating the **execution times** and **failure rates** of the test suites obtained by the four strategies.\n",
    "\n",
    "We statistically analyzed the results obtained over ten independent executions by applying the **Mann-Whitney U test** with a **significance level** set at **0.05**. The null hypothesis represents a non-relevant difference between the two approaches.\n",
    "\n",
    "In contrast, if the null hypothesis is rejected, the magnitude of the difference is computed using the **Vargha-Delaney effect size** $\\hat{A}_{12}$ (**we reject the null hypothesis for** p-values $< 0.05$.\n"
   ],
   "id": "5424b574a835f2d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def a12effs(lst1,lst2,rev=True):\n",
    "  \"Computing the Vargha-Delaney's A12 Effect Size\"\n",
    "  more = same = 0.0\n",
    "  for x in lst1:\n",
    "    for y in lst2:\n",
    "      if x==y : same += 1\n",
    "      elif rev and x > y : more += 1\n",
    "      elif not rev and x < y : more += 1\n",
    "  return (more + 0.5*same)  / (len(lst1)*len(lst2))\n",
    "\n",
    "def stat_test(app1, app2, alt):\n",
    "    \"\"\"Computing the Mann-Whitney's U test and the Vargha-Delaney's A12 Effect Size\"\"\"\n",
    "    statistic, pvalue = mannwhitneyu(app1, app2, alternative=alt)\n",
    "\n",
    "    # Calculate the A12 effect size using Vargha and Delaney's formula\n",
    "    a12_effect_size = a12effs(app1, app2)\n",
    "\n",
    "    return pvalue, a12_effect_size\n",
    "\n",
    "def complete_plotter(data_name, bootqa_costs, qaoa_costs, qtcs_costs, bootqa_rates, qaoa_rates, qtcs_rates):\n",
    "    \"\"\"This funcrtion plots the resultying subtestsuites of the compared algorithms\"\"\"\n",
    "    # Create a new figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot isolated points for each dataset\n",
    "    plt.scatter(bootqa_costs, bootqa_rates, label='BootQA', marker='o', color='b', s=50)\n",
    "    plt.scatter(qaoa_costs, qaoa_rates, label='SelectQAOA', marker='x', color='g', s=200)\n",
    "    plt.scatter(qtcs_costs, qtcs_rates, label='SelectQA', marker='s', color='r', s=50)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Cost')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.title(f'Cost vs Rate for {data_name}')\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "702e2261e791a190",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_names = [\"gsdtsr\",\"paintcontrol\",\"iofrol\",\"elevator\",\"elevator2\"]\n",
    "data_names_stats = {\"gsdtsr\": [], \"paintcontrol\": [], \"iofrol\": [], \"elevator\": [], \"elevator2\": []}\n",
    "\n",
    "for data_name in data_names:\n",
    "    if data_name != \"elevator\":\n",
    "        file_path_bootqa = f\"./results/bootqa/{data_name}.csv\"\n",
    "        \n",
    "        # read sum.csv\n",
    "        sum_df_bootqa = pd.read_csv(file_path_bootqa)\n",
    "        \n",
    "        # Get the lists time_list and rate_list and the variable coverage_level\n",
    "        final_test_suite_costs_bootqa = ast.literal_eval(sum_df_bootqa['final_test_suite_costs'].iloc[-1])\n",
    "        final_failure_rates_bootqa = ast.literal_eval(sum_df_bootqa['final_failure_rates'].iloc[-1])\n",
    "        \n",
    "        file_path_qtcs = (f\"./results/selectqa/{data_name}.csv\")\n",
    "        \n",
    "        # read qaoa results\n",
    "        sum_df_qtcs = pd.read_csv(file_path_qtcs)\n",
    "        \n",
    "        # Get the lists time_list and rate_list and the variable coverage_level\n",
    "        final_test_suite_costs_qtcs = ast.literal_eval(sum_df_qtcs['final_test_suite_costs'].iloc[-1])\n",
    "        final_failure_rates_qtcs = ast.literal_eval(sum_df_qtcs['final_failure_rates'].iloc[-1])\n",
    "    \n",
    "    file_path_qaoa = (f\"./results/selectqaoa/depolarizing_sim/05/{data_name}.csv\")\n",
    "    \n",
    "    # read qaoa results\n",
    "    sum_df_qaoa = pd.read_csv(file_path_qaoa)\n",
    "    \n",
    "    # Get the lists time_list and rate_list and the variable coverage_level\n",
    "    final_test_suite_costs_qaoa = ast.literal_eval(sum_df_qaoa['final_test_suite_costs'].iloc[-1])\n",
    "    final_failure_rates_qaoa = ast.literal_eval(sum_df_qaoa['final_failure_rates'].iloc[-1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    cost_p_value_qaoa_bootqa, cost_a12_qaoa_bootqa = stat_test(final_test_suite_costs_qaoa,final_test_suite_costs_bootqa,'less')\n",
    "    rate_p_value_qaoa_bootqa, rate_a12_qaoa_bootqa = stat_test(final_failure_rates_qaoa,final_failure_rates_bootqa,'greater')\n",
    "    cost_p_value_qaoa_qtcs, cost_a12_qaoa_qtcs = stat_test(final_test_suite_costs_qaoa, final_test_suite_costs_qtcs,'less')\n",
    "    rate_p_value_qaoa_qtcs, rate_a12_qaoa_qtcs = stat_test(final_failure_rates_qaoa, final_failure_rates_qtcs,'greater')\n",
    "    cost_p_value_qtcs_bootqa, cost_a12_qtcs_bootqa = stat_test(final_test_suite_costs_qtcs, final_test_suite_costs_bootqa,'less')\n",
    "    rate_p_value_qtcs_bootqa, rate_a12_qtcs_bootqa = stat_test(final_failure_rates_qtcs, final_failure_rates_bootqa,'greater')\n",
    "    \n",
    "    data_names_stats[data_name] = [(cost_p_value_qaoa_bootqa,cost_a12_qaoa_bootqa),\n",
    "                                   (rate_p_value_qaoa_bootqa, rate_a12_qaoa_bootqa),\n",
    "                                   (cost_p_value_qaoa_qtcs, cost_a12_qaoa_qtcs),\n",
    "                                   (rate_p_value_qaoa_qtcs, rate_a12_qaoa_qtcs),\n",
    "                                   (cost_p_value_qtcs_bootqa, cost_a12_qtcs_bootqa),\n",
    "                                   (rate_p_value_qtcs_bootqa,rate_a12_qtcs_bootqa)]\n",
    "    \n",
    "    # let's plot the graphic\n",
    "    complete_plotter(data_name,final_test_suite_costs_bootqa,final_test_suite_costs_qaoa,final_test_suite_costs_qtcs,\n",
    "            final_failure_rates_bootqa,final_failure_rates_qaoa,final_failure_rates_qtcs)\n",
    "    \n",
    "with open('results/selectqaoa/depolarizing_sim/05/stats_results.csv', 'w', newline='') as csvfile:\n",
    "    field_names = [\"data_name\", \"cost_p_value_qaoa_bootqa\", \"cost_a12_qaoa_bootqa\",\n",
    "                   \"rate_p_value_qaoa_bootqa\", \"rate_a12_qaoa_bootqa\", \"cost_p_value_qaoa_qtcs\",\n",
    "                   \"cost_a12_qaoa_qtcs\", \"rate_p_value_qaoa_qtcs\", \"rate_a12_qaoa_qtcs\",\n",
    "                   \"cost_p_value_qtcs_bootqa\", \"cost_a12_qtcs_bootqa\",\n",
    "                   \"rate_p_value_qtcs_bootqa\", \"rate_a12_qtcs_bootqa\"]\n",
    "    writer = csv.DictWriter(csvfile,fieldnames=field_names)\n",
    "    \n",
    "    # writing the header\n",
    "    writer.writeheader()\n",
    "\n",
    "    # rows\n",
    "    for data_name,values in data_names_stats.items():\n",
    "        row = {\n",
    "            'data_name': data_name,\n",
    "            'cost_p_value_qaoa_bootqa': values[0][0],\n",
    "            'cost_a12_qaoa_bootqa': values[0][1],\n",
    "            'rate_p_value_qaoa_bootqa': values[1][0],\n",
    "            'rate_a12_qaoa_bootqa': values[1][1],\n",
    "            'cost_p_value_qaoa_qtcs': values[2][0],\n",
    "            'cost_a12_qaoa_qtcs': values[2][1],\n",
    "            'rate_p_value_qaoa_qtcs': values[3][0],\n",
    "            'rate_a12_qaoa_qtcs': values[3][1],\n",
    "            'cost_p_value_qtcs_bootqa': values[4][0],\n",
    "            'cost_a12_qtcs_bootqa': values[4][1],\n",
    "            'rate_p_value_qtcs_bootqa': values[5][0],\n",
    "            'rate_a12_qtcs_bootqa': values[5][1]\n",
    "        }\n",
    "        writer.writerow(row)"
   ],
   "id": "a0461055d6116517",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Efficiency Empirical Evaluation\n",
    "To compare the efficiency of **SelectQAOA**, **IGDec-QAOA**, **BootQA**, and **SelectQA**, we analyzed their **total run times**.\n",
    "\n",
    "- **BootQA** executes a **local decomposition** through **bootstrap sampling** (in this work, executed on a **MacBook Air with an M1 Chip and 16GB of RAM**) and directly runs the **Advanced System QPU**.\n",
    "- **SelectQA** relies solely on the **hybrid_binary_quadratic_model_version2** to handle highly complex optimization problems.\n",
    "\n",
    "For **BootQA**, we considered the **total run time** as the sum of the **bootstrap sampling process** and **Advanced System QPU execution time**. \n",
    "\n",
    "For **SelectQA**, we computed the **total run time** of the **hybrid_binary_quadratic_model_version2**.\n",
    "\n",
    "**IGDec-QAOA** was runned on a noisy simulator based of **ibm_brisbane**.\n",
    "\n",
    "The total run time of **SelectQAOA** was computed by summing the **decomposition** and **execution times** of its run on a **Barbora (IT4Innovations supercomputer)** featuring 8 cores of the **Intel Xeon Gold 6240 CPU** with **2.60 GHz**. \n",
    "\n",
    "The **empirical reliability** of the findings was statistically validated by analyzing the distribution of the total run times obtained over ten independent runs by each algorithm, using the **Mann-Whitney U test**. The magnitude of the differences between the sequences was quantified using the **Vargha-Delaney effect size** (\\(\\hat{A}_{12}\\))."
   ],
   "id": "90207514746f8ce5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#the static values below have been manually stored after each run of the experiments\n",
    "#in MILLISECONDS(?)\n",
    "bootqa_exectime_gsdtsr = [30.332173582066126, 28.786949595831008, 28.030541394885834, 28.886914087320964, 28.325911383172897, 29.170149448910667, 29.090159383980886, 30.79620377019973, 28.63878090090797, 28.338307444080172]\n",
    "qtcs_exectime_gsdtsr = [2985.403, 2991.318, 2997.451, 2986.678, 2989.235, 2988.779, 2987.003, 2995.853, 2989.247, 2987.187]\n",
    "\n",
    "bootqa_exectime_paintcontrol = [32.38468448628743, 29.509634721272786, 29.978599889526368, 30.158860705973307, 32.14313527974447, 29.091114938761393, 30.612337720133464, 29.695283310343424, 29.793449418538412, 28.483634721272786]\n",
    "qtcs_exectime_paintcontrol = [2990.860, 2994.288, 2997.282, 2996.340, 2996.866, 2994.644, 2993.881, 2991.806, 2992.986, 2989.474]\n",
    "\n",
    "bootqa_exectime_iofrol = []\n",
    "qtcs_exectime_iofrol = []\n",
    "\n",
    "bootqa_exectime_elevator = []\n",
    "qtcs_exectime_elevator = []\n",
    "\n",
    "bootqa_exectime_elevator2 = []\n",
    "qtcs_exectime_elevator2 = []\n",
    "\n",
    "print(\"BootQA Mean Execution Time\")\n",
    "print(statistics.mean(bootqa_exectime_gsdtsr))\n",
    "print(\"BootQA Execution Time StDev\")\n",
    "print(statistics.stdev(bootqa_exectime_gsdtsr))\n",
    "\n",
    "print(\"SelectQA Mean Execution Time\")\n",
    "print(statistics.mean(qtcs_exectime_gsdtsr))\n",
    "print(\"SelectQA Execution Time StDev\")\n",
    "print(statistics.stdev(qtcs_exectime_gsdtsr))\n",
    "\n",
    "file_path_qaoa = (f\"./results/selectqaoa/statevector/gsdtsr.csv\")\n",
    "sum_df_qaoa = pd.read_csv(file_path_qaoa)\n",
    "statevector_times = ast.literal_eval(sum_df_qaoa['qpu_run_times(ms)'].iloc[-1])\n",
    "statevector_mean_time = ast.literal_eval(sum_df_qaoa['average_qpu_access_time(ms)'].iloc[-1])\n",
    "statevector_time_stdev = ast.literal_eval(sum_df_qaoa['stdev_qpu_access_time(ms)'].iloc[-1])\n",
    "print(\"SelectQAOA Mean Execution Time (Statevector)\")\n",
    "print(statevector_mean_time)\n",
    "print(\"SelectQAOA Execution Time StDev (Statevector)\")\n",
    "print(statevector_time_stdev)\n",
    "\n",
    "file_path_qaoa = (f\"./results/selectqaoa/ideal/gsdtsr.csv\")\n",
    "sum_df_qaoa = pd.read_csv(file_path_qaoa)\n",
    "ideal_times = ast.literal_eval(sum_df_qaoa['qpu_run_times(ms)'].iloc[-1])\n",
    "ideal_mean_time = ast.literal_eval(sum_df_qaoa['average_qpu_access_time(ms)'].iloc[-1])\n",
    "ideal_time_stdev = ast.literal_eval(sum_df_qaoa['stdev_qpu_access_time(ms)'].iloc[-1])\n",
    "print(\"SelectQAOA Mean Execution Time (Ideal)\")\n",
    "print(ideal_mean_time)\n",
    "print(\"SelectQAOA Execution Time StDev (Ideal)\")\n",
    "print(ideal_time_stdev)\n",
    "\n",
    "file_path_qaoa = (f\"./results/selectqaoa/fake_vigo/gsdtsr.csv\")\n",
    "sum_df_qaoa = pd.read_csv(file_path_qaoa)\n",
    "fake_times = ast.literal_eval(sum_df_qaoa['qpu_run_times(ms)'].iloc[-1])\n",
    "fake_mean_time = ast.literal_eval(sum_df_qaoa['average_qpu_access_time(ms)'].iloc[-1])\n",
    "fake_time_stdev = ast.literal_eval(sum_df_qaoa['stdev_qpu_access_time(ms)'].iloc[-1])\n",
    "print(\"SelectQAOA Mean Execution Time (Fake Vigo)\")\n",
    "print(fake_mean_time)\n",
    "print(\"SelectQAOA Execution Time StDev (Fake Vigo)\")\n",
    "print(fake_time_stdev)\n",
    "\n",
    "file_path_qaoa = (f\"./results/selectqaoa/depolarizing_sim/01/gsdtsr.csv\")\n",
    "sum_df_qaoa = pd.read_csv(file_path_qaoa)\n",
    "d1_times = ast.literal_eval(sum_df_qaoa['qpu_run_times(ms)'].iloc[-1])\n",
    "d1_mean_time = ast.literal_eval(sum_df_qaoa['average_qpu_access_time(ms)'].iloc[-1])\n",
    "d1_time_stdev = ast.literal_eval(sum_df_qaoa['stdev_qpu_access_time(ms)'].iloc[-1])\n",
    "print(\"SelectQAOA Mean Execution Time (Dep Error 1%)\")\n",
    "print(d1_mean_time)\n",
    "print(\"SelectQAOA Execution Time StDev (Dep Error 1%)\")\n",
    "print(d1_time_stdev)\n",
    "\n",
    "file_path_qaoa = (f\"./results/selectqaoa/depolarizing_sim/02/gsdtsr.csv\")\n",
    "sum_df_qaoa = pd.read_csv(file_path_qaoa)\n",
    "d2_times = ast.literal_eval(sum_df_qaoa['qpu_run_times(ms)'].iloc[-1])\n",
    "d2_mean_time = ast.literal_eval(sum_df_qaoa['average_qpu_access_time(ms)'].iloc[-1])\n",
    "d2_time_stdev = ast.literal_eval(sum_df_qaoa['stdev_qpu_access_time(ms)'].iloc[-1])\n",
    "print(\"SelectQAOA Mean Execution Time (Dep Error 2%)\")\n",
    "print(d2_mean_time)\n",
    "print(\"SelectQAOA Execution Time StDev (Dep Error 2%)\")\n",
    "print(d2_time_stdev)\n",
    "\n",
    "file_path_qaoa = (f\"./results/selectqaoa/depolarizing_sim/05/gsdtsr.csv\")\n",
    "sum_df_qaoa = pd.read_csv(file_path_qaoa)\n",
    "d5_times = ast.literal_eval(sum_df_qaoa['qpu_run_times(ms)'].iloc[-1])\n",
    "d5_mean_time = ast.literal_eval(sum_df_qaoa['average_qpu_access_time(ms)'].iloc[-1])\n",
    "d5_time_stdev = ast.literal_eval(sum_df_qaoa['stdev_qpu_access_time(ms)'].iloc[-1])\n",
    "print(\"SelectQAOA Mean Execution Time (Dep Error 5%)\")\n",
    "print(d5_mean_time)\n",
    "print(\"SelectQAOA Execution Time StDev (Dep Error 5%)\")\n",
    "print(d5_time_stdev)\n",
    "\n",
    "lists = [bootqa_exectime_gsdtsr,qtcs_exectime_gsdtsr,bootqa_exectime_paintcontrol,qtcs_exectime_paintcontrol,statevector_times,ideal_times,fake_times,d1_times,d2_times,d5_times]\n",
    "\n",
    "# we are interested in knowing if the obtained sequences are normally distributed to decide what statistical test we should apply\n",
    "for i, list in enumerate(lists, start=1):\n",
    "    stat, p_value = shapiro(list)\n",
    "    print(f\"List {i}: Statistic = {stat:.5f}, P-value = {p_value:.5f}\")\n",
    "    \n",
    "    # check if the sequence is normally distributed (using 0.05 as threshold)\n",
    "    if p_value > 0.05:\n",
    "        print(f\"List {i} seems to be normally distributed (p-value = {p_value:.5f})\")\n",
    "    else:\n",
    "        print(f\"List {i} NOT seems to be normally distributed (p-value = {p_value:.5f})\")"
   ],
   "id": "1f31784e98c83181",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def a12ef(lst1,lst2,rev=True):\n",
    "    \"\"\"This function determines the a12 metric value\"\"\"\n",
    "    #how often is x in lst1 more than y in lst2?\n",
    "    more = same = 0.0\n",
    "    for x in lst1:\n",
    "        for y in lst2:\n",
    "          if x==y : same += 1\n",
    "          elif rev and x > y : more += 1\n",
    "          elif not rev and x < y : more += 1\n",
    "    return (more + 0.5*same)  / (len(lst1)*len(lst2))\n",
    "\n",
    "def stat_test(app1, app2):\n",
    "    \"\"\"This function computes the p_value for the U Test\"\"\"\n",
    "    statistic, pvalue = mannwhitneyu(app1, app2, alternative='less')\n",
    "\n",
    "    # Calculate the A12 effect size using Vargha and Delaney's formula\n",
    "    a12_effect_size = a12ef(app1, app2)\n",
    "\n",
    "    return pvalue, a12_effect_size\n",
    "\n",
    "p_value, a_12 = stat_test(bootqa_exectime_paintcontrol,qtcs_exectime_paintcontrol)\n",
    "print(\"p_value: \" + str(p_value))\n",
    "print(\"a12: \" + str(a_12))"
   ],
   "id": "4dae2836e814c01b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
