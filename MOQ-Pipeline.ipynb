{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8df92c7f16f61fcc",
   "metadata": {},
   "source": [
    "# SelectQAOA: Regression Test Case Selection Using QAOA\n",
    "Regression testing is an important part of the software development process in software engineering. It is a practice aimed at identifying any regression, which are the emergence of new defects or issues in a software application following changes, enhancements, or updates made to the source code. In other words, regression testing focuses on how changes made to the software can affect the correct behavior of existing features. Regression testing is particularly important in agile software development environments, where changes are made frequently and rapidly. This practice helps ensure that the software remains stable and reliable as it evolves over time. Ideal regression testing would re-run all the available test cases of a given software system. However, in addition to being potentially very costly, this could even be impractical in some case. In this scenario, test case selection is one of the most widely investigated test suite optimization approaches.\n",
    "Test case selection focuses on selecting a subset from an initial test suite to test software changes, i.e., to test whether unmodified parts of a program continue to work correctly after changes involving other parts. Various techniques, such as Integer Programming, symbolic execution, data flow analysis, dependence graph-based methods, and flow graph-based approaches, can be employed to identify the modified portions of the software. Once test cases covering the unchanged program segments are pinpointed using a specific technique, an optimization algorithm (e.g., additional greedy, DIV-GA,\n",
    "SelectQA, BootQA or SelectQAOA) can select a minimal set of these test cases based on certain testing criteria (e.g., branch coverage). The ultimate aim is to reduce the expenses associated with regression testing."
   ]
  },
  {
   "cell_type": "code",
   "id": "26cfee6323f1164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T10:56:10.502318Z",
     "start_time": "2024-10-21T10:56:10.488795Z"
    }
   },
   "source": [
    "#this cell contains all the imports needed by the pipeline\n",
    "#to run it on the browser: jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "import json\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import statistics\n",
    "import csv\n",
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu, shapiro"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "93b4781bceb7a9f2",
   "metadata": {},
   "source": [
    "#this is the constant that represent the number of experiments' iterations\n",
    "tot_run = 10"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f36c35765771b1a0",
   "metadata": {},
   "source": [
    "## The pipeline dataset\n",
    "To experiment the performance of the proposed solution by this work and to compare its results to those of state-of-the-art solutions, 4 public programs have been downloaded from the SIR website. SIR is a repository of software-related artifacts meant to support rigorous controlled experimentation with program analysis and software testing techniques, and education in controlled experimentation. \n",
    "\n",
    "### Chosen SIR Programs\n",
    "The programs that will be used for experimentation have all been written in C and are:\n",
    "- flex (a program that generates a lexical analysis program, based on regular expressions and C statement contained in one or more input files);\n",
    "- grep (a useful program to search form matching patterns in a file);\n",
    "- gzip (a program that substitute a file, generally text files or web pages, with their compressed version)\n",
    "- sed (a powerful program for stream text editing).\n",
    "\n",
    "### Needed information\n",
    "The information needed by the quantum algorithm to work on every one of the 4 programs are:\n",
    "- a fault matrix: it indicates whether a precise test case already found, during previous execution, a bug in the source code or not;\n",
    "- execution cost: it indicates the execution cost of any test case of the suite;\n",
    "- statement coverage: it indicates statement coverage information for every test case. \n",
    "\n",
    "All this information have been gathered through previous experimentation on the four programs mentioned above and written in files organized in the SIR_Programs folder. So, the first goal of the project will be gathering data from these files for computational purposes."
   ]
  },
  {
   "cell_type": "code",
   "id": "7c35ee998a723ebe",
   "metadata": {},
   "source": [
    "#this cell contains all variable definitions that will be useful throughout the entire project\n",
    "sir_programs = [\"flex\",\"grep\",\"gzip\",\"sed\"]\n",
    "sir_programs_tests_number = {\"flex\":567,\"grep\":806,\"gzip\":214,\"sed\":360}\n",
    "sir_programs_end_lines = {\"flex\":14192,\"grep\":13281,\"gzip\":6701,\"sed\":7118}\n",
    "alpha = 0.5"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ad8b693ac894c8c",
   "metadata": {},
   "source": [
    "#let's make a function to read the fault matrices\n",
    "#IMPORTANT: all the fault-matrix files must be renamed as \"fault-matrix\".txt and must be written using the same standard used by the files of this project (i-th line->e0e1e2e3..., where ej is 0 if the i-th test found a bug when launched on the j-th version)\n",
    "def get_fault_list(program_name:str):\n",
    "    \"\"\"This function opens the fault-matrix file of a sir programs and makes a list of binary values for each test case of that program to indicate whether a test case found or not a bug in at least one of the available versions of the program\"\"\"\n",
    "    #open the fault-matrix file of the desired SIR program\n",
    "    program_file = open(\"SIR_Programs/\"+program_name+\"/fault-matrix.txt\")\n",
    "    lines = program_file.readlines()\n",
    "    \n",
    "    #we need a list which elements represent test cases, the i-th element is 1 if the i-th test case\n",
    "    #discovered a fault in the past, 0 otherwise\n",
    "    faults_test_by_test = list()\n",
    "    \n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        if \"1\" in line:\n",
    "            faults_test_by_test.append(1)\n",
    "        else:\n",
    "            faults_test_by_test.append(0)\n",
    "        i += 1\n",
    "    \n",
    "    program_file.close()\n",
    "    \n",
    "    return faults_test_by_test"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2c2a3b4b0b07580",
   "metadata": {},
   "source": [
    "#the next function is able to research into the json coverage information file of each test case\n",
    "#of each sir program to gather information about the single test cases costs and coverage\n",
    "def cost_and_coverage_information_gathering(program_name:str):\n",
    "    \"\"\"The aim of this function is to obtain a dictionary that for each test case of a program indicates its cost, and a dictionary that, for each code line of the program to test, makes a list of all the test cases that run that line\"\"\"\n",
    "    test_case_execution_cost = 0\n",
    "    \n",
    "    execution_cost_test_by_test = dict()\n",
    "    executed_lines_test_by_test = dict()\n",
    "    lines_of_tests_list = dict()\n",
    "    total_file_lines = 0\n",
    "    read_files = []\n",
    "    \n",
    "    for test_case in range(sir_programs_tests_number[program_name]):\n",
    "        #to open the correct file, we must remember that the folders and the json files are\n",
    "        #numbered from 1 and not from 0\n",
    "        if program_name == \"gzip\":\n",
    "            json_name = \"allfile\"\n",
    "        else:\n",
    "            json_name = program_name\n",
    "        test_case_json = open(\"SIR_Programs/\"+program_name+\"/json_\"+program_name+\"/t\"+str(test_case+1)+\"/\"+json_name+str(test_case+1)+\".gcov.json\")\n",
    "        \n",
    "        #read the JSON object as a dictionary\n",
    "        json_data = json.load(test_case_json)\n",
    "        \n",
    "        # to have coverage % we will need the total number of lines to cover\n",
    "        for file in json_data[\"files\"]:\n",
    "            if file[\"file\"] not in read_files:\n",
    "                total_file_lines += len(file[\"lines\"])\n",
    "                read_files.append(file[\"file\"])\n",
    "        \n",
    "        #for programs tested above more than one file, the initial row of a file will start from\n",
    "        #the final row of the preceding file\n",
    "        i = 0\n",
    "        for file in json_data[\"files\"]:\n",
    "            line_count_start = i\n",
    "            for line in file[\"lines\"]:\n",
    "                #if a line is executed, we want to remember FOR THAT LINE which are the tests\n",
    "                #that executed it, and we want to increment the execution cost\n",
    "                if line[\"unexecuted_block\"] == False:\n",
    "                    #the test suite exec cost = sum of the exec freq. of each executed basic block\n",
    "                    #by each test case\n",
    "                    #test_suite_execution_cost += line[\"count\"]\n",
    "                    test_case_execution_cost += line[\"count\"]\n",
    "                    \n",
    "                    if (line_count_start + line[\"line_number\"]) not in executed_lines_test_by_test:\n",
    "                        executed_lines_test_by_test[line_count_start + line[\"line_number\"]] = [test_case]\n",
    "                    else:\n",
    "                        executed_lines_test_by_test[line_count_start + line[\"line_number\"]].append(test_case)\n",
    "                    executed_lines_test_by_test[line_count_start + line[\"line_number\"]].sort()\n",
    "                    \n",
    "                    if test_case not in lines_of_tests_list:\n",
    "                        lines_of_tests_list[test_case] = [line_count_start + line[\"line_number\"]]\n",
    "                    else:\n",
    "                        lines_of_tests_list[test_case].append(line_count_start + line[\"line_number\"])\n",
    "                    lines_of_tests_list[test_case].sort()\n",
    "                i = line[\"line_number\"]\n",
    "        \n",
    "        #saving the total amount of execution cost for this test case and resetting for the next\n",
    "        execution_cost_test_by_test[test_case] = test_case_execution_cost\n",
    "        test_case_execution_cost = 0\n",
    "        \n",
    "        test_case_json.close()\n",
    "                        \n",
    "    return execution_cost_test_by_test, executed_lines_test_by_test, lines_of_tests_list, total_file_lines\n",
    "                    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4a07ab07f350b491",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#we can now gain all the historical information about past fault detection \n",
    "faults_dictionary = {\"flex\": None, \"grep\": None, \"gzip\": None, \"sed\": None}\n",
    "\n",
    "for sir_program in sir_programs:\n",
    "    faults_dictionary[sir_program] = (get_fault_list(sir_program))\n",
    "    \n",
    "print(faults_dictionary)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "70035b3df21f1327",
   "metadata": {},
   "source": [
    "#we can now gain costs and coverage information\n",
    "\n",
    "test_cases_costs = {\"flex\":None,\"grep\":None,\"gzip\":None,\"sed\":None}\n",
    "coverage = {\"flex\":None,\"grep\":None,\"gzip\":None,\"sed\":None}\n",
    "lines_of_tests_list = {\"flex\":None,\"grep\":None,\"gzip\":None,\"sed\":None}\n",
    "total_file_lines = {\"flex\":None,\"grep\":None,\"gzip\":None,\"sed\":None}\n",
    "\n",
    "for sir_program in sir_programs:\n",
    "    costs_and_coverage = cost_and_coverage_information_gathering(sir_program)\n",
    "    test_cases_costs[sir_program] = costs_and_coverage[0]\n",
    "    coverage[sir_program] = costs_and_coverage[1]\n",
    "    lines_of_tests_list[sir_program] = costs_and_coverage[2]\n",
    "    total_file_lines[sir_program] = costs_and_coverage[3]\n",
    "\n",
    "print(test_cases_costs)\n",
    "print(lines_of_tests_list)\n",
    "print(total_file_lines)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec65b50547d46586",
   "metadata": {},
   "source": [
    "## Quantum Approximate Optimization Algorithm (QAOA)\n",
    "//TODO: INSERT HERE PRECISE DESCRIPTION\n",
    "\n",
    "## QUBO Problems\n",
    "A Quadratic Unconstrained Binary Optimization (QUBO) problem is a type of mathematical problem where we seek to find the best combination of binary values (0 or 1) for a set of variables to minimize or maximize an objective function. In other words, we are looking for the optimal solution among all possible variable combinations that satisfies certain constraints and makes the objective function as small as possible.\n",
    "\n",
    "Weights for constraints (or penalty coefficients) are used in QUBO problems to assign a numerical value to the constraints and influence the optimization process. These weights are important because they allow for managing the priority and relative importance of constraints within the optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "id": "abead0754d567232",
   "metadata": {},
   "source": [
    "def create_adjvector_bqm(sir_program, alpha, P):\n",
    "    \"\"\"This function is the one that has to encode the QUBO problem that the D-Wave Hybrid Sampler will have to solve. The QUBO problem specifies the optimization to solve and a quadratic binary unconstrained problem\"\"\"\n",
    "    qubo = dimod.AdjVectorBQM(dimod.BINARY)\n",
    "    max_cost = max(test_cases_costs[sir_program].values())\n",
    "\n",
    "    #linear coefficients, that are the diagonal of the matrix encoding the QUBO\n",
    "    for i in range(sir_programs_tests_number[sir_program]):\n",
    "        cost = (alpha * (test_cases_costs[sir_program][i]/max_cost)) - (1 - alpha) * faults_dictionary[sir_program][i]\n",
    "        qubo.set_linear(i, cost)\n",
    "\n",
    "    #quadratic coefficient, that are the lower part of the QUBO matrix\n",
    "    for k in coverage[sir_program].keys():\n",
    "        test_cases = coverage[sir_program][k]\n",
    "        for i in test_cases:\n",
    "            for j in test_cases:\n",
    "                if i < j:\n",
    "                    qubo.set_linear(i,qubo.linear[i]-P)\n",
    "                    qubo.set_linear(j,qubo.linear[j]-P)\n",
    "                    try:\n",
    "                        qubo.set_quadratic(i, j,qubo.quadratic[i,j] + 2 * P)\n",
    "                    except:\n",
    "                        qubo.set_quadratic(i, j, 2 * P)\n",
    "\n",
    "    return qubo\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ab8c0bce740db9c",
   "metadata": {},
   "source": [
    "penalties_dictionary = {\"flex\":None,\"grep\":None,\"gzip\":None,\"sed\":None}\n",
    "\n",
    "#to obtain a QUBO problem from a quadratic problem with constraints, we have to insert those constraints into the Hamiltonian to solve (which is the one encoded by the QUBO problem). When we insert constraint into the Hamiltonian, we have to specify also penalties\n",
    "for sir_program in sir_programs:\n",
    "    max_penalty = 0\n",
    "    max_cost = max(test_cases_costs[sir_program].values())\n",
    "    for i in range(sir_programs_tests_number[sir_program]):\n",
    "        cost = (alpha * (test_cases_costs[sir_program][i]/max_cost)) - ((1 - alpha) * faults_dictionary[sir_program][i])\n",
    "        if cost > max_penalty:\n",
    "            max_penalty = cost\n",
    "    penalties_dictionary[sir_program] = max_penalty + 1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c7f468b9deadab3",
   "metadata": {},
   "source": [
    "qubos_dictionary = {\"flex\":None,\"grep\":None,\"gzip\":None,\"sed\":None}\n",
    "\n",
    "#make a dictionary that saves, for each program, the correspondent QUBO\n",
    "for sir_program in sir_programs:\n",
    "    qubos_dictionary[sir_program] = create_adjvector_bqm(sir_program,alpha,penalties_dictionary[sir_program])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "abaf9693911723af",
   "metadata": {},
   "source": [
    "def get_stmt_coverage(sir_program,test_cases_list):\n",
    "    covered_lines = set()\n",
    "    \n",
    "    for test_case in test_cases_list:\n",
    "        try:\n",
    "            for covered_line in lines_of_tests_list[sir_program][test_case]:\n",
    "                covered_lines.add(covered_line)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return len(covered_lines)\n",
    "\n",
    "def build_pareto_front(sir_program,selected_tests):\n",
    "    pareto_front = []\n",
    "    max_fault_coverage = 0\n",
    "    max_stmt_coverage = 0\n",
    "    \n",
    "    for index in range(1,len(selected_tests)+1):\n",
    "        #exract the first index selected tests\n",
    "        candidate_solution = selected_tests[:index]\n",
    "        candidate_solution_fault_coverage = 0\n",
    "        candidate_solution_stmt_coverage = 0\n",
    "        for selected_test in candidate_solution:\n",
    "            candidate_solution_fault_coverage += faults_dictionary[sir_program][selected_test]\n",
    "            candidate_solution_stmt_coverage += get_stmt_coverage(sir_program,candidate_solution)\n",
    "        #if the actual pareto front dominates the candidate solution, get to the next candidate\n",
    "        if max_fault_coverage >= candidate_solution_fault_coverage and max_stmt_coverage >= candidate_solution_stmt_coverage:\n",
    "            continue\n",
    "        #eventually update the pareto front information\n",
    "        if candidate_solution_stmt_coverage > max_stmt_coverage:\n",
    "            max_stmt_coverage = candidate_solution_stmt_coverage\n",
    "        if candidate_solution_fault_coverage > max_fault_coverage:\n",
    "            max_fault_coverage = candidate_solution_fault_coverage\n",
    "        #add the candidate solution to the pareto front\n",
    "        pareto_front.append(candidate_solution)\n",
    "    \n",
    "    return pareto_front"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "306aa159e2a51733",
   "metadata": {},
   "source": [
    "#old (original) sampling cell\n",
    "sampler = LeapHybridSampler(token=\"\")\n",
    "\n",
    "#I want to run the sampler 30 times to obtain different results for each sir program\n",
    "for sir_program in sir_programs:\n",
    "    #the fronts will be saved into files\n",
    "    print(sir_program)\n",
    "    file_path = \"results/selectqa/\" + sir_program + \"-data.json\"\n",
    "    json_data = {}\n",
    "    response = None\n",
    "    qpu_run_times = []\n",
    "    pareto_fronts_building_times = []\n",
    "    for i in range(10):\n",
    "        print(str(i))\n",
    "        #for each iteration get the result\n",
    "        response = sampler.sample_qubo(qubos_dictionary[sir_program].to_numpy_matrix())\n",
    "        #sample_dictionary is like {0:0,1:0,2:1,3:1,4:1,5:0,....}\n",
    "        sample_dictionary = {i: response.samples()[0][i] for i in range(len(response.samples()[0]))}\n",
    "        qpu_run_times.append(response.info[\"run_time\"])\n",
    "        #let's extract the selected tests\n",
    "        selected_tests = [test_case for test_case, x_val in sample_dictionary.items() if x_val == 1]\n",
    "        #now we have to build the pareto front\n",
    "        start = time.time()\n",
    "        pareto_front = build_pareto_front(sir_program,selected_tests)\n",
    "        end = time.time()\n",
    "        json_data[\"pareto_front_\"+str(i)] = pareto_front\n",
    "        pareto_front_building_time = (end-start)*1000\n",
    "        pareto_fronts_building_times.append(pareto_front_building_time)\n",
    "    \n",
    "    #compute the average time needed for the construction of a pareto frontier and run time\n",
    "    mean_qpu_run_time = statistics.mean(qpu_run_times)\n",
    "    mean_pareto_fronts_building_time = statistics.mean(pareto_fronts_building_times)\n",
    "    json_data[\"mean_qpu_run_time(ms)\"] = mean_qpu_run_time\n",
    "    json_data[\"mean_pareto_fronts_building_time(ms)\"] = mean_pareto_fronts_building_time\n",
    "    \n",
    "    with open(file_path,\"w\") as file:\n",
    "        json.dump(json_data,file)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "927ce932-68b7-4a3b-a8ed-edc0e7ded093",
   "metadata": {},
   "source": [
    "## Empirical Evaluations\n",
    "From the pareto fronts obtained on 10 different repetitions (for each programme under examination), the non-dominated samples were extrapolated. In this way, for each repetition of each program, a reference front was constructed to allow comparison between the algorithms: quantum annealing, additional greedy and DIV-GA.\n",
    "For each algorithm, divided by programmes, the average number (and std. dev.) of samples in their pareto fronts was calculated. Likewise, the average number (and std. dev.) of samples from their own front present in the reference front was also calculated.\n",
    "The latter metric was subjected to the Mann-Whitney's statistical U test and then to the Vargha-Delaney's $\\hat{A}_{12}$ effect size calculation to establish its validity and statistical magnitude.\n",
    "\n",
    "Also, the execution times of the three different algorithms were stored and confronted. Since the classical and quantum components of the D-Wave's hybrid solver work synchronously, the only metric available for such a comparison was the total \"run-time\"."
   ]
  },
  {
   "cell_type": "code",
   "id": "fd4f851336f7e0fc",
   "metadata": {},
   "source": [
    "#the other sizes are uniform\n",
    "#this static information can be easily obtained from the pareto .json files\n",
    "\n",
    "qtcs_flex_pareto_sizes = [284,283,284,283,284,284,284,283,284,283]\n",
    "divga_flex_pareto_sizes = [140,140,140,140,140,140,140,140,140,140]\n",
    "greedy_flex_size = 567\n",
    "\n",
    "qtcs_grep_pareto_sizes = [402,402,403,403,403,402,403,403,402,404]\n",
    "divga_grep_pareto_sizes = [70,70,70,70,70,70,70,70,70,70]\n",
    "greedy_grep_size = 802\n",
    "\n",
    "qtcs_gzip_pareto_sizes = [107,107,108,107,107,106,107,107,108,108]\n",
    "divga_gzip_pareto_sizes = [105,105,105,105,105,105,105,105,105,105]\n",
    "greedy_gzip_size = 199\n",
    "\n",
    "qtcs_sed_pareto_sizes = [180,180,180,180,180,180,180,180,180,180]\n",
    "divga_sed_pareto_sizes = [105,62,105,105,102,105,105,97,105,105]\n",
    "greedy_sed_size = 356"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c9ada78e44c97be4",
   "metadata": {},
   "source": [
    "def total_cost(sir_program,solution):\n",
    "    solution_cost = 0\n",
    "    \n",
    "    for test_case in solution:\n",
    "        solution_cost += test_cases_costs[sir_program][test_case]\n",
    "\n",
    "    return solution_cost\n",
    "        \n",
    "def total_coverage(sir_program,solution):\n",
    "    covered_lines = set()\n",
    "    \n",
    "    for test_case in solution:\n",
    "        try:\n",
    "            for covered_line in lines_of_tests_list[sir_program][test_case]:\n",
    "                covered_lines.add(covered_line)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return len(covered_lines)\n",
    "    \n",
    "def total_faults(sir_program,solution):\n",
    "    covered_faults = 0\n",
    "    \n",
    "    for test_case in solution:\n",
    "        covered_faults += faults_dictionary[sir_program][test_case]\n",
    "    \n",
    "    return covered_faults\n",
    "\n",
    "def pareto_dominance(tuple1, tuple2):\n",
    "    # Check if all conditions are satisfied\n",
    "    dominates = (\n",
    "        (tuple2[0] <= tuple1[0]) and \n",
    "        (tuple2[1] >= tuple1[1]) and \n",
    "        (tuple2[2] >= tuple1[2])\n",
    "    )\n",
    "    \n",
    "    # Check if at least one condition is strict\n",
    "    strict = (\n",
    "        (tuple2[0] < tuple1[0]) or \n",
    "        (tuple2[1] > tuple1[1]) or \n",
    "        (tuple2[2] > tuple1[2])\n",
    "    )\n",
    "    \n",
    "    # Return 1 if the second tuple dominates the first, otherwise 0\n",
    "    return 1 if dominates and strict else 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37611d4d066cd05b",
   "metadata": {},
   "source": [
    "with open('./results/add-greedy/sed_data.json', 'r') as file:\n",
    "    pareto_fronts_json = json.load(file)\n",
    "    \n",
    "greedy_pareto_front = pareto_fronts_json['pareto_front']\n",
    "\n",
    "#each solution of the pareto front is a subset of the test suite\n",
    "greedy_pareto_vectors = []\n",
    "for front_solution in greedy_pareto_front:\n",
    "    #compute the fitness scores of each single solution\n",
    "    greedy_pareto_vectors.append((total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62693154-2df4-44c5-98d4-54b1b2b2a34d",
   "metadata": {},
   "source": [
    "#here we make the same but for each of the fronts built by SelectQA and DIV-GA\n",
    "\n",
    "#the following 2 lists will contain much tuples as solutions in each of the 10 pareto fronts\n",
    "#each tuple represent the fitness value for each solution\n",
    "qtcs_pareto_vectors = []\n",
    "divga_pareto_vectors = []\n",
    "\n",
    "for index in range(0,10):\n",
    "    # Load the JSON file\n",
    "    with open('./results/selectqa/sed-data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qtcs_pareto_front = pareto_fronts_json['pareto_front_'+str(index)]\n",
    "    #a single solution is a subset of the initial test suite\n",
    "    for front_solution in qtcs_pareto_front:\n",
    "        qtcs_pareto_vectors.append((total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)))\n",
    "    #print(len(qtcs_pareto_vectors))\n",
    "    \n",
    "    with open('./results/divga/sed_pareto_fronts_divga.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    divga_pareto_front = pareto_fronts_json['sed_pareto_front_'+str(index)]\n",
    "    for front_solution in divga_pareto_front:\n",
    "        divga_pareto_vectors.append((total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)))\n",
    "    #print(len(divga_pareto_vectors))\n",
    "\n",
    "#once we have the pareto vectors from each pareto front obtained by each run, we extract \n",
    "#just the solution not dominated by anyone else\n",
    "total_fronts = [qtcs_pareto_vectors,divga_pareto_vectors,greedy_pareto_vectors]\n",
    "reference_pareto = []\n",
    "portions = [0,0,0]\n",
    "\n",
    "print(\"SELECTQA PARETO FRONTIER\")\n",
    "\n",
    "# get the reference frontier\n",
    "for index, front1 in enumerate(total_fronts):\n",
    "    for front_solution1 in front1:\n",
    "        is_dominated = 0\n",
    "        other_fronts = total_fronts[:index] + total_fronts[index+1:]\n",
    "        for front2 in other_fronts:\n",
    "            for front_solution2 in front2:\n",
    "                if pareto_dominance(front_solution1,front_solution2):\n",
    "                    is_dominated = 1\n",
    "                    break\n",
    "            if is_dominated:\n",
    "                break\n",
    "        if not is_dominated:\n",
    "            reference_pareto.append(front_solution1)\n",
    "            portions[index] = portions[index] + 1\n",
    "\n",
    "print(portions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77d824ace461716a",
   "metadata": {},
   "source": [
    "# For SelectQA and DIV-GA we want to compute, for each of the 10 iterations, how many of the solutions of the i-th pareto front were selected by the reference front\n",
    "\n",
    "qtcs_non_dominated_values = []\n",
    "divga_non_dominated_values = []\n",
    "greedy_non_dominated = 0\n",
    "\n",
    "for index in range(0,10):\n",
    "    #print(\"Iteratrion: \" + str(index))\n",
    "    \n",
    "    qtcs_non_dominated = 0\n",
    "    divga_non_dominated = 0\n",
    "    \n",
    "    # Load the JSON file\n",
    "    with open('.\\\\results\\\\selectqa\\\\sed-data.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    qtcs_pareto_front = pareto_fronts_json['pareto_front_'+str(index)]\n",
    "    for front_solution in qtcs_pareto_front:\n",
    "        if (total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)) in reference_pareto:\n",
    "            qtcs_non_dominated += 1\n",
    "    \n",
    "    qtcs_non_dominated_values.append(qtcs_non_dominated)\n",
    "    #print(qtcs_non_dominated)\n",
    "    \n",
    "    with open('.\\\\results\\\\divga\\\\sed_pareto_fronts_divga.json', 'r') as file:\n",
    "        pareto_fronts_json = json.load(file)\n",
    "    \n",
    "    divga_pareto_front = pareto_fronts_json['sed_pareto_front_'+str(index)]\n",
    "    for front_solution in divga_pareto_front:\n",
    "        if (total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)) in reference_pareto:\n",
    "            divga_non_dominated += 1\n",
    "            \n",
    "    divga_non_dominated_values.append(divga_non_dominated)\n",
    "    #print(divga_non_dominated)\n",
    "    \n",
    "for front_solution in greedy_pareto_front:\n",
    "    if (total_cost(\"sed\",front_solution),total_coverage(\"sed\",front_solution),total_faults(\"sed\",front_solution)) in reference_pareto:\n",
    "        greedy_non_dominated += 1\n",
    "print(greedy_non_dominated)\n",
    "\n",
    "print(\"QTCS/DIVGA/ADD GREEDY\")\n",
    "print(qtcs_non_dominated_values)\n",
    "print(statistics.mean(qtcs_non_dominated_values))\n",
    "print(statistics.stdev(qtcs_non_dominated_values))\n",
    "print(statistics.mean(divga_non_dominated_values))\n",
    "print(statistics.stdev(divga_non_dominated_values))\n",
    "print(divga_non_dominated_values)\n",
    "print(greedy_non_dominated)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "01879508-5e39-4f21-9b83-d67f52da655b",
   "metadata": {},
   "source": [
    "# Unpack the tuples for each list\n",
    "def unpack_tuples(data):\n",
    "    return zip(*data)\n",
    "\n",
    "#qtcs_pareto_vectors,divga_pareto_vectors,greedy_pareto_vectors\n",
    "\n",
    "x1, y1, z1 = unpack_tuples(qtcs_pareto_vectors)\n",
    "x2, y2, z2 = unpack_tuples(divga_pareto_vectors)\n",
    "x3, y3, z3 = unpack_tuples(greedy_pareto_vectors)\n",
    "x4, y4, z4 = unpack_tuples(reference_pareto)\n",
    "\n",
    "# Create 3D scatter plots\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "\n",
    "# First plot: list1 vs list4\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.scatter(x1, y1, z1, c='r', marker='s', s=100, label='QA')\n",
    "ax1.scatter(x4, y4, z4, c='b', marker='o', s=50, label='Reference Pareto')\n",
    "ax1.set_xlabel('Cost')\n",
    "ax1.set_ylabel('Statement Coverage')\n",
    "ax1.set_zlabel('Past Faults')\n",
    "ax1.set_title('QA vs Reference')\n",
    "ax1.legend()\n",
    "\n",
    "# Second plot: list2 vs list4\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax2.scatter(x2, y2, z2, c='g', marker='s', s=100, label='DIV-GA')\n",
    "ax2.scatter(x4, y4, z4, c='b', marker='o', s=50, label='Reference Pareto')\n",
    "ax2.set_xlabel('Cost')\n",
    "ax2.set_ylabel('Statement Coverage')\n",
    "ax2.set_zlabel('Past Faults')\n",
    "ax2.set_title('DIV-GA vs Reference Pareto')\n",
    "ax2.legend()\n",
    "\n",
    "# Third plot: list3 vs list4\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "ax3.scatter(x3, y3, z3, c='m', marker='s', s=100, label='Add. Greedy')\n",
    "ax3.scatter(x4, y4, z4, c='b', marker='o', s=50, label='Reference Pareto')\n",
    "ax3.set_xlabel('Cost')\n",
    "ax3.set_ylabel('Statement Coverage')\n",
    "ax3.set_zlabel('Past Faults')\n",
    "ax3.set_title('Add. Greedy vs Reference Pareto')\n",
    "ax3.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4cc05bbf4e37a2",
   "metadata": {},
   "source": [
    "#the static values below have been manually stored after each run of the experiments\n",
    "\n",
    "qtcs_nondom_flex = [194, 194, 193, 193, 194, 189, 194, 189, 188, 187]\n",
    "div_ga_nondom_flex = [140, 140, 140, 140, 140, 140, 140, 140, 140, 140]\n",
    "add_greedy_nondom_flex = [205]\n",
    "\n",
    "qtcs_nondom_grep = [258, 235, 261, 251, 260, 233, 259, 265, 261, 267]\n",
    "div_ga_nondom_grep = [70, 69, 70, 70, 70, 69, 70, 70, 70, 70]\n",
    "add_greedy_nondom_grep = [177]\n",
    "\n",
    "qtcs_nondom_gzip = [67, 66, 69, 31, 69, 32, 64, 33, 35, 35]\n",
    "div_ga_nondom_gzip = [104, 105, 101, 105, 104, 103, 103, 105, 105, 88]\n",
    "add_greedy_nondom_gzip = [71]\n",
    "\n",
    "qtcs_nondom_sed = [104, 123, 110, 105, 110, 110, 115, 101, 107, 99]\n",
    "div_ga_nondom_sed = [105, 62, 105, 105, 102, 105, 105, 97, 105, 105]\n",
    "add_greedy_nondom_sed = [80]\n",
    "\n",
    "lists = [qtcs_nondom_flex,div_ga_nondom_flex,qtcs_nondom_grep,div_ga_nondom_grep,qtcs_nondom_gzip,div_ga_nondom_gzip,qtcs_nondom_sed,div_ga_nondom_sed]\n",
    "\n",
    "# we are interested in knowing if the obtained sequences are normally distributed to decide what statistical test we should apply\n",
    "for i, list in enumerate(lists, start=1):\n",
    "    stat, p_value = shapiro(list)\n",
    "    print(f\"List {i}: Statistic = {stat:.5f}, P-value = {p_value:.5f}\")\n",
    "    \n",
    "    # check if the sequence is normally distributed (using 0.05 as threshold)\n",
    "    if p_value > 0.05:\n",
    "        print(f\"List {i} seems to be normally distributed (p-value = {p_value:.5f})\")\n",
    "    else:\n",
    "        print(f\"List {i} NOT seems to be normally distributed (p-value = {p_value:.5f})\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1363080a",
   "metadata": {},
   "source": [
    "def a12(lst1,lst2,rev=True):\n",
    "  \"how often is x in lst1 more than y in lst2?\"\n",
    "  more = same = 0.0\n",
    "  for x in lst1:\n",
    "    for y in lst2:\n",
    "      if x==y : same += 1\n",
    "      elif rev and x > y : more += 1\n",
    "      elif not rev and x < y : more += 1\n",
    "  return (more + 0.5*same)  / (len(lst1)*len(lst2))\n",
    "\n",
    "def stat_test(app1, app2):\n",
    "    statistic, pvalue = mannwhitneyu(app1, app2, alternative='two-sided')\n",
    "\n",
    "    # Calculate the A12 effect size using Vargha and Delaney's formula\n",
    "    a12_effect_size = a12(app1, app2)\n",
    "\n",
    "    return pvalue, a12_effect_size\n",
    "\n",
    "p_value, a_12 = stat_test(qtcs_nondom_sed,add_greedy_nondom_sed)\n",
    "print(\"p_value: \" + str(p_value))\n",
    "print(\"a12: \" + str(a_12))\n",
    "#5.8493119157503726e-05"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5adc0727-1230-43c8-8e54-e3de645c1029",
   "metadata": {},
   "source": [
    "with open('.\\\\results\\\\add-greedy\\\\sed_data.json', 'r') as file:\n",
    "    pareto_fronts_json = json.load(file)\n",
    "\n",
    "greedy_execution_time = pareto_fronts_json['resolution_time(ms)']\n",
    "print(greedy_execution_time)\n",
    "\n",
    "with open('.\\\\results\\\\selectqa\\\\sed-data.json', 'r') as file:\n",
    "    pareto_fronts_json = json.load(file)\n",
    "\n",
    "qtcs_qpu_execution_time = pareto_fronts_json['mean_qpu_run_time(ms)']\n",
    "qtcs_pareto_building_time = pareto_fronts_json['mean_pareto_fronts_building_time(ms)']\n",
    "print(qtcs_qpu_execution_time)\n",
    "print(qtcs_pareto_building_time)\n",
    "\n",
    "with open('.\\\\results\\\\divga\\\\sed_pareto_fronts_divga.json', 'r') as file:\n",
    "    pareto_fronts_json = json.load(file)\n",
    "\n",
    "divga_execution_time = pareto_fronts_json['DIVGA_II_mean_execution_time_ms']\n",
    "print(divga_execution_time)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1ed58fef23cbc91f",
   "metadata": {},
   "source": [
    "## BootQA\n",
    "\n",
    "BootQA is the first approach to a test case optimization problem using quantum computing. BootQA relies on real-world industrial datasets:\n",
    "\n",
    "- PaintControl: dataset gathered from ABB Robotics Norway;\n",
    "- GSDTSR: dataset gathered from Google.\n",
    "\n",
    " Each test case in both the datasets has values related to the properties:\n",
    " \n",
    "- \"execution time\": time needed by the test case to be executed;\n",
    "- \"failure rate\": the ability, in percentage, of a test case to spot a failure.\n",
    "\n",
    "To deal with physical limitations of quantum annealers, BootQA randomly divides the problem into m sub-problems of size n; once it has obtained the m sub-solutions, BootQA merges them into one final solution.\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "To compare SelectQA and BootQA were executed 10 indipendent experiments for each dataset. The resulting suites (10 for each dataset) were directly confronted in terms of \"execution time\" and \"failure rate\". The statistical tests used the same methods used for the previous comparisons and were developed in \"./BootQA/stats.py\".\n",
    "\n",
    "To compare the efficiency of the two methods, has been use the \"QPU Access Time\" metric."
   ]
  },
  {
   "cell_type": "code",
   "id": "5cce857b18a69eed",
   "metadata": {},
   "source": [
    "def bootstrap_confidence_interval(data, num_samples, confidence_alpha=0.95):\n",
    "    \"\"\"This function determines the statistical range within we would expect the mean value of execution times to fall; it relies on the bootstrapping strategy, which allows the calculation of the confidence interval by repeated sampling (with replacement) from the existing data to obtain an estimate of the confidence interval.\"\"\"\n",
    "    sample_means = []\n",
    "    for _ in range(num_samples):\n",
    "        bootstrap_sample = [random.choice(data) for _ in range(len(data))]\n",
    "        sample_mean = np.mean(bootstrap_sample)\n",
    "        sample_means.append(sample_mean)\n",
    "    \n",
    "    lower_percentile = (1 - confidence_alpha) / 2 * 100\n",
    "    upper_percentile = (confidence_alpha + (1 - confidence_alpha) / 2) * 100\n",
    "    lower_bound = np.percentile(sample_means, lower_percentile)\n",
    "    upper_bound = np.percentile(sample_means, upper_percentile)\n",
    "    \n",
    "    return lower_bound, upper_bound"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5209356b015e7dd",
   "metadata": {},
   "source": [
    "def create_linear_bqm(test_cases_costs,test_cases_rates,alpha):\n",
    "    \"\"\"This function is the one that has to encode the QUBO problem that the D-Wave Hybrid Sampler will have to solve. The QUBO problem specifies the optimization to solve and a quadratic binary unconstrained problem\"\"\"\n",
    "    qubo = dimod.AdjVectorBQM(dimod.BINARY)\n",
    "    \n",
    "    max_cost = max(test_cases_costs)\n",
    "\n",
    "    #linear coefficients, that are the diagonal of the matrix encoding the QUBO\n",
    "    for i in range(len(test_cases_costs)):\n",
    "        cost = (alpha * ((test_cases_costs[i])/max_cost)) - ((1-alpha)*test_cases_rates[i])\n",
    "        qubo.set_linear(i, cost)\n",
    "\n",
    "    return qubo"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1dc0f4aa-da57-42e3-9dc2-de9f8089400c",
   "metadata": {},
   "source": [
    "bootqa_programs = [\"gsdtsr\",\"paintcontrol\"]\n",
    "bootqa_alphas = {\"gsdtsr\": 0.98,\"paintcontrol\": 0.90}\n",
    "run_times_dictionary = {\"gsdtsr\": [],\"paintcontrol\": []}\n",
    "sampler = LeapHybridSampler(token=\"\",qpu=True)\n",
    "\n",
    "def get_data(data_name):\n",
    "    data = pd.read_csv(\"BootQA/\"+data_name+\"/\"+data_name+\".csv\", dtype={\"time\": float, \"rate\": float})\n",
    "    #I just consider data with failure rate > 0 (because it means they found at least 1 fault)\n",
    "    data = data.drop(data[data['rate'] == 0].index)\n",
    "    #this way every case has 1 as bool failure value\n",
    "    return data\n",
    "\n",
    "for bootqa_program in bootqa_programs:\n",
    "    final_test_suite_costs = []\n",
    "    final_failure_rates = []\n",
    "    for _ in range(tot_run):\n",
    "        data = get_data(bootqa_program)\n",
    "        test_cases_costs = data[\"time\"].tolist()\n",
    "        test_cases_rates = data[\"rate\"].tolist()\n",
    "        linear_bqm = create_linear_bqm(test_cases_costs, test_cases_rates, bootqa_alphas[bootqa_program])\n",
    "        #for each iteration get the result\n",
    "        response = sampler.sample_qubo(linear_bqm.to_numpy_matrix())\n",
    "        run_times_dictionary[bootqa_program].append(response.info['qpu_access_time'])\n",
    "        \n",
    "        first_sample_dictionary = {i: response.samples()[0][i] for i in range(len(response.samples()[0]))}\n",
    "    \n",
    "        #make a list with only the ids of test cases selected by the final solution\n",
    "        final_selected_cases = [id for id, value in first_sample_dictionary.items() if value == 1]\n",
    "        \n",
    "        #compute the final test suite cost\n",
    "        final_test_suite_cost = 0\n",
    "        for selected_test_case in final_selected_cases:\n",
    "            final_test_suite_cost += test_cases_costs[selected_test_case]\n",
    "        final_test_suite_costs.append(final_test_suite_cost)\n",
    "            \n",
    "        #compute the total failure rate\n",
    "        final_failure_rate = 0\n",
    "        for selected_test_case in final_selected_cases:\n",
    "            final_failure_rate += test_cases_rates[selected_test_case]\n",
    "        final_failure_rates.append(final_failure_rate)\n",
    "    \n",
    "    #compute the qpu access times\n",
    "    qpu_run_times_without_zeros = []\n",
    "    for access_time in run_times_dictionary[bootqa_program]:\n",
    "      if access_time != 0:\n",
    "        qpu_run_times_without_zeros.append(access_time)\n",
    "    lower_bound, upper_bound = bootstrap_confidence_interval(qpu_run_times_without_zeros, 1000, 0.95)\n",
    "    for i in range(len(run_times_dictionary[bootqa_program])):\n",
    "      if run_times_dictionary[bootqa_program][i] == 0:\n",
    "          run_times_dictionary[bootqa_program][i] = upper_bound\n",
    "    average_qpu_access_time = statistics.mean(run_times_dictionary[bootqa_program]) \n",
    "    \n",
    "    var_names = [\"final_test_suite_costs\", \"final_failure_rates\",\n",
    "                 \"average_qpu_access_time(ms)\", \"qpu_lower_bound(ms)\", \"qpu_upper_bound()\"]\n",
    "    values = [final_test_suite_costs, final_failure_rates, average_qpu_access_time, lower_bound, upper_bound]\n",
    "    \n",
    "    with open(\"BootQA/\"+bootqa_program+\"/sum.csv\", \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(var_names)\n",
    "        writer.writerow(values)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c89498f",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
